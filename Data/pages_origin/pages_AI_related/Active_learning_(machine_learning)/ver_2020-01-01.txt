{{Machine learning bar}}
{{about|a machine learning method|active learning in the context of education|active learning}}
'''Active learning''' is a special case of [[machine learning]] in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.<ref name="settles">{{cite document
 | title = Active Learning Literature Survey
 | url = http://pages.cs.wisc.edu/~bsettles/pub/settles.activelearning.pdf
 | author = Settles, Burr
 | series= Computer Sciences Technical Report 1648
 | publisher= University of Wisconsin–Madison
 | year = 2010
 | accessdate = 2014-11-18
}}</ref><ref name="rubens2016">{{cite book
 |last1=Rubens |first1=Neil
 |last2=Elahi |first2=Mehdi
 |last3=Sugiyama |first3=Masashi |last4=Kaplan |first4=Dain |editor1-last=Ricci
 |editor1-first=Francesco
 |editor2-last=Rokach |editor2-first=Lior
 |editor3-last=Shapira |editor3-first=Bracha
 |title=Recommender Systems Handbook
 |date=2016
 |publisher=Springer US
 |isbn=978-1-4899-7637-6
 |edition=2
 |chapter=Active Learning in Recommender Systems
 |doi=10.1007/978-1-4899-7637-6
|hdl=11311/1006123
}}</ref><ref name="das2016">{{cite book
 |last1=Das |first1=Shubhomoy
 |last2= Wong |first2=Weng-Keen
 |last3=Dietterich |first3=Thomas
 |last4=Fern |first4=Alan
 |last5=Emmott |first5=Andrew
 |editor1-last=Bonchi |editor1-first=Francesco
 |editor2-last=Domingo-Ferrer |editor2-first=Josep
 |editor3-last=Baeza-Yates |editor3-first=Ricardo
 |editor4-last=Zhou |editor4-first=Zhi-Hua
 |editor5-last=Wu |editor5-first=Xindong
 |chapter=Incorporating Expert Feedback into Active Anomaly Discovery
 |title=IEEE 16th International Conference on Data Mining
 |pages=853–858
|date=2016
 |publisher=IEEE
 |doi=10.1109/ICDM.2016.0102
|isbn=978-1-5090-5473-2
}}</ref> In statistics literature it is sometimes also called [[optimal experimental design]].<ref name="olsson">{{cite document | url=http://eprints.sics.se/3600/ | title=A literature survey of active machine learning in the context of natural language processing |series=SICS Technical Report T2009:06 | author=Olsson, Fredrik| date=April 2009 }}</ref>

There are situations in which unlabeled data is abundant but manually labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning,<ref name="multi"/> hybrid active learning<ref name="hybrid"/> and active learning in a single-pass (on-line) context,<ref name="single-pass"/> combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, [[incremental learning]] policies in the field of [[online machine learning]].

==Definitions==
Let {{mvar|T}} be the total set of all data under consideration. For example, in a protein engineering problem, {{mvar|T}} would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.

During each iteration, {{mvar|i}}, {{mvar|T}} is broken up into three subsets
#<math>\mathbf{T}_{K,i}</math>: Data points where the label is '''known'''.
#<math>\mathbf{T}_{U,i}</math>: Data points where the label is '''unknown'''.
#<math>\mathbf{T}_{C,i}</math>: A subset of {{mvar|T{{sub|U,i}}}} that is '''chosen''' to be labeled.

Most of the current research in active learning involves the best method to choose the data points for {{mvar|T{{sub|C,i}}}}.<ref>{{Cite web|url=https://ctl.byu.edu/tip/active-learning-techniques|title=Active Learning Techniques {{!}} CENTER FOR TEACHING AND LEARNING|website=ctl.byu.edu|access-date=2019-01-27}}</ref>

==Query strategies==
Algorithms for determining which data points should be labeled can be organized into a number of different categories, based upon their purpose:<ref name="settles" />

*'''Balance exploration and exploitation''': the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al.<ref name="Bouneffouf(2014)" /> propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label.
*'''Expected model change''': label those points that would most change the current model.
*'''Expected error reduction''': label those points that would most reduce the model's [[generalization error]].
*'''Exponentiated Gradient Exploration for Active Learning''':<ref name="Bouneffouf(2016)" /> In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration.
*'''Membership Query Synthesis''': This is where the learner generates its own instance from an underlying natural distribution. For example, if the dataset are pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human. This is particularly useful if your dataset is small.<ref>{{Cite journal |date=2015-01-05 |title=Active learning via query synthesis and nearest neighbour search |journal=Neurocomputing |volume=147 |pages=426–434 |doi=10.1016/j.neucom.2014.06.042 |last1=Wang |first1=Liantao |last2=Hu |first2=Xuelei |last3=Yuan |first3=Bo |last4=Lu |first4=Jianfeng|url=http://espace.library.uq.edu.au/view/UQ:344582/UQ344582_OA.pdf }}</ref>
*'''Pool-Based Sampling''': In this scenario, instances are drawn from the entire data pool and assigned an informative score, a measurement of how well the learner “understands” the data. The system then selects the most informative instances and queries the teacher for the labels.
*'''Stream-Based Selective Sampling''': Here, each unlabeled data point is examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each datapoint.
*'''Uncertainty sampling''': label those points for which the current model is least certain as to what the correct output should be.
*'''Query by committee''': a variety of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the "committee" disagrees the most
*'''Querying from diverse subspaces or partitions''':<ref name="shubhomoydas_github"/> When the underlying model is a forest of trees, the leaf nodes might represent (overlapping) partitions of the original [[feature (machine learning)|feature space]]. This offers the possibility of selecting instances from non-overlapping or minimally overlapping partitions for labeling.
*'''Variance reduction''': label those points that would minimize output variance, which is one of the components of error.
*'''Conformal Predictors''': This method predicts that a new data point will have a label similar to old data points in some specified way and degree of the similarity within the old examples is used to estimate the confidence in the prediction.<ref>{{Cite journal|last=Makili|first=Lázaro Emílio|last2=Sánchez|first2=Jesús A. Vega|last3=Dormido-Canto|first3=Sebastián|date=2012-10-01|title=Active Learning Using Conformal Predictors: Application to Image Classification|journal=Fusion Science and Technology|volume=62|issue=2|pages=347–355|doi=10.13182/FST12-A14626|issn=1536-1055}}</ref>

A wide variety of algorithms have been studied that fall into these categories.<ref name="settles" /><ref name="olsson" />

==Minimum Marginal Hyperplane==
Some active learning algorithms are built upon [[support-vector machine]]s (SVMs) and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the [[margin (machine learning)|margin]], {{mvar|W}}, of each unlabeled datum in {{mvar|T{{sub|U,i}}}} and treat {{mvar|W}} as an {{mvar|n}}-dimensional distance from that datum to the separating hyperplane.

Minimum Marginal Hyperplane methods assume that the data with the smallest {{mvar|W}} are those that the SVM is most uncertain about and therefore should be placed in {{mvar|T{{sub|C,i}}}} to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest {{mvar|W}}. Tradeoff methods choose a mix of the smallest and largest {{mvar|W}}s.

==Meetings==
* 2016 "Workshop Active Learning: Applications, Foundations and Emerging Trends" at iKNOW, Graz, Austria<ref>{{Cite web|url=http://vincentlemaire-labs.fr/iknow2016/|title=Workshop Active Learning: Applications, Foundations and Emerging Trends - iKNOW 2016|website=vincentlemaire-labs.fr|access-date=2018-12-04}}</ref>
* 2018 "Interactive Adaptive Learning" Workshop at ECML PKDD, Dublin, Ireland<ref>{{Cite web|url=http://www.uni-kassel.de/go/ial2018|title=IAL2018|last=Kottke|first=Daniel|website=www.uni-kassel.de|access-date=2018-12-04}}</ref>

==See also==
* [[List of datasets for machine learning research]]

==Notes==
{{reflist |refs=
<ref name="hybrid">{{cite journal |last1=Lughofer |first1=Edwin |title=Hybrid active learning for reducing the annotation effort of operators in classification systems |journal=Pattern Recognition |date=February 2012 |volume=45 |issue=2 |pages=884–896 |doi=10.1016/j.patcog.2011.08.009}}</ref>
<ref name="Bouneffouf(2014)">{{cite book |first=Djallel |last=Bouneffouf |first2=Romain |last2=Laroche |first3=Tanguy |last3=Urvoy |first4=Raphael |last4=Féraud |first5=Robin |last5=Allesiardo |year=2014 |chapter-url=https://hal.archives-ouvertes.fr/hal-01069802 |chapter=Contextual Bandit for Active Learning: Active Thompson |doi=10.1007/978-3-319-12637-1_51 |isbn=978-3-319-12636-4 |id=HAL Id: hal-01069802 |editors=Loo, C. K.; Yap, K. S.; Wong, K. W.; Teoh, A.; Huang, K. |title=Neural Information Processing |volume=8834 |pages=405–412 |series=Lecture Notes in Computer Science |url=https://hal.archives-ouvertes.fr/hal-01069802/file/Contextual_Bandit_for_Active_Learning.pdf }}</ref>
<ref name="multi">{{cite book |doi=10.1145/1557019.1557119 |isbn=978-1-60558-495-9 |chapter-url=https://www.microsoft.com/en-us/research/wp-content/uploads/2009/01/sigkdd09-yang.pdf|chapter=Effective multi-label active learning for text classification |title=Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '09 |pages=917 |year=2009 |last1=Yang |first1=Bishan |last2=Sun |first2=Jian-Tao |last3=Wang |first3=Tengjiao |last4=Chen |first4=Zheng |citeseerx=10.1.1.546.9358 }}</ref>
<ref name="single-pass">{{Cite journal | doi=10.1007/s12530-012-9060-7 |title = Single-pass active learning with conflict and ignorance| journal=Evolving Systems| volume=3| issue=4| pages=251–271|year = 2012|last1 = Lughofer|first1 = Edwin}}</ref>
<ref name="Bouneffouf(2016)">{{cite journal |last1=Bouneffouf |first1=Djallel |title=Exponentiated Gradient Exploration for Active Learning |journal=Computers |date=8 January 2016 |volume=5 |issue=1 |pages=1 |doi=10.3390/computers5010001|arxiv=1408.2196 }}</ref>
<ref name="shubhomoydas_github">{{Cite web|url=https://github.com/shubhomoydas/ad_examples#query-diversity-with-compact-descriptions|title=shubhomoydas/ad_examples|website=GitHub|language=en|access-date=2018-12-04}}</ref> 


}}

==Other references==
* [http://hunch.net/~active_learning/ Active Learning Tutorial], S. Dasgupta and J. Langford.

[[Category:Machine learning]]