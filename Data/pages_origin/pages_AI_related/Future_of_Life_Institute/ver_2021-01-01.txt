{{Confused|Future of Humanity Institute}}
{{coord|42.3736158|-71.1097335|display=title}}
{{short description|Nonprofit researching existential risk}}
{{Infobox organization
| name                = Future of Life Institute
| native_name         = 
| native_name_lang    = 
| named_after         = 
| image               = 
| image_size          = 
| alt                 = 
| caption             = 
| logo                = Future of Life Institute logo.svg
| logo_size           = 150
| logo_alt            = 
| logo_caption        = 
| map                 = 
| map_size            = 
| map_alt             = 
| map_caption         = 
| map2                = 
| map2_size           = 
| map2_alt            = 
| map2_caption        = 
| abbreviation        = 
| predecessor         = 
| merged              = 
| successor           = 
| formation           = {{start date and age|2014|03}}
| founders            =
* [[Jaan Tallinn]]
* [[Max Tegmark]]
* Viktoriya Krakovna
* [[Anthony Aguirre]]
* Meia Chita-Tegmark
| founding_location   = 
| extinction          = <!-- use {{end date and age|YYYY|MM|DD}} -->
| merger              = 
| type                = Non-profit research institute
| tax_id              = 47-1052538
| registration_id     = <!-- for non-profit org -->
| status              = Active
| purpose             = Mitigation of [[existential risk]]
| headquarters        = 
| location            = [[Cambridge, Massachusetts]], [[United States]]
| coords              = <!-- {{coord|42.3736158|-71.1097335|display=title}} -->
| region              = 
| services            = 
| products            = 
| methods             = 
| fields              = 
| membership          = 
| membership_year     = 
| language            = 
| owner               = <!-- or | owners = -->
| sec_gen             = 
| leader_title        = President
| leader_name         = Max Tegmark 
| leader_title2       = 
| leader_name2        = 
| leader_title3       = 
| leader_name3        = 
| leader_title4       = 
| leader_name4        = 
| board_of_directors  = 
| key_people          = 
| main_organ          = 
| parent_organization = 
| subsidiaries        = 
| secessions          = 
| affiliations        = 
| budget              = 
| budget_year         = 
| revenue             = 
| revenue_year        = 
| disbursements       = 
| expenses            = 
| expenses_year       = 
| endowment           = 
| staff               = 
| staff_year          = 
| volunteers          = 
| volunteers_year     = 
| motto               = Technology is giving life the potential to flourish like never before... Or to self destruct. Let's make a difference!
| website             = {{URL|futureoflife.org|FutureOfLife.org}}
| remarks             = 
| formerly            = 
| footnotes           = 
}}

The '''Future of Life Institute''' ('''FLI''') is a non-profit research institute and outreach organization in the [[Boston]] area that works to mitigate [[existential risks]] facing humanity, particularly [[existential risk from artificial general intelligence|existential risk from advanced artificial intelligence]] (AI). Its founders include MIT [[cosmologist]] [[Max Tegmark]] and [[Skype]] co-founder [[Jaan Tallinn]], and its board of advisors includes entrepreneur [[Elon Musk]].

==Background==
[[File:Max Tegmark.jpg|thumb|[[Max Tegmark]] professor at [[MIT]], one of the founders and current president of the Future of Life Institute]]
FLI's mission is to catalyze and support research and initiatives for safeguarding life and developing optimistic visions of the future, including positive ways for humanity to steer its course in response to new technologies and challenges.<ref name=huffpo_article>{{cite web | url=http://www.huffingtonpost.com/stephen-hawking/artificial-intelligence_b_5174265.html | title=Transcending Complacency on Superintelligent Machines | publisher=Huffington Post | date= 19 April 2014 | accessdate = 26 June 2014}}</ref><ref name=cser_news>{{cite web | url=http://cser.org/fli/ | title=CSER News: 'A new existential risk reduction organisation has launched in Cambridge, Massachusetts' | publisher=Centre for the Study of Existential Risk | date = 31 May 2014 | accessdate = 19 June 2014}}</ref> FLI is particularly focused on the potential risks to humanity from the development of human-level or [[superintelligent]] [[artificial general intelligence]] (AGI).<ref name=chronicle>{{cite journal | url=http://chronicle.com/article/Is-Artificial-Intelligence-a/148763/ | title = Is Artificial Intelligence a Threat? | journal = Chronicle of Higher Education | date = 11 September 2014 | accessdate = 18 Sep 2014| last1 = Chen | first1 = Angela }}</ref>

The Institute was founded in March 2014 by [[Massachusetts Institute of Technology|MIT]] cosmologist [[Max Tegmark]], [[Skype]] co-founder [[Jaan Tallinn]], [[Harvard University|Harvard]] graduate student and [[International Mathematical Olympiad]] (IMO) medalist Viktoriya Krakovna, [[Boston University]] graduate student Meia Chita-Tegmark (Tegmark's wife), and [[University of California, Santa Cruz|UCSC]] physicist [[Anthony Aguirre]]. The Institute's 14-person Scientific Advisory Board comprises 12 men and 2 women, and includes computer scientists [[Stuart J. Russell]] and [[Francesca Rossi]], biologist [[George M. Church|George Church]], cosmologist [[Saul Perlmutter]], astrophysicist [[Sandra Faber]], theoretical physicist [[Frank Wilczek]], entrepreneur [[Elon Musk]], and actors and science communicators [[Alan Alda]] and [[Morgan Freeman]] (as well as cosmologist [[Stephen Hawking]] prior to his death in 2018).<ref name=atlantic_article>{{cite web |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |publisher=The Atlantic | date = 9 May 2014 | accessdate = 13 April 2020}}</ref><ref name=fli_who_page>{{cite web | url=http://futureoflife.org/team | title = Who we are | publisher= Future of Life Institute | accessdate = 13 April 2020}}</ref><ref name=salon_article>{{cite web | url=http://www.salon.com/2014/10/05/our_science_fiction_apocalypse_meet_the_scientists_trying_to_predict_the_end_of_the_world/ |title = Our science-fiction apocalypse: Meet the scientists trying to predict the end of the world | work = Salon |date = 5 October 2014 | accessdate = 13 April 2020}}</ref>

== Events ==
On May 24, 2014, the Future of Life Institute held its opening event at [[Massachusetts Institute of Technology|MIT]]: a panel discussion on "The Future of Technology: Benefits and Risks", moderated by [[Alan Alda]].<ref>{{cite web | url=https://futureoflife.org/2014/05/24/the-future-of-technology-benefits-and-risks/ | title = The Future of Technology: Benefits and Risks | date = 24 May 2014 | publisher= Future of Life Institute }}</ref><ref name = "miri june newsletter">{{cite web | url=http://intelligence.org/2014/06/01/miris-june-2014-newsletter/ | title= Machine Intelligence Research Institute - June 2014 Newsletter | date= 2 June 2014 | accessdate = 19 June 2014}}</ref> The panelists were synthetic biologist [[George M. Church|George Church]], geneticist [[Ting Wu]], economist [[Andrew McAfee]], physicist and Nobel laureate [[Frank Wilczek]] and Skype co-founder [[Jaan Tallinn]].<ref name=fhi_news>{{cite web | url=http://www.fhi.ox.ac.uk/fli-mit/ | title = FHI News: 'Future of Life Institute hosts opening event at MIT' | publisher=Future of Humanity Institute | date = 20 May 2014 | accessdate = 19 June 2014}}</ref><ref name=pged>{{cite web | url=http://www.pged.org/event/the-future-of-technology-benefits-and-risks/ | title= The Future of Technology: Benefits and Risks | publisher= Personal Genetics Education Project | date = 9 May 2014 | accessdate = 19 June 2014}}</ref> The discussion covered a broad range of topics from the future of [[Biological engineering|bioengineering]] and personal genetics to autonomous weapons, [[Ethics of artificial intelligence|AI ethics]] and the [[Technological singularity|Singularity]].<ref name = panel_one_liners>{{cite web | url=http://dianacrowscience.com/fsi-risk-benefits-top-23/ | title = Top 23 One-liners From a Panel Discussion That Gave Me a Crazy Idea | publisher=Diana Crow Science | accessdate = 11 June 2014 | date = 29 May 2014}}</ref>

On January 2-5, 2015, FLI organized "The Future of AI: Opportunities and Challenges" conference in Puerto Rico, which brought together the world's leading AI builders from academia and industry to engage with each other and experts in economics, law, and ethics. The goal was to identify promising research directions that can help maximize the future benefits of AI.<ref name=ai_conference>{{cite web | url=https://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico/ |title=AI safety conference in Puerto Rico |publisher=Future of Life Institute| accessdate = 19 January 2015}}</ref> The Institute circulated an [[Open Letter on Artificial Intelligence|open letter on AI safety]] at the conference which was subsequently signed by [[Stephen Hawking]], [[Elon Musk]], and many artificial intelligence experts.<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=https://futureoflife.org/ai-open-letter|publisher=Future of Life Institute}}</ref>

On January 5-8, 2017, FLI organized the [[Asilomar Conference on Beneficial AI | Beneficial AI conference]] in Asilomar, California,<ref>{{Cite web |url=https://futureoflife.org/bai-2017/ |title=Beneficial AI 2017 | publisher=Future of Life Institute}}</ref> a private gathering of what ''The New York Times'' called "heavy hitters of A.I." (including [[Yann LeCun]], Elon Musk, and [[Nick Bostrom]]).<ref name="nyt0617"> {{Cite web |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |last=Metz |first=Cade |work=NYT |quote=The private gathering at the Asilomar Hotel was organized by the Future of Life Institute, a think tank built to discuss the existential risks of A.I. and other technologies. |date=June 9, 2018 |accessdate=June 10, 2018}}</ref> The institute released a set of principles for responsible AI development that came out of the discussion at the conference, signed by [[Yoshua Bengio]], [[Yann LeCun]], and many other AI researchers.<ref>{{Cite web |url=https://futureoflife.org/ai-principles/ |title=Asilomar AI Principles | publisher=Future of Life Institute}}</ref>

On January 4-7, 2019, FLI organized the Beneficial AGI conference in Puerto Rico.<ref>{{Cite web |url=https://futureoflife.org/beneficial-agi-2019/ |title=Beneficial AGI 2019 | publisher=Future of Life Institute}}</ref> This meeting focused on long-term questions on ensuring that Artificial General Intelligence is beneficial to humanity.<ref>{{Cite web |url=https://www.cser.ac.uk/news/cser-beneficial-agi-2019-conference/ |title=CSER at the Beneficial AGI 2019 Conference | publisher=Center for the Study of Existential Risk}}</ref>

== Global research program ==
On January 15, 2015, the Future of Life Institute announced that Elon Musk had donated $10 million to fund a global AI research endeavor.<ref>{{cite web|url=https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/ |title=Elon Musk donates $10M to keep AI beneficial |publisher=Future of Life Institute |date=15 January 2015}}</ref><ref>{{cite web|url=http://www.slashgear.com/elon-musk-donates-10m-to-artificial-intelligence-research-15364795/| title=Elon Musk donates $10M to Artificial Intelligence research|publisher=SlashGear|date=15 January 2015}}</ref><ref name=fastcompany_article>{{cite web | url=http://www.fastcompany.com/3041007/fast-feed/elon-musk-is-donating-10m-of-his-own-money-to-artificial-intelligence-research | title= Elon Musk is Donating $10M of his own Money to Artificial Intelligence Research | publisher= Fast Company | date = 15 January 2015}}</ref> On January 22, 2015, the FLI released a request for proposals from researchers in academic and other non-profit institutions.<ref>{{cite web|url=https://futureoflife.org/grants-timeline|title=An International Request for Proposals - Timeline|publisher=Future of Life Institute|date=22 January 2015}}</ref> Unlike typical AI research, this program is focused on making AI safer or more beneficial to society, rather than just more powerful.<ref>{{cite web|url=http://futureoflife.org/grants-rfp|title=2015 INTERNATIONAL GRANTS COMPETITION|publisher=Future of Life Institute}}</ref> On July 1, 2015, a total of $7 million was awarded to 37 research projects.<ref>{{cite web|url=https://futureoflife.org/2015selection |title=New International Grants Program Jump-Starts Research to Ensure AI Remains Beneficial |publisher=Future of Life Institute}}</ref>

== In the media ==
* United States and Allies Protest U.N. Talks to Ban Nuclear Weapons in "[[The New York Times]]"<ref>{{cite web|url=https://www.nytimes.com/2017/03/27/world/americas/un-nuclear-weapons-talks.html |title=United States and Allies Protest U.N. Talks to Ban Nuclear Weapons |work=[[New York Times]] |author1=Somini Sengupta |author2=Rick Gladstone |date=March 27, 2017}}</ref>
* "Is Artificial Intelligence a Threat?" in ''[[The Chronicle of Higher Education]]'', including interviews with FLI founders [[Max Tegmark]], [[Jaan Tallinn]] and Viktoriya Krakovna.<ref name=chronicle />
* "But What Would the End of Humanity Mean for Me?", an interview with [[Max Tegmark]] on the ideas behind FLI in ''[[The Atlantic]]''.<ref name=atlantic_article />
* "Transcending Complacency on Superintelligent Machines", an op-ed in the ''[[Huffington Post]]'' by [[Max Tegmark]], [[Stephen Hawking]], [[Frank Wilczek]] and [[Stuart J. Russell]] on the movie [[Transcendence (2014 film)|Transcendence]].<ref name=huffpo_article />
* "Top 23 One-liners From a Panel Discussion That Gave Me a Crazy Idea" in Diana Crow Science.<ref name = panel_one_liners />
* "An Open Letter to Everyone Tricked into Fearing Artificial Intelligence", includes "Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter" by the FLI <ref name=popsci_article>{{cite web | url=http://www.popsci.com/open-letter-everyone-tricked-fearing-ai | title= An Open Letter to Everyone Tricked into Fearing Artificial Intelligence | publisher= Popular Science | date = 14 January 2015 | accessdate = 19 January 2015}}</ref>
* {{cite web | title=Startup branding doesn't hide apocalyptic undertones of letter signed by Elon Musk | date=15 January 2015 | url=https://www.bizjournals.com/bizjournals/news/2015/01/15/startup-branding-doesn-t-hide-apocalyptic.html | author=Michael del Castillo | work=[[Upstart Bus. J.|Upstart Business Journal]] }}
* {{cite web | title=Ex Machina movie asks: is AI research in safe hands? | date=21 January 2015 | url=http://eandt.theiet.org/magazine/2015/02/ex-machina-ai-robots.cfm | author=Edd Gent | work=[[Engineering & Technology]] | access-date=26 January 2015 | archive-url=https://web.archive.org/web/20150126023624/http://eandt.theiet.org/magazine/2015/02/ex-machina-ai-robots.cfm | archive-date=26 January 2015 | url-status=dead }}
* "Creating Artificial Intelligence" on PBS<ref>{{cite web|url=https://www.pbs.org/wnet/religionandethics/2015/04/17/april-17-2015-creating-artificial-intelligence/25770/|title=Creating Artificial Intelligence|publisher=PBS|date=17 April 2015}}</ref>

== See also ==
* [[Future of Humanity Institute]]
* [[Centre for the Study of Existential Risk]]
* [[Global catastrophic risk]]
* [[Leverhulme Centre for the Future of Intelligence]]
* [[Machine Intelligence Research Institute]]
* [[Vasily Arkhipov (vice admiral)|Vasily Arkhipov]] "The man who saved the world"

== References ==
{{Reflist}}

== External links ==
* {{Official website|http://futureoflife.org}}

{{Effective altruism}}
{{Global catastrophic risks}}
{{Existential risk from artificial intelligence}}

[[Category:Futures studies]]
[[Category:2014 establishments in Massachusetts]]
[[Category:Research institutes established in 2014]]
[[Category:Artificial intelligence associations]]
[[Category:Transhumanist organizations]]
[[Category:Existential risk organizations]]
[[Category:Existential risk from artificial general intelligence]]