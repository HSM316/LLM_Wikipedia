{{Short description|International nonprofit research institute}}
{{Distinguish|Future of Humanity Institute}}
{{coord|42.3736158|-71.1097335|display=title}}
{{Infobox organization
| name                = Future of Life Institute
| native_name         = 
| native_name_lang    = 
| named_after         = 
| image               = Future of Life Institute logo.svg
| image_size          = 150
| alt                 = Logo of the Future of Life Institute
| caption             = 
| logo                = 
| logo_size           = 
| logo_alt            = 
| logo_caption        = 
| map                 = 
| map_size            = 
| map_alt             = 
| map_caption         = 
| map2                = 
| map2_size           = 
| map2_alt            = 
| map2_caption        = 
| abbreviation        = FLI
| predecessor         = 
| merged              = 
| successor           = 
| formation           = {{start date and age|2014|03}}
| founders            = {{Plainlist|
* [[Jaan Tallinn]]
* [[Max Tegmark]]
* Viktoriya Krakovna
* [[Anthony Aguirre]]
* Meia Chita-Tegmark
}}

| founding_location   = 
| extinction          = <!-- use {{end date and age|YYYY|MM|DD}} -->
| merger              = 
| type                = Non-profit research institute
| tax_id              = 
| registration_id     = <!-- for non-profit org -->
| status              =
| purpose             = Reduction of [[existential risk]], particularly from advanced artificial intelligence
| headquarters        = 
| location            = {{Plainlist|
* '''Global'''
* [[Brussels]], Belgium
* [[Campbell, California]], United States
}}
| coords              = <!-- {{coord|42.3736158|-71.1097335|display=title}} -->
| region              = 
| services            = 
| products            = 
| methods             = 
| fields              = 
| membership          = 
| membership_year     = 
| language            = 
| owner               = <!-- or | owners = -->
| sec_gen             = 
| leader_title        = President
| leader_name         = Max Tegmark 
| leader_title2       = 
| leader_name2        = 
| leader_title3       = 
| leader_name3        = 
| leader_title4       = 
| leader_name4        = 
| board_of_directors  = 
| key_people          = 
| main_organ          = 
| parent_organization = 
| subsidiaries        = 
| secessions          = 
| affiliations        = 
| budget              = 
| budget_year         = 
| revenue             = 
| revenue_year        = 
| disbursements       = 
| expenses            = 
| expenses_year       = 
| endowment           = 
| staff               = 
| staff_year          = 
| volunteers          = 
| volunteers_year     = 
| website             = {{URL|futureoflife.org|futureoflife.org}}
| remarks             = 
| formerly            = 
| footnotes           = 
}}

The '''Future of Life Institute''' ('''FLI''') is a [[nonprofit organization]] which aims to steer [[wikt:transformative|transformative]] technology towards benefiting life and away from large-scale risks, with a focus on [[existential risk from artificial general intelligence|existential risk from advanced artificial intelligence]] (AI). FLI's work includes [[grantmaking]], [[educational outreach]], and [[advocacy]] within the [[United Nations]], [[United States]] government, and [[European Union]] institutions. 

The founders of the Institute include [[Massachusetts Institute of Technology|MIT]] [[cosmologist]] [[Max Tegmark]], [[University of California, Santa Cruz|UCSC]] cosmologist [[Anthony Aguirre]], and [[Skype]] co-founder [[Jaan Tallinn]]; among the Institute's advisors is entrepreneur [[Elon Musk]].

== Purpose ==
[[File:Max Tegmark.jpg|thumb|[[Max Tegmark]], professor at [[MIT]], one of the founders and current president of the Future of Life Institute]]
FLI's stated mission is to steer transformative technology towards benefiting life and away from large-scale risks.<ref name=fli_website>{{cite web | url=https://futureoflife.org/ | title=Future of Life Institute homepage | publisher=Future of Life Institute | date=9 September 2021 | access-date=9 September 2021 | archive-date=8 September 2021 | archive-url=https://web.archive.org/web/20210908143821/https://futureoflife.org/ | url-status=live }}</ref> FLI's philosophy focuses on the potential risk to humanity from the development of human-level or [[superintelligent]] [[artificial general intelligence]] (AGI), but also works to mitigate risk from biotechnology, nuclear weapons and global warming.<ref name=chronicle>{{cite journal | url = http://chronicle.com/article/Is-Artificial-Intelligence-a/148763/ | title = Is Artificial Intelligence a Threat? | journal = Chronicle of Higher Education | date = 11 September 2014 | access-date = 18 Sep 2014 | last1 = Chen | first1 = Angela | archive-date = 22 December 2016 | archive-url = https://web.archive.org/web/20161222004051/http://www.chronicle.com/article/Is-Artificial-Intelligence-a/148763 | url-status = live }}</ref>

== History ==
FLI was founded in March 2014 by [[Massachusetts Institute of Technology|MIT]] cosmologist [[Max Tegmark]], [[Skype]] co-founder [[Jaan Tallinn]], [[DeepMind]] research scientist Viktoriya Krakovna, [[Tufts University]] postdoctoral scholar Meia Chita-Tegmark, and [[University of California, Santa Cruz|UCSC]] physicist [[Anthony Aguirre]]. The Institute's advisors include computer scientists [[Stuart J. Russell]] and [[Francesca Rossi]], biologist [[George M. Church|George Church]], cosmologist [[Saul Perlmutter]], astrophysicist [[Sandra Faber]], theoretical physicist [[Frank Wilczek]], entrepreneur [[Elon Musk]], and actors and science communicators [[Alan Alda]] and [[Morgan Freeman]] (as well as cosmologist [[Stephen Hawking]] prior to his death in 2018).<ref name=atlantic_article>{{cite web |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |publisher=The Atlantic |date=9 May 2014 |access-date=13 April 2020 |archive-date=4 June 2014 |archive-url=https://web.archive.org/web/20140604211145/http://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |url-status=live }}</ref><ref name=fli_who_page>{{cite web | url=http://futureoflife.org/team | title=Who we are | publisher=Future of Life Institute | access-date=13 April 2020 | archive-date=6 April 2020 | archive-url=https://web.archive.org/web/20200406150924/https://futureoflife.org/team/ | url-status=live }}</ref><ref name=salon_article>{{cite web |url = http://www.salon.com/2014/10/05/our_science_fiction_apocalypse_meet_the_scientists_trying_to_predict_the_end_of_the_world/ |title = Our science-fiction apocalypse: Meet the scientists trying to predict the end of the world |work = Salon |date = 5 October 2014 |access-date = 13 April 2020 |archive-date = 18 March 2021 |archive-url = https://web.archive.org/web/20210318143459/https://www.salon.com/2014/10/05/our_science_fiction_apocalypse_meet_the_scientists_trying_to_predict_the_end_of_the_world/ |url-status = live }}</ref>

Starting in 2017, FLI has offered an annual "Future of Life Award", with the first awardee being [[Vasili Arkhipov]]. The same year, FLI released ''[[Slaughterbots]]'', a short arms-control advocacy film. FLI released a sequel in 2021.<ref>{{cite news |last1=Walsh |first1=Bryan |title=The physicist Max Tegmark works to ensure that life has a future |url=https://www.vox.com/future-perfect/23380941/future-perfect-50-max-tegmark-future-of-life-institute-physicist |access-date=31 March 2023 |work=Vox |date=20 October 2022 |language=en |archive-date=31 March 2023 |archive-url=https://web.archive.org/web/20230331053401/https://www.vox.com/future-perfect/23380941/future-perfect-50-max-tegmark-future-of-life-institute-physicist |url-status=live }}</ref> 

In 2018, FLI drafted a letter calling for "laws against lethal autonomous weapons". Signatories included [[Elon Musk]], [[Demis Hassabis]], [[Shane Legg]], and [[Mustafa Suleyman]].<ref>{{cite news |title=AI Innovators Take Pledge Against Autonomous Killer Weapons |url=https://www.npr.org/2018/07/18/630146884/ai-innovators-take-pledge-against-autonomous-killer-weapons |access-date=31 March 2023 |work=NPR |date=2018 |archive-date=31 March 2023 |archive-url=https://web.archive.org/web/20230331053358/https://www.npr.org/2018/07/18/630146884/ai-innovators-take-pledge-against-autonomous-killer-weapons |url-status=live }}</ref>

In January 2023, Swedish magazine [[Expo (magazine)|''Expo'']] reported that the FLI had offered a grant of $100,000 to a foundation set up by ''[[Nya Dagbladet]]'', a Swedish [[Far-right politics|far-right]] online newspaper.<ref name=":5">{{Cite web |last1=Dalsbro |first1=Anders |last2=Leman |first2=Jonathan |date=2023-01-13 |title=Elon Musk-funded nonprofit run by MIT professor offered to finance Swedish pro-nazi group |url=https://expo.se/2023/01/elon-musk-funded-nonprofit-run-mit-professor-offered-finance-swedish-pro-nazi-group |url-status=live |archive-url=https://web.archive.org/web/20230625040739/https://expo.se/2023/01/elon-musk-funded-nonprofit-run-mit-professor-offered-finance-swedish-pro-nazi-group |archive-date=2023-06-25 |access-date=2023-08-17 |website=Expo |language=}}</ref><ref name=":4">{{Cite web |last=Hume |first=Tim |date=2023-01-19 |title=Elon Musk-Backed Non-Profit Offered $100K Grant to 'Pro-Nazi' Media Outlet |url=https://www.vice.com/en/article/93a475/future-of-life-institute-max-tegmark-elon-musk |url-status=live |archive-url=https://web.archive.org/web/20230623160433/https://www.vice.com/en/article/93a475/future-of-life-institute-max-tegmark-elon-musk |archive-date=2023-06-23 |access-date=2023-08-17 |website=[[Vice (magazine)|Vice]] |language=en}}</ref> In response, Tegmark said that the institute had only become aware of ''Nya Dagbladet''<nowiki/>'s positions during [[due diligence]] processes a few months after the grant was initially offered, and that the grant had been immediately revoked.<ref name=":4" />

=== Open letter on an AI pause ===
{{vanchor|In March 2023, FLI published a letter|Pause Giant AI Experiments}} titled "[[Pause Giant AI Experiments: An Open Letter]]". This called on major AI developers to agree on a verifiable six-month pause of any systems "more powerful than [[GPT-4]]" and to use that time to institute a framework for ensuring safety; or, failing that, for governments to step in with a moratorium. The letter said: "recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no-one - not even their creators - can understand, predict, or reliably control".<ref>{{Cite news |date=2023-03-29 |title=Elon Musk among experts urging a halt to AI training |language=en-GB |work=BBC News |url=https://www.bbc.com/news/technology-65110030 |access-date=2023-04-01 |archive-date=2023-04-01 |archive-url=https://web.archive.org/web/20230401115027/https://www.bbc.com/news/technology-65110030 |url-status=live }}</ref> The letter referred to the possibility of "a profound change in the history of life on Earth" as well as potential risks of AI-generated propaganda, loss of jobs, human obsolescence, and society-wide loss of control.<ref>{{cite news |title=Elon Musk and other tech leaders call for pause in 'out of control' AI race |url=https://www.cnn.com/2023/03/29/tech/ai-letter-elon-musk-tech-leaders/index.html |access-date=30 March 2023 |work=CNN |date=29 March 2023 |language=en |archive-date=10 April 2023 |archive-url=https://web.archive.org/web/20230410150317/https://www.cnn.com/2023/03/29/tech/ai-letter-elon-musk-tech-leaders/index.html |url-status=live }}</ref><ref>{{cite web |title=Pause Giant AI Experiments: An Open Letter |url=https://futureoflife.org/open-letter/pause-giant-ai-experiments/ |website=Future of Life Institute |access-date=30 March 2023 |archive-date=27 March 2023 |archive-url=https://web.archive.org/web/20230327111111/https://futureoflife.org/open-letter/pause-giant-ai-experiments/ |url-status=live }}</ref>

Prominent signatories of the letter included [[Elon Musk]], [[Steve Wozniak]], [[Evan Sharp]], [[Chris Larsen]], and [[Gary Marcus]]; AI lab CEOs Connor Leahy and [[Emad Mostaque]]; politician [[Andrew Yang]]; deep-learning researcher [[Yoshua Bengio]]; and [[Yuval Noah Harari]].<ref>{{Cite news |last=Ball |first=James |date=2023-04-02 |title=We're in an AI race, banning it would be foolish |language=en |work=[[The Sunday Times]] |url=https://www.thetimes.co.uk/article/were-in-an-ai-race-banning-it-would-be-foolish-kl3qrrn6x |access-date=2023-04-02 |issn= |archive-date=2023-08-19 |archive-url=https://web.archive.org/web/20230819123650/https://www.thetimes.co.uk/article/were-in-an-ai-race-banning-it-would-be-foolish-kl3qrrn6x |url-status=live }}</ref> Marcus stated "the letter isn't perfect, but the spirit is right." Mostaque stated, "I don't think a six month pause is the best idea or agree with everything but there are some interesting things in that letter." In contrast, Bengio explicitly endorsed the six-month pause in a press conference.<ref>{{cite news |title=Musk and Wozniak among 1,100+ signing open letter calling for 6-month ban on creating powerful A.I. |url=https://fortune.com/2023/03/29/elon-musk-apple-steve-wozniak-over-1100-sign-open-letter-6-month-ban-creating-powerful-ai/ |access-date=30 March 2023 |work=Fortune |date=March 2023 |language=en |archive-date=29 March 2023 |archive-url=https://web.archive.org/web/20230329233553/https://fortune.com/2023/03/29/elon-musk-apple-steve-wozniak-over-1100-sign-open-letter-6-month-ban-creating-powerful-ai/ |url-status=live }}</ref><ref>{{cite news |title=The Open Letter to Stop 'Dangerous' AI Race Is a Huge Mess |url=https://www.vice.com/en/article/qjvppm/the-open-letter-to-stop-dangerous-ai-race-is-a-huge-mess |access-date=30 March 2023 |work=www.vice.com |date=March 2023 |language=en |archive-date=30 March 2023 |archive-url=https://web.archive.org/web/20230330004040/https://www.vice.com/en/article/qjvppm/the-open-letter-to-stop-dangerous-ai-race-is-a-huge-mess |url-status=live }}</ref> Musk predicted that "Leading AGI developers will not heed this warning, but at least it was said."<ref>{{cite web |title=Elon Musk |url=https://mobile.twitter.com/elonmusk/status/1641132241035329536 |website=Twitter |access-date=30 March 2023 |language=en |archive-date=30 March 2023 |archive-url=https://web.archive.org/web/20230330045430/https://mobile.twitter.com/elonmusk/status/1641132241035329536 |url-status=live }}</ref> Some signatories, including Musk, said they were motivated by fears of [[existential risk from artificial general intelligence]].<ref>{{cite news |last1=Rosenberg |first1=Scott |title=Open letter sparks debate over "pausing" AI research over risks |url=https://www.axios.com/2023/03/30/chatgpt-ai-pause-debate-existential-risk |access-date=31 March 2023 |work=Axios |date=30 March 2023 |language=en |archive-date=31 March 2023 |archive-url=https://web.archive.org/web/20230331053920/https://www.axios.com/2023/03/30/chatgpt-ai-pause-debate-existential-risk |url-status=live }}</ref> Some of the other signatories, such as Marcus, instead said they signed out of concern about risks such as AI-generated propaganda.<ref>{{cite news |title=Tech leaders urge a pause in the 'out-of-control' artificial intelligence race |url=https://www.npr.org/2023/03/29/1166896809/tech-leaders-urge-a-pause-in-the-out-of-control-artificial-intelligence-race |access-date=30 March 2023 |work=NPR |date=2023 |archive-date=29 March 2023 |archive-url=https://web.archive.org/web/20230329223523/https://www.npr.org/2023/03/29/1166896809/tech-leaders-urge-a-pause-in-the-out-of-control-artificial-intelligence-race |url-status=live }}</ref>

The authors of one of the papers cited in FLI's letter, "[[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?]]"<ref>{{Cite book |last1=Bender |first1=Emily M. |last2=Gebru |first2=Timnit |last3=McMillan-Major |first3=Angelina |last4=Shmitchell |first4=Shmargaret |title=Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency |chapter=On the Dangers of Stochastic Parrots: Can Language Models be Too Big? |date=2021-03-03 |language=en |location=Virtual Event Canada |publisher=ACM |pages=610–623 |doi=10.1145/3442188.3445922 |isbn=978-1-4503-8309-7 |doi-access=free }}</ref> including [[Emily M. Bender]], [[Timnit Gebru]], and [[Margaret Mitchell (scientist)|Margaret Mitchell]], criticised the letter.<ref name=":0">{{Cite news |last=Kari |first=Paul |date=2023-04-01 |title=Letter signed by Elon Musk demanding AI research pause sparks controversy |language=en-GB |work=[[The Guardian]] |url=https://www.theguardian.com/technology/2023/mar/31/ai-research-pause-elon-musk-chatgpt |access-date=2023-04-01 |issn= |archive-date=2023-04-01 |archive-url=https://web.archive.org/web/20230401063716/https://www.theguardian.com/technology/2023/mar/31/ai-research-pause-elon-musk-chatgpt |url-status=live }}</ref> Mitchell said that “by treating a lot of questionable ideas as a given, the letter asserts a set of priorities and a narrative on AI that benefits the supporters of FLI. Ignoring active harms right now is a privilege that some of us don’t have.”<ref name=":0" />

== Operations ==

=== Advocacy ===
FLI has actively contributed to policymaking on AI. In October 2023, for example, U.S. Senate majority leader [[Chuck Schumer]] invited FLI to share its perspective on AI regulation with selected senators.<ref>{{Cite web |last=Krishan |first=Nihal |date=2023-10-26 |title=Sen. Chuck Schumer's second AI Insight Forum covers increased R&D funding, immigration challenges and safeguards |url=https://fedscoop.com/sen-chuck-schumers-second-ai-insight-forum-covers-increased-rd-funding-immigration-challenges-and-safeguards/ |access-date=2024-03-16 |website=FedScoop |language=en-US}}</ref> In Europe, FLI successfully advocated for the inclusion of more general AI systems, such as [[GPT-4]], in the EU's [[Artificial Intelligence Act]].<ref>{{Cite web |title=EU artificial intelligence act not 'futureproof', experts warn MEPs |url=https://sciencebusiness.net/news/eu-artificial-intelligence-act-not-futureproof-experts-warn-meps |access-date=2024-03-16 |website=Science{{!}}Business |language=en}}</ref>
[[File:Placard for the Future of Life Institute at the United Nations.jpg|alt=Future of Life Institute placard at the United Nations|thumb|FLI at the United Nations, Geneva HQ, 2021. On autonomous weapons.]]
In military policy, FLI coordinated the support of the scientific community for the [[Treaty on the Prohibition of Nuclear Weapons]].<ref>{{Citation |title=Scientists Support a Nuclear Ban | date=16 June 2017 |url=https://www.youtube.com/watch?v=Psk_VfHEjow |access-date=2024-03-16 |language=en}}</ref> At the UN and elsewhere, the Institute has also advocated for a treaty on [[Lethal autonomous weapon|autonomous weapons]].<ref>{{Cite web |title=Educating about Lethal Autonomous Weapons |url=https://futureoflife.org/project/lethal-autonomous-weapons-systems/ |access-date=2024-03-16 |website=Future of Life Institute |language=en-US}}</ref><ref>{{Cite web |last=Government of Costa Rica |date=February 24, 2023 |title=FLI address |url=https://conferenciaawscostarica2023.com/wp-content/uploads/2023/02/Future-of-Life-Institute-FLI-Statement.pdf |website=Latin American and the Caribbean conference on the social and humanitarian impact of autonomous weapons}}</ref>

=== Research grants ===
The FLI research program started in 2015 with an initial donation of $10 million from Elon Musk.<ref>{{cite web |date=15 January 2015 |title=Elon Musk donates $10M to keep AI beneficial |url=https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/ |url-status=dead |archive-url=https://web.archive.org/web/20180228042613/https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/ |archive-date=28 February 2018 |access-date=28 July 2019 |publisher=Future of Life Institute}}</ref><ref>{{cite web |date=15 January 2015 |title=Elon Musk donates $10M to Artificial Intelligence research |url=http://www.slashgear.com/elon-musk-donates-10m-to-artificial-intelligence-research-15364795/ |url-status=live |archive-url=https://web.archive.org/web/20150407185511/http://www.slashgear.com/elon-musk-donates-10m-to-artificial-intelligence-research-15364795/ |archive-date=7 April 2015 |access-date=26 April 2015 |publisher=SlashGear}}</ref><ref name="fastcompany_article">{{cite web |date=15 January 2015 |title=Elon Musk is Donating $10M of his own Money to Artificial Intelligence Research |url=http://www.fastcompany.com/3041007/fast-feed/elon-musk-is-donating-10m-of-his-own-money-to-artificial-intelligence-research |url-status=live |archive-url=https://web.archive.org/web/20151030202356/http://www.fastcompany.com/3041007/fast-feed/elon-musk-is-donating-10m-of-his-own-money-to-artificial-intelligence-research |archive-date=30 October 2015 |access-date=19 January 2015 |publisher=Fast Company}}</ref> In this initial round, a total of $7 million was awarded to 37 research projects.<ref>{{cite web |date=28 October 2015 |title=New International Grants Program Jump-Starts Research to Ensure AI Remains Beneficial |url=https://futureoflife.org/2015selection |url-status=live |archive-url=https://web.archive.org/web/20190728221400/https://futureoflife.org/2015selection/ |archive-date=28 July 2019 |access-date=28 July 2019 |publisher=Future of Life Institute}}</ref> In July 2021, FLI announced that it would launch a new $25 million grant program with funding from the Russian–Canadian programmer [[Vitalik Buterin]].<ref>{{cite web |date=2 July 2021 |title=FLI announces $25M grants program for existential risk reduction |url=https://futureoflife.org/2021/07/02/fli-june-2021-newsletter/ |url-status=live |archive-url=https://web.archive.org/web/20210909151941/https://futureoflife.org/2021/07/02/fli-june-2021-newsletter/ |archive-date=9 September 2021 |access-date=9 September 2021 |publisher=Future of Life Institute}}</ref>

=== Conferences ===
In 2014, the Future of Life Institute held its opening event at [[Massachusetts Institute of Technology|MIT]]: a panel discussion on "The Future of Technology: Benefits and Risks", moderated by [[Alan Alda]].<ref>{{cite web | url = https://futureoflife.org/2014/05/24/the-future-of-technology-benefits-and-risks/ | title = The Future of Technology: Benefits and Risks | date = 24 May 2014 | publisher = Future of Life Institute | access-date = 28 July 2019 | archive-date = 28 July 2019 | archive-url = https://web.archive.org/web/20190728114618/https://futureoflife.org/2014/05/24/the-future-of-technology-benefits-and-risks/ | url-status = live }}</ref><ref name = "miri june newsletter">{{cite web | url= http://intelligence.org/2014/06/01/miris-june-2014-newsletter/ | title= Machine Intelligence Research Institute - June 2014 Newsletter | date= 2 June 2014 | access-date= 19 June 2014 | archive-date= 3 July 2014 | archive-url= https://web.archive.org/web/20140703084814/http://intelligence.org/2014/06/01/miris-june-2014-newsletter/ | url-status= live }}</ref> The panelists were synthetic biologist [[George M. Church|George Church]], geneticist [[Ting Wu]], economist [[Andrew McAfee]], physicist and Nobel laureate [[Frank Wilczek]] and Skype co-founder [[Jaan Tallinn]].<ref name=fhi_news>{{cite web | url=http://www.fhi.ox.ac.uk/fli-mit/ | title=FHI News: 'Future of Life Institute hosts opening event at MIT' | publisher=Future of Humanity Institute | date=20 May 2014 | access-date=19 June 2014 | archive-date=27 July 2014 | archive-url=https://web.archive.org/web/20140727060336/http://www.fhi.ox.ac.uk/fli-mit/ | url-status=live }}</ref><ref name=pged>{{cite web | url= http://www.pged.org/event/the-future-of-technology-benefits-and-risks/ | title= The Future of Technology: Benefits and Risks | publisher= Personal Genetics Education Project | date= 9 May 2014 | access-date= 19 June 2014 | archive-date= 22 December 2015 | archive-url= https://web.archive.org/web/20151222085729/http://www.pged.org/event/the-future-of-technology-benefits-and-risks/ | url-status= live }}</ref>

Since 2015, FLI has organised biannual conferences with the stated purpose of bringing together AI researchers from academia and industry. {{As of|2023|Apr}}, the following conferences have taken place:
* "The Future of AI: Opportunities and Challenges" conference in Puerto Rico (2015). The stated goal was to identify promising research directions that could help maximize the future benefits of AI.<ref name=ai_conference>{{cite web | url=https://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico/ | title=AI safety conference in Puerto Rico | publisher=Future of Life Institute | access-date=19 January 2015 | archive-date=7 November 2015 | archive-url=https://web.archive.org/web/20151107081150/http://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico/ | url-status=live }}</ref> At the conference, FLI circulated an [[Open Letter on Artificial Intelligence|open letter on AI safety]] which was subsequently signed by [[Stephen Hawking]], [[Elon Musk]], and many artificial intelligence researchers.<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=https://futureoflife.org/ai-open-letter|publisher=Future of Life Institute|access-date=2019-07-28|archive-date=2019-08-10|archive-url=https://web.archive.org/web/20190810020404/https://futureoflife.org/ai-open-letter|url-status=live}}</ref>
* The [[Asilomar Conference on Beneficial AI|Beneficial AI conference]] in Asilomar, California (2017),<ref>{{Cite web |url=https://futureoflife.org/bai-2017/ |title=Beneficial AI 2017 |publisher=Future of Life Institute |access-date=2019-07-28 |archive-date=2020-02-24 |archive-url=https://web.archive.org/web/20200224010000/https://futureoflife.org/bai-2017/ |url-status=live }}</ref> a private gathering of what ''The New York Times'' called "heavy hitters of A.I." (including [[Yann LeCun]], Elon Musk, and [[Nick Bostrom]]).<ref name="nyt0617">{{Cite web |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |last=Metz |first=Cade |work=NYT |quote=The private gathering at the Asilomar Hotel was organized by the Future of Life Institute, a think tank built to discuss the existential risks of A.I. and other technologies. |date=June 9, 2018 |access-date=June 10, 2018 |archive-date=February 15, 2021 |archive-url=https://web.archive.org/web/20210215051949/https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |url-status=live }}</ref> The institute released a set of principles for responsible AI development that came out of the discussion at the conference, signed by [[Yoshua Bengio]], Yann LeCun, and many other AI researchers.<ref>{{Cite web |url=https://futureoflife.org/ai-principles/ |title=Asilomar AI Principles |publisher=Future of Life Institute |access-date=2019-07-28 |archive-date=2017-12-11 |archive-url=https://web.archive.org/web/20171211171044/https://futureoflife.org/ai-principles/ |url-status=live }}</ref> These principles may have influenced the [[regulation of artificial intelligence]] and subsequent initiatives, such as the [[OECD]] Principles on Artificial Intelligence.<ref>{{Cite web |url=https://www.oecd.org/going-digital/ai-intelligent-machines-smart-policies/conference-agenda/ai-intelligent-machines-smart-policies-oheigeartaigh.pdf |title=Asilomar Principles |publisher=OECD |access-date=2021-09-09 |archive-date=2021-09-09 |archive-url=https://web.archive.org/web/20210909151942/https://www.oecd.org/going-digital/ai-intelligent-machines-smart-policies/conference-agenda/ai-intelligent-machines-smart-policies-oheigeartaigh.pdf |url-status=live }}</ref>
* The beneficial AGI conference in Puerto Rico (2019).<ref>{{Cite web |url=https://futureoflife.org/beneficial-agi-2019/ |title=Beneficial AGI 2019 |publisher=Future of Life Institute |access-date=2019-07-28 |archive-date=2019-07-28 |archive-url=https://web.archive.org/web/20190728114618/https://futureoflife.org/beneficial-agi-2019/ |url-status=live }}</ref> The stated focus of the meeting was answering long-term questions with the goal of ensuring that [[artificial general intelligence]] is beneficial to humanity.<ref>{{Cite web |url=https://www.cser.ac.uk/news/cser-beneficial-agi-2019-conference/ |title=CSER at the Beneficial AGI 2019 Conference |publisher=Center for the Study of Existential Risk |access-date=2019-07-28 |archive-date=2019-07-28 |archive-url=https://web.archive.org/web/20190728114617/https://www.cser.ac.uk/news/cser-beneficial-agi-2019-conference/ |url-status=live }}</ref>

== In the media ==
* "The Fight to Define When AI is 'High-Risk'" in [[Wired (magazine)|''Wired'']].
* "Lethal Autonomous Weapons exist; They Must Be Banned" in ''[[IEEE Spectrum]]''.
* "United States and Allies Protest U.N. Talks to Ban Nuclear Weapons" in ''[[The New York Times]]''.
* "Is Artificial Intelligence a Threat?" in ''[[The Chronicle of Higher Education]]'', including interviews with FLI founders [[Max Tegmark]], [[Jaan Tallinn]] and Viktoriya Krakovna.
* "But What Would the End of Humanity Mean for Me?", an interview with [[Max Tegmark]] on the ideas behind FLI in ''[[The Atlantic]]''.

== See also ==
* [[Future of Humanity Institute]]
* [[Centre for the Study of Existential Risk]]
* [[Global catastrophic risk]]
* [[Leverhulme Centre for the Future of Intelligence]]
* [[Machine Intelligence Research Institute]]
* ''[[The Precipice: Existential Risk and the Future of Humanity]]''

== References ==
{{Reflist}}

== External links ==
* {{Official website|http://futureoflife.org}}

{{Effective altruism}}
{{Global catastrophic risks}}
{{Existential risk from artificial intelligence}}

[[Category:Futures studies organizations]]
[[Category:2014 establishments in Massachusetts]]
[[Category:Research institutes established in 2014]]
[[Category:Artificial intelligence associations]]
[[Category:Transhumanist organizations]]
[[Category:Existential risk organizations]]
[[Category:Existential risk from artificial general intelligence]]
[[Category:Organizations associated with effective altruism]]
[[Category:Regulation of artificial intelligence]]