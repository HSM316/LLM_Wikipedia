{{Short description|International nonprofit research institute}}
{{Distinguish|Future of Humanity Institute}}
{{coord|42.3736158|-71.1097335|display=title}}
{{Infobox organization
| name                = Future of Life Institute
| native_name         = 
| native_name_lang    = 
| named_after         = 
| image               = 
| image_size          = 
| alt                 = 
| caption             = 
| logo                = Future of Life Institute logo.svg
| logo_size           = 150
| logo_alt            = Logo of the Future of Life Institute
| logo_caption        = 
| map                 = 
| map_size            = 
| map_alt             = 
| map_caption         = 
| map2                = 
| map2_size           = 
| map2_alt            = 
| map2_caption        = 
| abbreviation        = FLI
| predecessor         = 
| merged              = 
| successor           = 
| formation           = {{start date and age|2014|03}}
| founders            =
* [[Jaan Tallinn]]
* [[Max Tegmark]]
* Viktoriya Krakovna
* [[Anthony Aguirre]]
* Meia Chita-Tegmark
| founding_location   = 
| extinction          = <!-- use {{end date and age|YYYY|MM|DD}} -->
| merger              = 
| type                = Non-profit research institute
| tax_id              = 47-1052538
| registration_id     = <!-- for non-profit org -->
| status              = Active
| purpose             = Reduction of [[existential risk]], particularly from advanced artificial intelligence
| headquarters        = 
| location            = [[Cambridge, Massachusetts]], United States
| coords              = <!-- {{coord|42.3736158|-71.1097335|display=title}} -->
| region              = 
| services            = 
| products            = 
| methods             = 
| fields              = 
| membership          = 
| membership_year     = 
| language            = 
| owner               = <!-- or | owners = -->
| sec_gen             = 
| leader_title        = President
| leader_name         = Max Tegmark 
| leader_title2       = 
| leader_name2        = 
| leader_title3       = 
| leader_name3        = 
| leader_title4       = 
| leader_name4        = 
| board_of_directors  = 
| key_people          = 
| main_organ          = 
| parent_organization = 
| subsidiaries        = 
| secessions          = 
| affiliations        = 
| budget              = 
| budget_year         = 
| revenue             = 
| revenue_year        = 
| disbursements       = 
| expenses            = 
| expenses_year       = 
| endowment           = 
| staff               = 
| staff_year          = 
| volunteers          = 
| volunteers_year     = 
| website             = {{URL|futureoflife.org|futureoflife.org}}
| remarks             = 
| formerly            = 
| footnotes           = 
}}

The '''Future of Life Institute''' ('''FLI''') is a nonprofit organization that works to reduce global catastrophic and [[existential risks]] facing humanity, particularly [[existential risk from artificial general intelligence|existential risk from advanced artificial intelligence]] (AI). The Institute's work is made up of three main strands: grantmaking for risk reduction, educational outreach, and advocacy within the United Nations, US government and European Union institutions. Its founders include MIT [[cosmologist]] [[Max Tegmark]] and [[Skype]] co-founder [[Jaan Tallinn]], and its advisors include entrepreneur [[Elon Musk]].

== Mission ==
[[File:Max Tegmark.jpg|thumb|[[Max Tegmark]], professor at [[MIT]], one of the founders and current president of the Future of Life Institute]]
FLI's mission is reduce global catastrophic and existential risk from powerful technologies.<ref name=fli_website>{{cite web | url=https://futureoflife.org/ | title=Future of Life Institute homepage | publisher=Future of Life Institute | date= 9 September 2021 | access-date = 9 September 2021}}</ref> FLI is particularly focused on the potential risks to humanity from the development of human-level or [[superintelligent]] [[artificial general intelligence]] (AGI), but also works on risks from biotechnology, nuclear weapons and climate change.<ref name=chronicle>{{cite journal | url=http://chronicle.com/article/Is-Artificial-Intelligence-a/148763/ | title = Is Artificial Intelligence a Threat? | journal = Chronicle of Higher Education | date = 11 September 2014 | access-date = 18 Sep 2014| last1 = Chen | first1 = Angela }}</ref> The Institute's work is made up grantmaking for risk reduction, educational outreach, and advocacy within the United Nations, US government and European Union institutions.<ref name=LinkedIn>{{cite web | url=https://www.linkedin.com/company/future-of-life-institute/about/ | title = About the Future of Life Institute | publisher=LinkedIn | date = 9 September 2021 | access-date = 9 September 2021}}</ref>

== Key people ==
The Institute was founded in March 2014 by [[Massachusetts Institute of Technology|MIT]] cosmologist [[Max Tegmark]], [[Skype]] co-founder [[Jaan Tallinn]], [[DeepMind]] research scientist Viktoriya Krakovna, [[Tufts University]] postdoctoral scholar Meia Chita-Tegmark, and [[University of California, Santa Cruz|UCSC]] physicist [[Anthony Aguirre]]. The Institute's advisors include computer scientists [[Stuart J. Russell]] and [[Francesca Rossi]], biologist [[George M. Church|George Church]], cosmologist [[Saul Perlmutter]], astrophysicist [[Sandra Faber]], theoretical physicist [[Frank Wilczek]], entrepreneur [[Elon Musk]], and actors and science communicators [[Alan Alda]] and [[Morgan Freeman]] (as well as cosmologist [[Stephen Hawking]] prior to his death in 2018).<ref name=atlantic_article>{{cite web |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |publisher=The Atlantic | date = 9 May 2014 | access-date = 13 April 2020}}</ref><ref name=fli_who_page>{{cite web | url=http://futureoflife.org/team | title = Who we are | publisher= Future of Life Institute | access-date = 13 April 2020}}</ref><ref name=salon_article>{{cite web | url=http://www.salon.com/2014/10/05/our_science_fiction_apocalypse_meet_the_scientists_trying_to_predict_the_end_of_the_world/ |title = Our science-fiction apocalypse: Meet the scientists trying to predict the end of the world | work = Salon |date = 5 October 2014 | access-date = 13 April 2020}}</ref>

== Conferences ==
In 2014, the Future of Life Institute held its opening event at [[Massachusetts Institute of Technology|MIT]]: a panel discussion on "The Future of Technology: Benefits and Risks", moderated by [[Alan Alda]].<ref>{{cite web | url=https://futureoflife.org/2014/05/24/the-future-of-technology-benefits-and-risks/ | title = The Future of Technology: Benefits and Risks | date = 24 May 2014 | publisher= Future of Life Institute }}</ref><ref name = "miri june newsletter">{{cite web | url=http://intelligence.org/2014/06/01/miris-june-2014-newsletter/ | title= Machine Intelligence Research Institute - June 2014 Newsletter | date= 2 June 2014 | access-date = 19 June 2014}}</ref> The panelists were synthetic biologist [[George M. Church|George Church]], geneticist [[Ting Wu]], economist [[Andrew McAfee]], physicist and Nobel laureate [[Frank Wilczek]] and Skype co-founder [[Jaan Tallinn]].<ref name=fhi_news>{{cite web | url=http://www.fhi.ox.ac.uk/fli-mit/ | title = FHI News: 'Future of Life Institute hosts opening event at MIT' | publisher=Future of Humanity Institute | date = 20 May 2014 | access-date = 19 June 2014}}</ref><ref name=pged>{{cite web | url=http://www.pged.org/event/the-future-of-technology-benefits-and-risks/ | title= The Future of Technology: Benefits and Risks | publisher= Personal Genetics Education Project | date = 9 May 2014 | access-date = 19 June 2014}}</ref>

Since 2015, FLI has organised biannual conferences that bring together leading AI builders from academia and industry. To date, the following conferences have taken place:
* "The Future of AI: Opportunities and Challenges" conference in Puerto Rico (2015). The goal was to identify promising research directions that can help maximize the future benefits of AI.<ref name=ai_conference>{{cite web | url=https://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico/ |title=AI safety conference in Puerto Rico |publisher=Future of Life Institute| access-date = 19 January 2015}}</ref> At the conference, the Institute circulated an [[Open Letter on Artificial Intelligence|open letter on AI safety]] which was subsequently signed by [[Stephen Hawking]], [[Elon Musk]], and many artificial intelligence experts.<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=https://futureoflife.org/ai-open-letter|publisher=Future of Life Institute}}</ref>
* The [[Asilomar Conference on Beneficial AI|Beneficial AI conference]] in Asilomar, California (2017),<ref>{{Cite web |url=https://futureoflife.org/bai-2017/ |title=Beneficial AI 2017 | publisher=Future of Life Institute}}</ref> a private gathering of what ''The New York Times'' called "heavy hitters of A.I." (including [[Yann LeCun]], Elon Musk, and [[Nick Bostrom]]).<ref name="nyt0617">{{Cite web |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |last=Metz |first=Cade |work=NYT |quote=The private gathering at the Asilomar Hotel was organized by the Future of Life Institute, a think tank built to discuss the existential risks of A.I. and other technologies. |date=June 9, 2018 |access-date=June 10, 2018}}</ref> The institute released a set of principles for responsible AI development that came out of the discussion at the conference, signed by [[Yoshua Bengio]], Yann LeCun, and many other AI researchers.<ref>{{Cite web |url=https://futureoflife.org/ai-principles/ |title=Asilomar AI Principles | publisher=Future of Life Institute}}</ref> These principles influenced the [[regulation of artificial intelligence]] and subsequent initiatives, such as the [[OECD]] Principles on Artificial Intelligence.<ref>{{Cite web |url=https://www.oecd.org/going-digital/ai-intelligent-machines-smart-policies/conference-agenda/ai-intelligent-machines-smart-policies-oheigeartaigh.pdf |title=Asilomar Principles | publisher=OECD}}</ref>
* The beneficial AGI conference in Puerto Rico (2019).<ref>{{Cite web |url=https://futureoflife.org/beneficial-agi-2019/ |title=Beneficial AGI 2019 | publisher=Future of Life Institute}}</ref> This meeting focused on long-term questions on ensuring that Artificial General Intelligence is beneficial to humanity.<ref>{{Cite web |url=https://www.cser.ac.uk/news/cser-beneficial-agi-2019-conference/ |title=CSER at the Beneficial AGI 2019 Conference | publisher=Center for the Study of Existential Risk}}</ref>

== Global research program ==
The FLI research program started in 2015 with an initial donation of $10 million from Elon Musk.<ref>{{cite web|url=https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/ |title=Elon Musk donates $10M to keep AI beneficial |publisher=Future of Life Institute |date=15 January 2015}}</ref><ref>{{cite web|url=http://www.slashgear.com/elon-musk-donates-10m-to-artificial-intelligence-research-15364795/| title=Elon Musk donates $10M to Artificial Intelligence research|publisher=SlashGear|date=15 January 2015}}</ref><ref name=fastcompany_article>{{cite web | url=http://www.fastcompany.com/3041007/fast-feed/elon-musk-is-donating-10m-of-his-own-money-to-artificial-intelligence-research | title= Elon Musk is Donating $10M of his own Money to Artificial Intelligence Research | publisher= Fast Company | date = 15 January 2015}}</ref> Unlike typical AI research, this program is focused on making AI safer or more beneficial to society, rather than just more powerful.<ref>{{cite web|url=http://futureoflife.org/grants-rfp|title=2015 INTERNATIONAL GRANTS COMPETITION|publisher=Future of Life Institute}}</ref> In this initial round, a total of $7 million was awarded to 37 research projects.<ref>{{cite web|url=https://futureoflife.org/2015selection |title=New International Grants Program Jump-Starts Research to Ensure AI Remains Beneficial |publisher=Future of Life Institute}}</ref> In July 2021, FLI announced that it would launch a new $25 million grant program with funding from the Russian–Canadian programmer [[Vitalik Buterin]].<ref>{{cite web|url=https://futureoflife.org/2021/07/02/fli-june-2021-newsletter/ |title=FLI announces $25M grants program for existential risk reduction|date=2 July 2021|publisher=Future of Life Institute}}</ref>

== In the media ==
* "The Fight to Define When AI is 'High-Risk'" in [[Wired (magazine)|''Wired'']].<ref>{{cite magazine|url=https://www.wired.com/story/fight-to-define-when-ai-is-high-risk/ |title=The Fight to Define When AI is 'High-Risk' |magazine=[[Wired (magazine)|Wired]] |author1=Khari Johnson |date=September 1, 2021}}</ref>
* "Lethal Autonomous Weapons exist; They Must Be Banned" in [[IEEE Spectrum]].<ref>{{cite web|url=https://spectrum.ieee.org/lethal-autonomous-weapons-exist-they-must-be-banned |title=Lethal Autonomous Weapons Exist; They Must Be Banned |work=[[IEEE Spectrum]] |author1=Stuart Russell|author2=Anthony Aguirre |date=June 16, 2021}}</ref>
* "United States and Allies Protest U.N. Talks to Ban Nuclear Weapons" in ''[[The New York Times]]''.<ref>{{cite web|url=https://www.nytimes.com/2017/03/27/world/americas/un-nuclear-weapons-talks.html |title=United States and Allies Protest U.N. Talks to Ban Nuclear Weapons |work=[[New York Times]] |author1=Somini Sengupta |author2=Rick Gladstone |date=March 27, 2017}}</ref>
* "Is Artificial Intelligence a Threat?" in ''[[The Chronicle of Higher Education]]'', including interviews with FLI founders [[Max Tegmark]], [[Jaan Tallinn]] and Viktoriya Krakovna.<ref name=chronicle />
* "But What Would the End of Humanity Mean for Me?", an interview with [[Max Tegmark]] on the ideas behind FLI in ''[[The Atlantic]]''.<ref name=atlantic_article />
* {{cite web | title=Startup branding doesn't hide apocalyptic undertones of letter signed by Elon Musk | date=15 January 2015 | url=https://www.bizjournals.com/bizjournals/news/2015/01/15/startup-branding-doesn-t-hide-apocalyptic.html | author=Michael del Castillo | work=[[Upstart Bus. J.|Upstart Business Journal]] }}

== See also ==
* [[Future of Humanity Institute]]
* [[Centre for the Study of Existential Risk]]
* [[Global catastrophic risk]]
* [[Leverhulme Centre for the Future of Intelligence]]
* [[Machine Intelligence Research Institute]]
* [[The Precipice: Existential Risk and the Future of Humanity]]

== References ==
{{Reflist}}

== External links ==
* {{Official website|http://futureoflife.org}}

{{Effective altruism}}
{{Global catastrophic risks}}
{{Existential risk from artificial intelligence}}

[[Category:Futures studies organizations]]
[[Category:2014 establishments in Massachusetts]]
[[Category:Research institutes established in 2014]]
[[Category:Artificial intelligence associations]]
[[Category:Transhumanist organizations]]
[[Category:Existential risk organizations]]
[[Category:Existential risk from artificial general intelligence]]
[[Category:Organizations associated with effective altruism]]