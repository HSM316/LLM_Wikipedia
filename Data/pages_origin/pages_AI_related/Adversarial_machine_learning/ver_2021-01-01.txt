{{short description|Research field that lies at the intersection of machine learning and computer security}}
{{Distinguish|Generative adversarial network}}
{{excessive citations|date=September 2018}}
'''Adversarial machine learning''' is a [[machine learning]] technique that attempts to fool models by supplying deceptive input.<ref>{{Cite book|chapter=Timing Attacks on Machine Learning: State of the Art
|last1=Kianpour|first1=Mazaher|last2=Wen|first2=Shao-Fang|title=Intelligent Systems and Applications
|series=Advances in Intelligent Systems and Computing
|date=2020|volume=1037
|pages=111–125
|language=en|doi=10.1007/978-3-030-29516-5_10|isbn=978-3-030-29515-8
}}</ref><ref>{{Cite arxiv|title=Adversarial Machine Learning at Scale|last1=Bengio|first1=Samy|last2=Goodfellow|first2=Ian J.|date=2017|last3=Kurakin|first3=Alexey|class=cs.CV|eprint=1611.01236}}</ref><ref name="LimTaeihagh2019">{{Cite journal|last1=Lim|first1=Hazel Si Min|last2=Taeihagh|first2=Araz|date=2019|title=Algorithmic Decision-Making in AVs: Understanding Ethical and Technical Concerns for Smart Cities|journal=Sustainability|language=en|volume=11|issue=20|pages=5791|doi=10.3390/su11205791|arxiv=1910.13122|bibcode=2019arXiv191013122L|s2cid=204951009}}</ref> The most common reason is to cause a malfunction in a machine learning model.

Most machine learning techniques were designed to work on specific problem sets in which the training and test data are generated from the same statistical distribution ([[Independent and identically distributed random variables|IID]]). When those models are applied to the real world, adversaries may supply data that violates that statistical assumption. This data may be arranged to exploit specific vulnerabilities and compromise the results.<ref name="LimTaeihagh2019" /><ref name="GoodfellowMcDaniel2018">{{cite journal |last1=Goodfellow |first1=Ian |last2=McDaniel |first2=Patrick |last3=Papernot |first3=Nicolas |title=Making machine learning robust against adversarial inputs |journal=Communications of the ACM |date=25 June 2018 |volume=61 |issue=7 |pages=56–66 |doi=10.1145/3134599|url= https://cacm.acm.org/magazines/2018/7/229030-making-machine-learning-robust-against-adversarial-inputs/fulltex |access-date=2018-12-13 |issn=0001-0782|language=en|doi-access=free }}</ref>

== History ==
In ''[[Snow Crash]]'' (1992), the author offered scenarios of technology that was vulnerable to an adversarial attack. In ''[[Zero History]]'' (2010), a character dons a t-shirt decorated in a way that renders him invisible to electronic surveillance.<ref>{{cite news |last1=Vincent |first1=James |title=Magic AI: these are the optical illusions that trick, fool, and flummox computers |url=https://www.theverge.com/2017/4/12/15271874/ai-adversarial-images-fooling-attacks-artificial-intelligence |accessdate=27 March 2020 |work=The Verge |date=12 April 2017 |language=en}}</ref>

In 2004, Nilesh Dalvi and others noted that [[linear classifier]]s used in [[Email filtering|spam filters]] could be defeated by simple "[[evasion (network security)|evasion]] attacks" as spammers inserted "good words" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within "image spam" in order to defeat [[Optical character recognition|OCR]]-based filters.) In 2006, Marco Barreno and others published "Can Machine Learning Be Secure?", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as [[support vector machine]]s and [[neural networks]]) might be robust to adversaries. In 2012, [[Deep learning|deep neural networks]] began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries.<ref name=":0">{{cite journal |last1=Biggio |first1=Battista |last2=Roli |first2=Fabio |title=Wild patterns: Ten years after the rise of adversarial machine learning |journal=Pattern Recognition |date=December 2018 |volume=84 |pages=317–331 |doi=10.1016/j.patcog.2018.07.023|arxiv=1712.03141 |s2cid=207324435 }}</ref>

Recently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noises.<ref>{{cite arxiv |eprint=1607.02533|last1=Kurakin|first1=Alexey|last2=Goodfellow|first2=Ian|last3=Bengio|first3=Samy|title=Adversarial examples in the physical world|year=2016|class=cs.CV}}</ref><ref>Gupta, Kishor Datta, Dipankar Dasgupta, and Zahid Akhtar. "Applicability issues of Evasion-Based Adversarial Attacks and Mitigation Techniques." 2020 IEEE Symposium Series on Computational Intelligence (SSCI). 2020.</ref> For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality.

=== Examples ===
Examples include attacks in [[spam filtering]], where spam messages are obfuscated through the misspelling of “bad” words or the insertion of “good” words;<ref name="BiggioFumera2010">{{cite journal|last1=Biggio|first1=Battista|last2=Fumera|first2=Giorgio|last3=Roli|first3=Fabio|title=Multiple classifier systems for robust classifier design in adversarial environments|journal=International Journal of Machine Learning and Cybernetics|volume=1|issue=1–4|year=2010|pages=27–41|issn=1868-8071|doi=10.1007/s13042-010-0007-7|s2cid=8729381|url=http://pralab.diee.unica.it/en/node/671}}</ref><ref name="Adversarial Machine Learning_18A">{{cite journal |last1=Brückner |first1=Michael |last2=Kanzow |first2=Christian |last3=Scheffer |first3=Tobias |title=Static Prediction Games for Adversarial Learning Problems |journal=Journal of Machine Learning Research |date=2012 |volume=13 |issue=Sep |pages=2617–2654 |url=http://www.jmlr.org/papers/volume13/brueckner12a/brueckner12a.pdf |issn=1533-7928}}</ref> attacks in [[computer security]], such as obfuscating malware code within [[network packet]]s or to mislead signature detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user;<ref name="RodriguesLing2009">{{cite journal |last1=Rodrigues |first1=Ricardo N. |last2=Ling |first2=Lee Luan |last3=Govindaraju |first3=Venu |title=Robustness of multimodal biometric fusion methods against spoof attacks |journal=Journal of Visual Languages & Computing |date=1 June 2009 |volume=20 |issue=3 |pages=169–179 |doi=10.1016/j.jvlc.2009.01.010 |url=http://cubs.cedar.buffalo.edu/images/pdf/pub/robustness-of-multimodal-biometric-fusion-methods-against-spoof-attacks.pdf |language=en |issn=1045-926X}}</ref> or to compromise users' template galleries that adapt to updated traits over time.

Researchers showed that by changing only one-pixel it was possible to fool deep learning algorithms.<ref>{{Cite journal | arxiv=1710.08864| doi=10.1109/TEVC.2019.2890858| title=One Pixel Attack for Fooling Deep Neural Networks| year=2019| last1=Su| first1=Jiawei| last2=Vargas| first2=Danilo Vasconcellos| last3=Sakurai| first3=Kouichi| journal=IEEE Transactions on Evolutionary Computation| volume=23| issue=5| pages=828–841| s2cid=2698863}}</ref><ref>{{cite journal |last1=Su |first1=Jiawei |last2=Vargas |first2=Danilo Vasconcellos |last3=Sakurai |first3=Kouichi |title=One Pixel Attack for Fooling Deep Neural Networks |journal=IEEE Transactions on Evolutionary Computation |date=October 2019 |volume=23 |issue=5 |pages=828–841 |doi=10.1109/TEVC.2019.2890858 |arxiv=1710.08864 |s2cid=2698863 |issn=1941-0026}}</ref> Others [[3-D print]]ed a toy turtle with a texture engineered to make Google's object detection [[Artificial intelligence|AI]] classify it as a rifle regardless of the angle from which the turtle was viewed.<ref>{{cite news|title=Single pixel change fools AI programs|url=https://www.bbc.com/news/technology-41845878|accessdate=12 February 2018|work=BBC News|date=3 November 2017}}</ref> Creating the turtle required only low-cost commercially available 3-D printing technology.<ref>{{cite arxiv |eprint=1707.07397|last1=Athalye|first1=Anish|last2=Engstrom|first2=Logan|last3=Ilyas|first3=Andrew|last4=Kwok|first4=Kevin|title=Synthesizing Robust Adversarial Examples|year=2017|class=cs.CV}}</ref>

A machine-tweaked image of a dog was shown to look like a cat to both computers and humans.<ref>{{cite news|title=AI Has a Hallucination Problem That's Proving Tough to Fix|url=https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/|accessdate=10 March 2018|work=WIRED|date=2018}}</ref> A 2019 study reported that humans can guess how machines will classify adversarial images.<ref>{{cite journal |doi=10.1038/s41467-019-08931-6 |doi-access=free|title=Humans can decipher adversarial images|year=2019|last1=Zhou|first1=Zhenglong|last2=Firestone|first2=Chaz|journal=Nature Communications|volume=10|issue=1|page=1334|pmid=30902973|pmc=6430776|arxiv=1809.04120|bibcode=2019NatCo..10.1334Z}}</ref> Researchers discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle classified it as a merge or speed limit sign.<ref name="LimTaeihagh2019" /><ref>{{Cite web|url=https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa|title=Breaking neural networks with adversarial attacks - Towards Data Science|last=Jain|first=Anant|date=2019-02-09|website=Medium|language=en|access-date=2019-07-15}}</ref><ref>{{Cite web|url=https://spectrum.ieee.org/cars-that-think/transportation/sensors/slight-street-sign-modifications-can-fool-machine-learning-algorithms|title=Slight Street Sign Modifications Can Completely Fool Machine Learning Algorithms|last=Ackerman|first=Evan|date=2017-08-04|website=IEEE Spectrum: Technology, Engineering, and Science News|language=en|access-date=2019-07-15}}</ref>

[[McAfee]] attacked [[Tesla, Inc.|Tesla]]'s former [[Mobileye]] system, fooling it into driving 50&nbsp;mph over the speed limit, simply by adding a two-inch strip of black tape to a speed limit sign.<ref>{{cite news |title=A Tiny Piece of Tape Tricked Teslas Into Speeding Up 50 MPH |url=https://www.wired.com/story/tesla-speed-up-adversarial-example-mgm-breach-ransomware/ |accessdate=11 March 2020 |work=Wired |date=2020 |language=en}}</ref><ref>{{Cite web|url=https://securingtomorrow.mcafee.com/blogs/other-blogs/mcafee-labs/model-hacking-adas-to-pave-safer-roads-for-autonomous-vehicles|title=Model Hacking ADAS to Pave Safer Roads for Autonomous Vehicles|date=2020-02-19|website=McAfee Blogs|language=en-US|access-date=2020-03-11}}</ref>

Adversarial patterns on glasses or clothing designed to deceive facial-recognition systems or license-plate readers, have led to a niche industry of "stealth streetwear".<ref>{{cite news |last1=Seabrook |first1=John |title=Dressing for the Surveillance Age |url=https://www.newyorker.com/magazine/2020/03/16/dressing-for-the-surveillance-age |accessdate=5 April 2020 |work=The New Yorker |date=2020 |language=en}}</ref>

An adversarial attack on a neural network can allow an attacker to inject algorithms into the target system.<ref name="nature why">{{cite journal|last1=Heaven|first1=Douglas|title=Why deep-learning AIs are so easy to fool|date=October 2019|journal=Nature|volume=574|issue=7777|pages=163–166|language=en|doi=10.1038/d41586-019-03013-5|pmid=31597977|bibcode=2019Natur.574..163H|doi-access=free}}</ref> Researchers can also create adversarial audio inputs to disguise commands to intelligent assistants in benign-seeming audio.<ref>{{cite journal|last1=Hutson|first1=Matthew|date=10 May 2019|title=AI can now defend itself against malicious messages hidden in speech|journal=Nature|doi=10.1038/d41586-019-01510-1|pmid=32385365}}</ref>

Clustering algorithms are used in security applications. Malware and [[computer viruses|computer virus]] analysis aims to identify malware families, and to generate specific detection signatures.<ref name="Adversarial Machine Learning_42A">D. B. Skillicorn. "Adversarial knowledge discovery". IEEE Intelligent Systems, 24:54–61, 2009.</ref><ref name="Adversarial Machine Learning_46A">B. Biggio, G. Fumera, and F. Roli. "[http://pralab.diee.unica.it/en/node/1103 Pattern recognition systems under attack: Design issues and research challenges]". Int'l J. Patt. Recogn. Artif. Intell., 28(7):1460002, 2014.</ref>

==Attack Modalities==
===Taxonomy===
Attacks against (supervised) machine learning algorithms have been categorized along three primary axes:<ref name="Adversarial Machine Learning_2">{{Cite journal|url=https://link.springer.com/content/pdf/10.1007/s10994-010-5188-5.pdf|doi = 10.1007/s10994-010-5188-5|title = The security of machine learning|year = 2010|last1 = Barreno|first1 = Marco|last2 = Nelson|first2 = Blaine|last3 = Joseph|first3 = Anthony D.|last4 = Tygar|first4 = J. D.|journal = Machine Learning|volume = 81|issue = 2|pages = 121–148|s2cid = 2304759}}</ref> influence on the classifier, the security violation and their specificity.

* Classifier influence: An attack can influence the classifier by disrupting the classification phase. This may be preceded by an exploration phase to identify vulnerabilities. The attacker's capabilities might restricted by the presence of data manipulation constraints.<ref>{{cite book |last=Sikos |first=Leslie F. |title=AI in Cybersecurity |volume=151 |location=Cham |publisher=Springer |isbn=978-3-319-98841-2 |doi=10.1007/978-3-319-98842-9 | page = 50 |series=Intelligent Systems Reference Library |year=2019 }}</ref>
*Security violation: An attack can supply malicious data that gets classified as legitimate. Malicious data supplied during training can cause legitimate data to be rejected after training.
*Specificity: A targeted attack attempts to allow a specific intrusion/disruption. Alternatively, an indiscriminate attack creates general mayhem.
This taxonomy has been extended into a more comprehensive threat model that allows explicit assumptions about the adversary's goal, knowledge of the attacked system, capability of manipulating the input data/system components, and on attack strategy.<ref name="Adversarial Machine Learning_4A">B. Biggio, G. Fumera, and F. Roli. "[http://pralab.diee.unica.it/en/node/657 Security evaluation of pattern classifiers under attack] {{Webarchive|url=https://web.archive.org/web/20180518055115/http://pralab.diee.unica.it/en/node/657|date=2018-05-18}}". IEEE Transactions on Knowledge and Data Engineering, 26(4):984–996, 2014.</ref><ref name="Adversarial Machine Learning_5A">{{cite book|last1=Biggio|first1=Battista|title=Support Vector Machines Applications|last2=Corona|first2=Igino|last3=Nelson|first3=Blaine|last4=Rubinstein|first4=Benjamin I. P.|last5=Maiorca|first5=Davide|last6=Fumera|first6=Giorgio|last7=Giacinto|first7=Giorgio|last8=Roli|first8=Fabio|date=2014|publisher=Springer International Publishing|isbn=978-3-319-02300-7|pages=105–153|language=en|chapter=Security Evaluation of Support Vector Machines in Adversarial Environments|arxiv=1401.7727|doi=10.1007/978-3-319-02300-7_4|s2cid=18666561}}</ref> Two of the main attack scenarios are:

=== Strategies ===

====Evasion ====
Evasion attacks<ref name="Adversarial Machine Learning_4A" /><ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_36A">B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph, S. J. Lee, S. Rao, and J. D. Tygar. "[http://www.jmlr.org/papers/volume13/nelson12a/nelson12a.pdf Query strategies for evading convex-inducing classifiers]". J. Mach. Learn. Res., 13:1293–1332, 2012</ref> are the most prevalent type of attack. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and [[malware]]. Samples are modified to evade detection; that is, to be classified as legitimate. This does not involve influence over the training data. A clear example of evasion is [[Image spam|image-based spam]] in which the spam content is embedded within an attached image to evade textual analysis by anti-spam filters. Another example of evasion is given by spoofing attacks against biometric verification systems.<ref name="RodriguesLing2009" />

====Poisoning====
Poisoning is adversarial contamination of training data. Machine learning systems can be re-trained using data collected during operations. For instance, [[Intrusion detection system|intrusion detection systems (IDSs)]] are often re-trained using such data. An attacker may poison this data by injecting malicious samples during operation that subsequently disrupt retraining.<ref name="Adversarial Machine Learning_4A" /><ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_2" /><ref name="Adversarial Machine Learning_15A">B. Biggio, B. Nelson, and P. Laskov. "[http://pralab.diee.unica.it/en/node/751 Support vector machines under adversarial label noise]". In Journal of Machine Learning Research - Proc. 3rd Asian Conf. Machine Learning, volume 20, pp. 97–112, 2011.</ref><ref name="Adversarial Machine Learning_29A">M. Kloft and P. Laskov. "[http://www.jmlr.org/papers/volume13/kloft12b/kloft12b.pdf Security analysis of online centroid anomaly detection]". Journal of Machine Learning Research, 13:3647–3690, 2012.</ref><ref>{{Cite web|url=https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db|title=Poisoning attacks on Machine Learning - Towards Data Science|last=Moisejevs|first=Ilja|date=2019-07-15|website=Medium|language=en|access-date=2019-07-15}}</ref>

==== Model Stealing ====
Model stealing (also called model extraction) involves an adversary probing a black box machine learning system in order to either reconstruct the model or extract the data it was trained on.<ref>{{Cite web|date=2020-04-06|title=How to steal modern NLP systems with gibberish?|url=http://cleverhans.io/2020/04/06/stealing-bert.html|access-date=2020-10-15|website=cleverhans-blog|language=en}}</ref><ref name=":1">{{Cite journal|last1=Wang|first1=Xinran|last2=Xiang|first2=Yu|last3=Gao|first3=Jun|last4=Ding|first4=Jie|date=2020-09-13|title=Information Laundering for Model Privacy|url=http://arxiv.org/abs/2009.06112|journal=arXiv:2009.06112 [cs, math]|arxiv=2009.06112}}</ref>  This can cause issues when either the training data or the model itself is sensitive and confidential. For example, model stealing could be used to extract a proprietary stock trading model which the adversary could then use for their own financial benefit.

== Specific Attacks Types ==
There are a large variety of different adversarial attacks that can be used against machine learning systems. Many of these work on both [[deep learning]] systems as well as traditional machine learning models such as [[Support vector machine|SVMs]]<ref>{{cite arxiv|last1=Biggio|first1=Battista|last2=Nelson|first2=Blaine|last3=Laskov|first3=Pavel|date=2013-03-25|title=Poisoning Attacks against Support Vector Machines|class=cs.LG|eprint=1206.6389}}</ref> and  [[linear regression]].<ref>{{Cite journal|last1=Jagielski|first1=Matthew|last2=Oprea|first2=Alina|last3=Biggio|first3=Battista|last4=Liu|first4=Chang|last5=Nita-Rotaru|first5=Cristina|last6=Li|first6=Bo|date=May 2018|title=Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning|url=http://dx.doi.org/10.1109/sp.2018.00057|journal=2018 IEEE Symposium on Security and Privacy (SP)|pages=19–35|publisher=IEEE|doi=10.1109/sp.2018.00057|arxiv=1804.00308|isbn=978-1-5386-4353-2|s2cid=4551073}}</ref> A high level sample of these attack types include:

* Adversarial Examples<ref>{{Cite web|date=2017-02-24|title=Attacking Machine Learning with Adversarial Examples|url=https://openai.com/blog/adversarial-example-research/|access-date=2020-10-15|website=OpenAI|language=en}}</ref>
* Trojan Attacks / Backdoor Attacks<ref>{{cite arxiv|last1=Gu|first1=Tianyu|last2=Dolan-Gavitt|first2=Brendan|last3=Garg|first3=Siddharth|date=2019-03-11|title=BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain|class=cs.CR|eprint=1708.06733}}</ref>
* Model Inversion<ref>{{Cite journal|last1=Veale|first1=Michael|last2=Binns|first2=Reuben|last3=Edwards|first3=Lilian|date=2018-11-28|title=Algorithms that remember: model inversion attacks and data protection law|journal=Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences|volume=376|issue=2133|doi=10.1098/rsta.2018.0083|issn=1364-503X|pmc=6191664|pmid=30322998|arxiv=1807.04644|bibcode=2018RSPTA.37680083V}}</ref>
* Membership Inference <ref>{{cite arxiv|last1=Shokri|first1=Reza|last2=Stronati|first2=Marco|last3=Song|first3=Congzheng|last4=Shmatikov|first4=Vitaly|date=2017-03-31|title=Membership Inference Attacks against Machine Learning Models|class=cs.CR|eprint=1610.05820}}</ref>

=== Adversarial Examples ===
An adversarial examples refers to specially crafted input which is design to look "normal" to humans but causes misclassification to a machine learning model.  Often, a form of specially designed "noise"  is used to elicit the misclassifications. Below are some current techniques for generating  adversarial examples in the literature (by no means an exhaustive list).

* Fast Gradient Sign Method (FGSM)<ref>{{cite arxiv|last1=Goodfellow|first1=Ian J.|last2=Shlens|first2=Jonathon|last3=Szegedy|first3=Christian|date=2015-03-20|title=Explaining and Harnessing Adversarial Examples|class=stat.ML|eprint=1412.6572}}</ref>
* Projected Gradient Descent (PGD)<ref>{{cite arxiv|last1=Madry|first1=Aleksander|last2=Makelov|first2=Aleksandar|last3=Schmidt|first3=Ludwig|last4=Tsipras|first4=Dimitris|last5=Vladu|first5=Adrian|date=2019-09-04|title=Towards Deep Learning Models Resistant to Adversarial Attacks|class=stat.ML|eprint=1706.06083}}</ref>
* Carlini and Wagner (C&W) attack<ref>{{cite arxiv|last1=Carlini|first1=Nicholas|last2=Wagner|first2=David|date=2017-03-22|title=Towards Evaluating the Robustness of Neural Networks|class=cs.CR|eprint=1608.04644}}</ref>
* Adversarial patch attack<ref>{{cite arxiv|last1=Brown|first1=Tom B.|last2=Mané|first2=Dandelion|last3=Roy|first3=Aurko|last4=Abadi|first4=Martín|last5=Gilmer|first5=Justin|date=2018-05-16|title=Adversarial Patch|class=cs.CV|eprint=1712.09665}}</ref>

== Defenses ==
[[File:Proactive arms race.jpg|thumb|Conceptual representation of the proactive arms race<ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_46A" />]]Researchers have proposed a multi-step approach to protecting machine learning.<ref name=":0" />

* Threat modeling - Formalize the attackers goals and capabilities with respect to the target system.
* Attack simulation - Formalize the optimization problem the attacker tries to solve according to possible attack strategies.
* Attack impact evaluation
* Countermeasure design
* Noise detection (For evasion based attack)<ref>{{Cite arXiv|eprint = 2007.00337|author1 = Kishor Datta Gupta|last2 = Akhtar|first2 = Zahid|last3 = Dasgupta|first3 = Dipankar|title = Determining Sequence of Image Processing Technique (IPT) to Detect Adversarial Attacks|year = 2020|class = cs.CV}}</ref>
*Information laundering - Alter the information received by adversaries (for model stealing attacks)<ref name=":1" />

=== Mechanisms ===
A number of defense mechanisms against evasion, poisoning, and privacy attacks have been proposed, including:

* Secure learning algorithms<ref name="Adversarial Machine Learning_18A" /><ref name="Adversarial Machine Learning_22A">O. Dekel, O. Shamir, and L. Xiao. "[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DekelShXi09.pdf Learning to classify with missing and corrupted features]". Machine Learning, 81:149–178, 2010.</ref><ref name="Adversarial Machine Learning_45A">{{Cite journal|url=https://link.springer.com/content/pdf/10.1007/s10994-010-5199-2.pdf|doi=10.1007/s10994-010-5199-2|title=Mining adversarial patterns via regularized loss minimization|year=2010|last1=Liu|first1=Wei|last2=Chawla|first2=Sanjay|journal=Machine Learning|volume=81|pages=69–83|s2cid=17497168}}</ref>
* Multiple classifier systems<ref name="BiggioFumera2010" /><ref name="Adversarial Machine Learning_10A">B. Biggio, G. Fumera, and F. Roli. "[http://pralab.diee.unica.it/en/node/642 Evade hard multiple classifier systems]". In O. Okun and G. Valentini, editors, Supervised and Unsupervised Ensemble Methods and Their Applications, volume 245 of Studies in Computational Intelligence, pages 15–38. Springer Berlin / Heidelberg, 2009.</ref>
* AI-written algorithms.<ref name="nature why"/>
* AIs that explore the training environment; for example, in image recognition, actively navigating a 3D environment rather than passively scanning a fixed set of 2D images.<ref name="nature why"/>
* Privacy-preserving learning<ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_41A">B. I. P. Rubinstein, P. L. Bartlett, L. Huang, and N. Taft. "[https://arxiv.org/abs/0911.5708 Learning in a large function space: Privacy- preserving mechanisms for svm learning]". Journal of Privacy and Confidentiality, 4(1):65–100, 2012.</ref>
* Ladder algorithm for [[Kaggle]]-style competitions
* Game theoretic models<ref name="feature_select">M. Kantarcioglu, B. Xi, C. Clifton. [http://www.stat.purdue.edu/~xbw/research/BoweiXi.AdversarialClassification2010.pdf "Classifier Evaluation and Attribute Selection against Active Adversaries"]. Data Min. Knowl. Discov., 22:291–335, January 2011.</ref><ref>{{Cite journal|last1=Chivukula|first1=Aneesh|last2=Yang|first2=Xinghao|last3=Liu|first3=Wei|last4=Zhu|first4=Tianqing|last5=Zhou|first5=Wanlei|date=2020|title=Game Theoretical Adversarial Deep Learning with Variational Adversaries|url=https://ieeexplore.ieee.org/document/8986751|journal=IEEE Transactions on Knowledge and Data Engineering|pages=1|doi=10.1109/TKDE.2020.2972320|issn=1558-2191}}</ref><ref>{{Cite journal|last1=Chivukula|first1=Aneesh Sreevallabh|last2=Liu|first2=Wei|date=2019|title=Adversarial Deep Learning Models with Multiple Adversaries|url=https://ieeexplore.ieee.org/document/8399545|journal=IEEE Transactions on Knowledge and Data Engineering|volume=31|issue=6|pages=1066–1079|doi=10.1109/TKDE.2018.2851247|s2cid=67024195|issn=1558-2191}}</ref>
* Sanitizing training data
* Adversarial training<ref>{{cite arxiv|last1=Goodfellow|first1=Ian J.|last2=Shlens|first2=Jonathon|last3=Szegedy|first3=Christian|date=2015-03-20|title=Explaining and Harnessing Adversarial Examples|class=stat.ML|eprint=1412.6572}}</ref>
* Backdoor detection algorithms<ref>{{Cite web|title=TrojAI|url=https://www.iarpa.gov/index.php/research-programs/trojai|access-date=2020-10-14|website=www.iarpa.gov}}</ref>

==Software==
Available software libraries, mainly for testing and research.
* [http://pralab.diee.unica.it/en/AdversariaLib AdversariaLib] - includes implementation of evasion attacks
* [https://github.com/vu-aml/adlib AdLib] - Python library with a scikit-style interface which includes implementations of a number of published evasion attacks and defenses
* [http://pralab.diee.unica.it/en/ALFASVMLib AlfaSVMLib] - Adversarial Label Flip Attacks against Support Vector Machines<ref name="Adversarial Machine Learning_49A">H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli. "[http://pralab.diee.unica.it/en/node/1104 Support vector machines under adversarial label contamination]". Neurocomputing, Special Issue on Advances in Learning with Label Noise, In Press.</ref>
* [http://pralab.diee.unica.it/en/BattistaBiggio/Code Poisoning Attacks against Support Vector Machines], and [http://pralab.diee.unica.it/en/BattistaBiggio/Code Attacks against Clustering Algorithms]
* [https://github.com/cchio/deep-pwning deep-pwning] - Metasploit for deep learning which currently has attacks on deep neural networks using [[TensorFlow|Tensorflow]].<ref>{{Cite web|url=https://github.com/cchio/deep-pwning|title=cchio/deep-pwning|website=GitHub|access-date=2016-08-08}}</ref> This framework currently updates to maintain compatibility with the latest versions of Python.
* [https://github.com/tensorflow/cleverhans Cleverhans] - A Tensorflow Library to test existing deep learning models versus known attacks
*[https://github.com/bethgelab/foolbox foolbox] - Python Library to create adversarial examples, implements multiple attacks
*[https://gitlab.com/secml/secml SecML] - Python Library for secure and explainable machine learning - includes implementation of a wide range of ML and attack algorithms, support for dense and sparse data, multiprocessing, visualization tools.
*[https://github.com/trojai/trojai TrojAI]- Python Library for generating backdoored and trojaned models at scale for research into trojan detection
*[https://github.com/Trusted-AI/adversarial-robustness-toolbox Adversarial Robustness Toolkit (IBM ART)] - Python Library for Machine Learning Security 
*[https://github.com/BorealisAI/advertorch Advertorch] - Python toolbox for adversarial robustness research whose main functions are implemented in [[PyTorch]]

==See also==
* [[Pattern recognition]]

==References==
{{reflist}}

== External links ==
* NIPS 2007 Workshop on [https://web.archive.org/web/20120108072159/http://nips.cc/Conferences/2007/Program/event.php?ID=615 Machine Learning in Adversarial Environments for Computer Security]
* {{cite journal |doi=10.1007/s10994-010-5207-6|title=Machine learning in adversarial environments|year=2010|last1=Laskov|first1=Pavel|last2=Lippmann|first2=Richard|journal=Machine Learning|volume=81|issue=2|pages=115–119|s2cid=12567278}}
* Dagstuhl Perspectives Workshop on "[http://www.dagstuhl.de/en/program/calendar/semhp/?semnr=12371 Machine Learning Methods for Computer Security]"
* Workshop on [http://aisec.cc Artificial Intelligence and Security], (AISec) Series

{{Differentiable computing}}

[[Category:Machine learning]]
[[Category:Computer security]]