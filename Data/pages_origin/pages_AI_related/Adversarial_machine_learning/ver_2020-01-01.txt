{{Distinguish|Generative adversarial network}}
{{multiple issues|
{{cleanup reorganize|date=September 2018}}
{{excessive citations|date=September 2018}}
}}
'''Adversarial machine learning''' is a technique employed in the field of machine learning which attempts to fool models through malicious input.<ref>{{Cite web|url=https://ai.google/research/pubs/pub45816|title=Adversarial Machine Learning at Scale|last=Bengio|first=Samy|last2=Goodfellow|first2=Ian J.|date=2017|website=Google AI|language=en|access-date=2018-12-13|last3=Kurakin|first3=Alexey}}</ref><ref name=":0">{{Cite journal|last=Lim|first=Hazel Si Min|last2=Taeihagh|first2=Araz|date=2019|title=Algorithmic Decision-Making in AVs: Understanding Ethical and Technical Concerns for Smart Cities|url=https://www.mdpi.com/2071-1050/11/20/5791|journal=Sustainability|language=en|volume=11|issue=20|pages=5791|doi=10.3390/su11205791|via=}}</ref> This technique can be applied for a variety of reasons, the most common being to attack or cause a malfunction in standard machine learning models.

[[Machine learning]] techniques were originally designed for stationary and benign environments in which the training and test data are assumed to be generated from the same statistical distribution. However, when those models are implemented in the real world, the presence of intelligent and adaptive adversaries may violate that statistical assumption to some degree, depending on the adversary. This technique shows how a malicious adversary can surreptitiously manipulate the input data so as to exploit specific vulnerabilities of [[Machine learning|learning algorithms]] and compromise the security of the machine learning system.<ref name=":0" /><ref>{{Cite web|url=https://cacm.acm.org/magazines/2018/7/229030-making-machine-learning-robust-against-adversarial-inputs/fulltext|title=Making Machine Learning Robust Against Adversarial Inputs|last=Papernot|first=Ian Goodfellow, Patrick McDaniel, Nicolas|website=cacm.acm.org|language=en|access-date=2018-12-13}}</ref>

== Examples ==
Examples include attacks in [[spam filtering]], where spam messages are obfuscated through the misspelling of “bad” words or the insertion of “good” words;<ref name="Adversarial Machine Learning_12A">B. Biggio, G. Fumera, and F. Roli. "[http://pralab.diee.unica.it/en/node/671 Multiple classifier systems for robust classifier design in adversarial environments]". International Journal of Machine Learning and Cybernetics, 1(1):27–41, 2010.</ref><ref name="Adversarial Machine Learning_18A">M. Bruckner, C. Kanzow, and T. Scheffer. "[http://www.jmlr.org/papers/volume13/brueckner12a/brueckner12a.pdf Static prediction games for adversarial learning problems]". J. Mach. Learn. Res., 13:2617–2654, 2012.</ref> attacks in [[computer security]], such as obfuscating malware code within [[network packet]]s or to mislead signature detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user;<ref name="Adversarial Machine Learning_44A">R. N. Rodrigues, L. L. Ling, and V. Govindaraju. "[http://cubs.cedar.buffalo.edu/images/pdf/pub/robustness-of-multimodal-biometric-fusion-methods-against-spoof-attacks.pdf Robustness of multimodal biometric fusion methods against spoof attacks]". J. Vis. Lang. Comput., 20(3):169–179, 2009.</ref> or to compromise users' template galleries that adapt to updated traits over time.

In 2017, researchers at the [[Massachusetts Institute of Technology]] [[3-D print]]ed a toy turtle with a texture engineered to make Google's object detection [[Artificial intelligence|AI]] classify it as a rifle regardless of the angle from which the turtle was viewed.<ref>{{cite news|title=Single pixel change fools AI programs|url=https://www.bbc.com/news/technology-41845878|accessdate=12 February 2018|work=BBC News|date=3 November 2017}}</ref> Creating the turtle required only low-cost commercially available 3-D printing technology.<ref>Athalye, A., & Sutskever, I. (2017). [https://arxiv.org/pdf/1707.07397.pdf Synthesizing robust adversarial examples]. arXiv preprint arXiv:1707.07397.</ref> In 2018, [[Google Brain]] published a machine-tweaked image of a dog that looked like a cat both to computers and to humans.<ref>{{cite news|title=AI Has a Hallucination Problem That's Proving Tough to Fix|url=https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/|accessdate=10 March 2018|work=WIRED|date=2018}}</ref> A 2019 study from [[Johns Hopkins University]] showed that, when asked, humans can guess how machines will misclassify adversarial images.<ref>Zhou, Z., & Firestone, C. (2019). [https://nature.com/articles/s41467-019-08931-6 Humans can decipher adversarial images]. Nature Communications, 10, 1334.</ref> Researchers have also discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle will classify it as a merge or speed limit sign.<ref name=":0" /><ref>{{Cite web|url=https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa|title=Breaking neural networks with adversarial attacks - Towards Data Science|last=Jain|first=Anant|date=2019-02-09|website=Medium|language=en|access-date=2019-07-15}}</ref> <ref>{{Cite web|url=https://spectrum.ieee.org/cars-that-think/transportation/sensors/slight-street-sign-modifications-can-fool-machine-learning-algorithms|title=Slight Street Sign Modifications Can Completely Fool Machine Learning Algorithms|last=Ackerman|first=Evan|date=2017-08-04|website=IEEE Spectrum: Technology, Engineering, and Science News|language=en|access-date=2019-07-15}}</ref>

==Security evaluation==
[[File:Reactive arms race.jpg|thumb|Conceptual representation of the reactive arms race<ref name="Adversarial Machine Learning_4A">B. Biggio, G. Fumera, and F. Roli. "[http://pralab.diee.unica.it/en/node/657 Security evaluation of pattern classifiers under attack]". IEEE Transactions on Knowledge and Data Engineering, 26(4):984–996, 2014.</ref><ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_46A" />]]
To understand the security properties of learning algorithms in adversarial settings, the following main issues should be addressed:<ref name="Adversarial Machine Learning_4A" /><ref name="Adversarial Machine Learning_5A">B. Biggio, I. Corona, B. Nelson, B. Rubinstein, D. Maiorca, G. Fumera, G. Giacinto, and F. Roli. "[http://pralab.diee.unica.it/en/node/1047 Security evaluation of support vector machines in adversarial environments]". In Y. Ma and G. Guo, editors, Support Vector Machines Applications, pp. 105–153. Springer, 2014.</ref><ref name="Adversarial Machine Learning_46A">B. Biggio, G. Fumera, and F. Roli. "[http://pralab.diee.unica.it/en/node/1103 Pattern recognition systems under attack: Design issues and research challenges]". Int'l J. Patt. Recogn. Artif. Intell., 28(7):1460002, 2014.</ref><ref name="Adversarial Machine Learning_26A">L. Huang, A. D. Joseph, B. Nelson, B. Rubinstein, and J. D. Tygar. "[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.360.168&rep=rep1&type=pdf Adversarial machine learning]". In 4th ACM Workshop on Artificial Intelligence and Security (AISec 2011), pages 43–57, Chicago, IL, USA, October 2011.</ref>
* identifying potential vulnerabilities of machine learning algorithms during learning and classification
* devising appropriate attacks that correspond to the identified threats and evaluating their impact on the targeted system
* proposing countermeasures to improve the security of machine learning algorithms against the considered attacks

This process amounts to simulating a proactive arms race (instead of a reactive one, as depicted in Figures 1 and 2), where system designers try to anticipate the adversary in order to understand whether there are potential vulnerabilities that should be fixed in advance; for instance, by means of specific countermeasures such as additional features or different learning algorithms. However, proactive approaches are not necessarily superior to reactive ones: under some circumstances, reactive approaches are more suitable for improving system security.<ref name="Adversarial Machine Learning_3A">A. Barth, B. I. P. Rubinstein, M. Sundararajan, J. C. Mitchell, D. Song, and P. L. Bartlett. "[http://www.redpel.com/A%20Learning-Based%20Approach%20to%20Reactive%20Security.pdf A learning-based approach to reactive security]. IEEE Transactions on Dependable and Secure Computing", 9(4):482–493, 2012.</ref>
[[File:Proactive arms race.jpg|thumb|Conceptual representation of the proactive arms race<ref name="Adversarial Machine Learning_4A" /><ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_46A" />]]

==Attacks against machine learning algorithms (supervised)==
The first step of the arms race described above is identifying potential attacks against machine learning algorithms. A substantial amount of work has been done in this direction.<ref name="Adversarial Machine Learning_4A" /><ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_46A" /><ref name="Adversarial Machine Learning_2">M. Barreno, B. Nelson, A. Joseph, and J. Tygar. "[https://link.springer.com/content/pdf/10.1007/s10994-010-5188-5.pdf The security of machine learning]". Machine Learning, 81:121–148, 2010</ref>

===Taxonomy of potential attacks against machine learning===
Attacks against (supervised) machine learning algorithms have been categorized along three primary axes:<ref name="Adversarial Machine Learning_2" /> their influence on the classifier, the security violation they cause, and their specificity.

* ''Attack influence'': An attack can have a causative influence if it aims to introduce vulnerabilities to be exploited at the classification phase by manipulating training data, or an exploratory influence if the attack aims to find and subsequently exploit vulnerabilities at classification phase. The attacker's capabilities might also be influenced by the presence of data manipulation constraints.<ref>{{cite book |last=Sikos |first=Leslie F. |date=2018 |title=AI in Cybersecurity |volume=151 |location=Cham |publisher=Springer |isbn=978-3-319-98841-2 |doi=10.1007/978-3-319-98842-9 | page = 50 |series=Intelligent Systems Reference Library }}</ref>
* ''Security violation'': An attack can cause an integrity violation if it aims to get malicious samples misclassified as legitimate, or it may cause an availability violation if the goal is to increase the wrong classification rate of legitimate samples, making the classifier unusable (e.g., a denial of service).
* ''Attack specificity'': An attack can be targeted if specific samples are considered (e.g. the adversary aims to allow a specific intrusion or wants a given spam email to get past the filter), or indiscriminate.

This taxonomy has been extended into a more comprehensive threat model that allows one to make explicit assumptions on the adversary's goal, knowledge of the attacked system, capability of manipulating the input data and/or the system components, and on the corresponding (potentially, formally-defined) attack strategy.<ref name="Adversarial Machine Learning_4A" /><ref name="Adversarial Machine Learning_5A" /> Two of the main attack scenarios identified according to this threat model are described below.

===Evasion attacks===
Evasion attacks<ref name="Adversarial Machine Learning_4A" /><ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_36A">B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph, S. J. Lee, S. Rao, and J. D. Tygar. "[http://www.jmlr.org/papers/volume13/nelson12a/nelson12a.pdf Query strategies for evading convex-inducing classifiers]". J. Mach. Learn. Res., 13:1293–1332, 2012</ref> are the most prevalent type of attack that may be encountered in adversarial settings during system operation. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware code. In the evasion setting, malicious samples are modified at test time to evade detection; that is, to be misclassified as legitimate. No attacker influence over the training data is assumed. A clear example of evasion is [[Image spam|image-based spam]] in which the spam content is embedded within an attached image to evade the textual analysis performed by anti-spam filters. Another example of evasion is given by spoofing attacks against biometric verification systems.<ref name="Adversarial Machine Learning_44A" />

===Poisoning attacks===
Machine learning algorithms are often re-trained on data collected during operation to adapt to changes in the underlying data distribution. For instance, intrusion detection systems (IDSs) are often re-trained on a set of samples collected during network operation. Within this scenario, an attacker may poison the training data by injecting carefully designed samples to eventually compromise the whole learning process. Poisoning may thus be regarded as an adversarial contamination of the training data. Examples of poisoning attacks against machine learning algorithms including learning in the presence of worst-case adversarial label flips in the training data can be found in the following reference links.<ref name="Adversarial Machine Learning_4A" /><ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_2" /><ref name="Adversarial Machine Learning_15A">B. Biggio, B. Nelson, and P. Laskov. "[http://pralab.diee.unica.it/en/node/751 Support vector machines under adversarial label noise]". In Journal of Machine Learning Research - Proc. 3rd Asian Conf. Machine Learning, volume 20, pp. 97–112, 2011.</ref><ref name="Adversarial Machine Learning_29A">M. Kloft and P. Laskov. "[http://www.jmlr.org/papers/volume13/kloft12b/kloft12b.pdf Security analysis of online centroid anomaly detection]". Journal of Machine Learning Research, 13:3647–3690, 2012.</ref> Adversarial stop signs (stop signs that look normal to the human eye but are classified as non-stop signs by neural networks) are primary examples of poisoning attacks.<ref>{{Cite web|url=https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db|title=Poisoning attacks on Machine Learning - Towards Data Science|last=Moisejevs|first=Ilja|date=2019-07-15|website=Medium|language=en|access-date=2019-07-15}}</ref>

==Attacks against clustering algorithms==
Clustering algorithms have been increasingly adopted in security applications to find dangerous or illicit activities. For instance, clustering of malware and computer viruses aims to identify and categorize different existing malware families, and to generate specific signatures for their detection by anti-viruses or signature-based [[intrusion detection system]]s like Snort.

However, clustering algorithms were not originally devised to deal with deliberate attack attempts that are designed to subvert the clustering process itself. If clustering can be safely adopted in such settings, this remains questionable.<ref name="Adversarial Machine Learning_42A">D. B. Skillicorn. "Adversarial knowledge discovery". IEEE Intelligent Systems, 24:54–61, 2009.</ref>

==Secure learning in adversarial settings==
A number of defense mechanisms against evasion, poisoning, and privacy attacks have been proposed in the field of adversarial machine learning, including:

* The definition of secure learning algorithms<ref name="Adversarial Machine Learning_18A" /><ref name="Adversarial Machine Learning_22A">O. Dekel, O. Shamir, and L. Xiao. "[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DekelShXi09.pdf Learning to classify with missing and corrupted features]". Machine Learning, 81:149–178, 2010.</ref><ref name="Adversarial Machine Learning_45A">W. Liu and S. Chawla. "[https://link.springer.com/content/pdf/10.1007/s10994-010-5199-2.pdf Mining adversarial patterns via regularized loss minimization]". Machine Learning, 81(1):69–83, 2010.</ref>
* The use of multiple classifier systems<ref name="Adversarial Machine Learning_12A" /><ref name="Adversarial Machine Learning_10A">B. Biggio, G. Fumera, and F. Roli. "[http://pralab.diee.unica.it/en/node/642 Evade hard multiple classifier systems]". In O. Okun and G. Valentini, editors, Supervised and Unsupervised Ensemble Methods and Their Applications, volume 245 of Studies in Computational Intelligence, pages 15–38. Springer Berlin / Heidelberg, 2009.</ref>
* The study of privacy-preserving learning<ref name="Adversarial Machine Learning_5A" /><ref name="Adversarial Machine Learning_41A">B. I. P. Rubinstein, P. L. Bartlett, L. Huang, and N. Taft. "[https://arxiv.org/pdf/0911.5708 Learning in a large function space: Privacy- preserving mechanisms for svm learning]". Journal of Privacy and Confidentiality, 4(1):65–100, 2012.</ref>
* Ladder algorithm for Kaggle-style competitions
* Game theoretic models for adversarial machine learning and data mining<ref name="feature_select">M. Kantarcioglu, B. Xi, C. Clifton. [http://www.stat.purdue.edu/~xbw/research/BoweiXi.AdversarialClassification2010.pdf "Classifier Evaluation and Attribute Selection against Active Adversaries"]. Data Min. Knowl. Discov., 22:291–335, January 2011.</ref>
* Sanitizing training data from adversarial poisoning attacks

==Software==
Some software libraries are available, mainly for testing purposes and research.
* [http://pralab.diee.unica.it/en/AdversariaLib AdversariaLib] - includes implementation of evasion attacks
* [https://github.com/vu-aml/adlib AdLib] - Python library with a scikit-style interface which includes implementations of a number of published evasion attacks and defenses
* [http://pralab.diee.unica.it/en/ALFASVMLib AlfaSVMLib] - Adversarial Label Flip Attacks against Support Vector Machines<ref name="Adversarial Machine Learning_49A">H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli. "[http://pralab.diee.unica.it/en/node/1104 Support vector machines under adversarial label contamination]". Neurocomputing, Special Issue on Advances in Learning with Label Noise, In Press.</ref>
* [http://pralab.diee.unica.it/en/BattistaBiggio/Code Poisoning Attacks against Support Vector Machines], and [http://pralab.diee.unica.it/en/BattistaBiggio/Code Attacks against Clustering Algorithms]
* [https://github.com/cchio/deep-pwning deep-pwning] - Metasploit for deep learning which currently has attacks on deep neural networks using [[TensorFlow|Tensorflow]].<ref>{{Cite web|url=https://github.com/cchio/deep-pwning|title=cchio/deep-pwning|website=GitHub|access-date=2016-08-08}}</ref>  This framework currently updates to maintain compatibility with the latest versions of Python.
* [https://github.com/tensorflow/cleverhans Cleverhans] - A Tensorflow Library to test existing deep learning models versus known attacks
*[https://github.com/bethgelab/foolbox foolbox] - Python Library to create adversarial examples, implements multiple attacks
*[https://gitlab.com/secml/secml SecML] - Python Library for secure and explainable machine learning - includes implementation of a wide range of ML and attack algorithms, support for dense and sparse data, multiprocessing, visualization tools.
 
===Past events===
* NIPS 2007 Workshop on [https://web.archive.org/web/20120108072159/http://nips.cc/Conferences/2007/Program/event.php?ID=615 Machine Learning in Adversarial Environments for Computer Security]
* Special Issue on [https://link.springer.com/article/10.1007%2Fs10994-010-5207-6 "Machine Learning in Adversarial Environments"] in the journal of Machine Learning
* Dagstuhl Perspectives Workshop on "[http://www.dagstuhl.de/en/program/calendar/semhp/?semnr=12371 Machine Learning Methods for Computer Security]"<ref name="Adversarial Machine Learning_27A">A. D. Joseph, P. Laskov, F. Roli, J. D. Tygar, and B. Nelson. "[http://drops.dagstuhl.de/opus/volltexte/2013/4356/pdf/dagman-v003-i001-p001-12371.pdf Machine Learning Methods for Computer Security]" (Dagstuhl Perspectives Workshop 12371). Dagstuhl Manifestos, 3(1):1–30, 2013.</ref>
* Workshop on [http://aisec.cc Artificial Intelligence and Security], (AISec) Series

==See also==
* [[Machine learning]]
* [[Pattern recognition]]

==References==
{{reflist|30em}}

[[Category:Machine learning]]
[[Category:Computer security]]