{{short description|Type of neural network}}
{{About|a technique used in machine learning||Highway}}

In [[machine learning]], the '''Highway Network''' was the first working very deep [[feedforward neural network]] with hundreds of layers, much deeper than previous [[artificial neural network]]s.<ref name="highway2015" /><ref name="highway2015neurips" /><ref name="mostcited2021" />
It uses skip connections modulated by learned gating mechanisms to regulate information flow, inspired by [[Long short-term memory|Long Short-Term Memory]] (LSTM) [[recurrent neural networks]].<ref name="lstm1997">{{Cite journal 
 | author = Sepp Hochreiter
 | author2 = Jürgen Schmidhuber
 | title = Long short-term memory
 | journal = [[Neural Computation (journal)|Neural Computation]]
 | volume = 9
 | issue = 8
 | pages = 1735–1780
 | year = 1997
 | url = https://www.researchgate.net/publication/13853244
 | doi=10.1162/neco.1997.9.8.1735
 | pmid=9377276
| s2cid = 1915014
 | author2-link = Jürgen Schmidhuber
 | author-link = Sepp Hochreiter
 }}</ref><ref name="lstm2000">{{Cite journal 
 | author = Felix A. Gers
 | author2 = Jürgen Schmidhuber
 | author3 = Fred Cummins
 | title = Learning to Forget: Continual Prediction with LSTM
 | journal = [[Neural Computation (journal)|Neural Computation]]
 | volume = 12
 | issue = 10
 | pages = 2451–2471
 | year = 2000
 | doi=10.1162/089976600300015015
| pmid = 11032042
 | citeseerx = 10.1.1.55.5709
 | s2cid = 11598600
 }}</ref>
The advantage of a Highway Network over the common deep neural networks is that it solves or partially prevents the [[vanishing gradient problem]],<ref name="hochreiter1991">{{cite thesis 
|url=http://www.bioinf.jku.at/publications/older/3804.pdf
|degree=diploma 
|first=Sepp
|last=Hochreiter
|title=Untersuchungen zu dynamischen neuronalen Netzen
|publisher=Technical University Munich, Institute of Computer Science, advisor: J. Schmidhuber
|year=1991}}</ref> thus leading to easier to optimize neural networks.
The gating mechanisms facilitate information flow across many layers ("information highways").<ref name="highway2015">{{cite arXiv|last1=Srivastava|first1=Rupesh Kumar|last2=Greff|first2=Klaus|last3=Schmidhuber|first3=Jürgen|title=Highway Networks|eprint=1505.00387|date=2 May 2015|class=cs.LG}}</ref><ref name="highway2015neurips">{{cite journal|last1=Srivastava|first1=Rupesh K|last2=Greff|first2=Klaus|last3=Schmidhuber|first3=Juergen|title=Training Very Deep Networks|journal=Advances in Neural Information Processing Systems 28|date=2015|volume=28|pages=2377–2385|url=http://papers.nips.cc/paper/5850-training-very-deep-networks|publisher=Curran Associates, Inc.}}</ref>

Highway Networks have been used as part of [[Semantic analysis (machine learning)|text sequence labeling]] and [[speech recognition]] tasks.<ref>{{cite arXiv|last1=Liu|first1=Liyuan|last2=Shang|first2=Jingbo|last3=Xu|first3=Frank F.|last4=Ren|first4=Xiang|last5=Gui|first5=Huan|last6=Peng|first6=Jian|last7=Han|first7=Jiawei|title=Empower Sequence Labeling with Task-Aware Neural Language Model|eprint=1709.04109|date=12 September 2017|class=cs.CL}}</ref><ref>{{cite arXiv|last1=Kurata|first1=Gakuto|last2=Ramabhadran|first2=Bhuvana|last3=Saon|first3=George|last4=Sethy|first4=Abhinav|title=Language Modeling with Highway LSTM|eprint=1709.06436|date=19 September 2017|class=cs.CL}}</ref>
An open-gated or gateless Highway Network variant called [[Residual neural network]]<ref name="resnet2015">{{Cite conference|last1=He|first1=Kaiming|last2=Zhang|first2=Xiangyu|last3=Ren|first3=Shaoqing|last4=Sun|first4=Jian|date=2016|title=Deep Residual Learning for Image Recognition|url=https://ieeexplore.ieee.org/document/7780459|journal=2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)|location=Las Vegas, NV, USA|publisher=IEEE|pages=770–778|arxiv=1512.03385|doi=10.1109/CVPR.2016.90|isbn=978-1-4673-8851-1}}</ref> was used to win the ImageNet 2015 competition. This has become the most cited neural network of the 21st century.<ref name="mostcited2021">{{Cite news| last=Schmidhuber |first=Jürgen |url=https://people.idsia.ch/~juergen/most-cited-neural-nets.html|title=The most cited neural networks all build on work done in my labs|date=2021|work=AI Blog| location=IDSIA, Switzerland | access-date=2022-04-30}}</ref>

== Model ==
The model has two gates in addition to the '''H(W<sub>H</sub>, x)''' gate: the transform gate '''T(W<sub>T</sub>, x''') and the carry gate '''C(W<sub>C</sub>, x)'''. Those two last gates are non-linear transfer functions (by convention [[Sigmoid function]]). The '''H(W<sub>H</sub>, x)''' function can be any desired transfer function.

The carry gate is defined as '''C(W<sub>C</sub>, x) = 1 - T(W<sub>T</sub>, x)'''. While the transform gate is just a gate with a sigmoid transfer function.

== Structure ==
The structure of a hidden layer follows the equation:

<math>
\begin{align}
y = H(x,W_{H}) \centerdot T(x,W_{T}) + x \centerdot C(x,W_{C}) = H(x,W_{H}) \centerdot T(x,W_{T}) + x \centerdot (1 - T(x,W_{T}))
\end{align}
</math>

==References==
{{reflist}}

[[Category:Machine learning]]


{{compu-ai-stub}}