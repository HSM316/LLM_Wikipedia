{{short description|Type of artificial neural network}}
{{About|a technique used in machine learning||Highway}}

In [[machine learning]], the '''Highway Network''' was the first working very deep [[feedforward neural network]] with hundreds of layers, much deeper than previous [[Neural network (machine learning)|neural networks]].<ref name="highway2015" /><ref name="highway2015neurips" /><ref name="mostcited2021">{{Cite news |last=Schmidhuber |first=Jürgen |date=2021 |title=The most cited neural networks all build on work done in my labs |url=https://people.idsia.ch/~juergen/most-cited-neural-nets.html |access-date=2022-04-30 |work=AI Blog |location=IDSIA, Switzerland}}</ref>
It uses ''skip connections'' modulated by learned [[gating mechanism]]s to regulate information flow, inspired by [[long short-term memory]] (LSTM) [[recurrent neural networks]].<ref name="lstm1997">{{Cite journal 
 | author = Sepp Hochreiter
 | author2 = Jürgen Schmidhuber
 | title = Long short-term memory
 | journal = [[Neural Computation (journal)|Neural Computation]]
 | volume = 9
 | issue = 8
 | pages = 1735–1780
 | year = 1997
 | url = https://www.researchgate.net/publication/13853244
 | doi=10.1162/neco.1997.9.8.1735
 | pmid=9377276
| s2cid = 1915014
 | author2-link = Jürgen Schmidhuber
 | author-link = Sepp Hochreiter
 }}</ref><ref name="lstm2000">{{Cite journal 
 | author = Felix A. Gers
 | author2 = Jürgen Schmidhuber
 | author3 = Fred Cummins
 | title = Learning to Forget: Continual Prediction with LSTM
 | journal = [[Neural Computation (journal)|Neural Computation]]
 | volume = 12
 | issue = 10
 | pages = 2451–2471
 | year = 2000
 | doi=10.1162/089976600300015015
| pmid = 11032042
 | citeseerx = 10.1.1.55.5709
 | s2cid = 11598600
 }}</ref>
The advantage of the Highway Network over other [[deep learning]] architectures is its ability to overcome or partially prevent the [[vanishing gradient problem]],<ref name="hochreiter1991">{{cite thesis 
|url=http://www.bioinf.jku.at/publications/older/3804.pdf
|degree=diploma 
|first=Sepp
|last=Hochreiter
|title=Untersuchungen zu dynamischen neuronalen Netzen
|publisher=Technical University Munich, Institute of Computer Science, advisor: J. Schmidhuber
|year=1991}}</ref> thus improving its optimization. Gating mechanisms are used to facilitate information flow across the many layers ("information highways").<ref name="highway2015">{{cite arXiv|last1=Srivastava|first1=Rupesh Kumar|last2=Greff|first2=Klaus|last3=Schmidhuber|first3=Jürgen|title=Highway Networks|eprint=1505.00387|date=2 May 2015|class=cs.LG}}</ref><ref name="highway2015neurips">{{cite journal|last1=Srivastava|first1=Rupesh K|last2=Greff|first2=Klaus|last3=Schmidhuber|first3=Juergen|title=Training Very Deep Networks|journal=Advances in Neural Information Processing Systems |date=2015|volume=28|pages=2377–2385|url=http://papers.nips.cc/paper/5850-training-very-deep-networks|publisher=Curran Associates, Inc.}}</ref>

Highway Networks have found use in [[Semantic analysis (machine learning)|text sequence labeling]] and [[speech recognition]] tasks.<ref>{{cite arXiv|last1=Liu|first1=Liyuan|last2=Shang|first2=Jingbo|last3=Xu|first3=Frank F.|last4=Ren|first4=Xiang|last5=Gui|first5=Huan|last6=Peng|first6=Jian|last7=Han|first7=Jiawei|title=Empower Sequence Labeling with Task-Aware Neural Language Model|eprint=1709.04109|date=12 September 2017|class=cs.CL}}</ref><ref>{{cite arXiv|last1=Kurata|first1=Gakuto|last2=Ramabhadran|first2=Bhuvana|author2-link=Bhuvana Ramabhadran|last3=Saon|first3=George|last4=Sethy|first4=Abhinav|title=Language Modeling with Highway LSTM|eprint=1709.06436|date=19 September 2017|class=cs.CL}}</ref>

In 2014, the state of the art was training deep neural networks with 20 to 30 layers.<ref>{{Citation |last1=Simonyan |first1=Karen |title=Very Deep Convolutional Networks for Large-Scale Image Recognition |date=2015-04-10 |arxiv=1409.1556 |last2=Zisserman |first2=Andrew}}</ref> Stacking too many layers led to a steep reduction in [[Training, validation, and test data sets|training]] accuracy,<ref name="prelu">{{cite arXiv |eprint=1502.01852 |class=cs.CV |first1=Kaiming |last1=He |first2=Xiangyu |last2=Zhang |title=Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification |last3=Ren |first3=Shaoqing |last4=Sun |first4=Jian |year=2016}}</ref> known as the "degradation" problem.<ref name="resnet">{{Cite conference |last1=He |first1=Kaiming |last2=Zhang |first2=Xiangyu |last3=Ren |first3=Shaoqing |last4=Sun |first4=Jian |date=10 Dec 2015 |title=Deep Residual Learning for Image Recognition |arxiv=1512.03385}}</ref> In 2015, two techniques were developed to train such networks: the Highway Network (published in May), and the [[residual neural network]], or ResNet<ref name="resnet20152">{{Cite conference |last1=He |first1=Kaiming |last2=Zhang |first2=Xiangyu |last3=Ren |first3=Shaoqing |last4=Sun |first4=Jian |date=2016 |title=Deep Residual Learning for Image Recognition |url=https://ieeexplore.ieee.org/document/7780459 |location=Las Vegas, NV, USA |publisher=IEEE |pages=770–778 |arxiv=1512.03385 |doi=10.1109/CVPR.2016.90 |isbn=978-1-4673-8851-1 |journal=2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}}</ref> (December). ResNet behaves like an open-gated Highway Net.

== Model ==
The model has two gates in addition to the <math>H(W_H,x)</math> gate: the transform gate <math>T(W_T,x)</math> and the carry gate <math>C(W_C,x)</math>. The latter two gates are non-linear transfer functions (specifically [[Sigmoid function|sigmoid]] by convention). The function <math>H</math> can be any desired transfer function.

The carry gate is defined as:

<math display=block>C(W_C,x)=1-T(W_T,x)</math>

while the transform gate is just a gate with a sigmoid transfer function.

== Structure ==
The structure of a hidden layer in the Highway Network follows the equation:

<math>
\begin{align}
y = H(x,W_{H}) \cdot T(x,W_{T}) + x \cdot C(x,W_{C}) \\
= H(x,W_{H}) \cdot T(x,W_{T}) + x \cdot (1 - T(x,W_{T}))
\end{align}
</math>

== Related Work ==
[[Sepp Hochreiter]] analyzed the [[vanishing gradient problem]] in 1991 and attributed to it the reason why [[deep learning]] did not work well.<ref name="hochreiter1991"/>
To overcome this problem, [[Long short-term memory|Long Short-Term Memory]] (LSTM) [[recurrent neural networks]]<ref name="lstm1997"/>
have residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute <math display="inline"> y_{t+1} = F(x_{t}) + x_t </math>. During [[backpropagation through time]], this becomes the  residual formula <math display="inline">y = F(x) + x</math> for feedforward neural networks. This enables training very deep [[recurrent neural networks]] with a very long time span t. A later LSTM version published in 2000<ref name="lstm2000"/> modulates the identity LSTM connections by so-called "forget gates" such that their weights are not fixed to 1.0 but can be learned. In experiments, the forget gates were initialized with positive bias weights,<ref name="lstm2000"/> thus being opened, addressing the vanishing gradient problem.
As long as the forget gates of the 2000 LSTM are open, it behaves like the 1997 LSTM. 

The [[Highway_network|Highway Network]] of May 2015<ref name="highway2015"/>
applies these principles to [[feedforward neural network]]s.
It was reported to be "the first very deep feedforward network with hundreds of layers".<ref name="highwayblog">{{cite web
    | title= Highway Networks (May 2015): First Working Really Deep Feedforward Neural Networks With Over 100 Layers | url=https://people.idsia.ch/~juergen/highway-networks.html
	| last = Schmidhuber
	| first = Jürgen
    | date = 2015
}}
</ref> 
It is like a 2000 LSTM with forget gates [[backpropagation through time |unfolded in time]],<ref name="lstm2000"/> while the later Residual Nets have no equivalent of forget gates and are like the unfolded original 1997 LSTM.<ref name="lstm1997"/>
If the skip connections in Highway Networks are "without gates," or if their gates are kept open (activation 1.0), they become Residual Networks. 

The residual connection is a special case of the "short-cut connection" or "skip connection" by Rosenblatt (1961)<ref name="mlpbook">
{{cite book
	| last = Rosenblatt
	| first = Frank 
	| author-link = Frank Rosenblatt
	| date = 1961
	| title = Principles of neurodynamics. perceptrons and the theory of brain mechanisms 
	| url = https://safari.ethz.ch/digitaltechnik/spring2018/lib/exe/fetch.php?media=neurodynamics1962rosenblatt.pdf#page=327
    | location = 
	| publisher = 
	| page = 
	| isbn = 
}}</ref> and Lang & Witbrock (1988)<ref name="skip1988">{{Cite journal |last1=Lang |first1=Kevin| last2=Witbrock |first2=Michael|date= |year=1988 |title=Learning to tell two spirals apart |url=https://gwern.net/doc/ai/nn/fully-connected/1988-lang.pdf |journal=Proceedings of the 1988 Connectionist Models Summer school |pages=52–59}}</ref> which has the form {{awrap|<math>x \mapsto F(x) + Ax</math>.}} Here the randomly initialized weight matrix A does not have to be the identity mapping. Every residual connection is a skip connection, but almost all skip connections are not residual connections. 

The original Highway Network paper<ref name="highway2015may">{{cite arXiv |eprint=1505.00387 |class=cs.LG |first1=Rupesh Kumar |last1=Srivastava |first2=Klaus |last2=Greff |title=Highway Networks |date=3 May 2015 |last3=Schmidhuber |first3=Jürgen}}</ref> not only introduced the basic principle for very deep feedforward networks, but also included experimental results with 20, 50, and 100 layers networks, and mentioned ongoing experiments with up to 900 layers. Networks with 50 or 100 layers had lower training error than their plain network counterparts, but no lower training error than their 20 layers counterpart (on the MNIST dataset, Figure 1 in <ref name="highway2015may" />). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in <ref name="highway2015may" />). The ResNet paper,<ref name="resnetv2">{{cite arXiv |eprint=1603.05027 |class=cs.CV |first1=Kaiming |last1=He |first2=Xiangyu |last2=Zhang |title=Identity Mappings in Deep Residual Networks |last3=Ren |first3=Shaoqing |last4=Sun |first4=Jian |year=2015}}</ref> however, provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in <ref name="resnetv2" />). This is also  why the forget gates of the 2000 LSTM<ref name="lstm20002">{{Cite journal |author=Felix A. Gers |author2=Jürgen Schmidhuber |author3=Fred Cummins |year=2000 |title=Learning to Forget: Continual Prediction with LSTM |journal=[[Neural Computation (journal)|Neural Computation]] |volume=12 |issue=10 |pages=2451–2471 |citeseerx=10.1.1.55.5709 |doi=10.1162/089976600300015015 |pmid=11032042 |s2cid=11598600}}</ref> were initially opened through positive bias weights: as long as the gates are open, it behaves like the 1997 LSTM. Similarly, a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. The skip connections used in modern neural networks (e.g., [[Transformer (machine learning model)|Transformers]]) are dominantly identity mappings.

==References==
{{reflist}}

[[Category:Machine learning]]


{{compu-ai-stub}}