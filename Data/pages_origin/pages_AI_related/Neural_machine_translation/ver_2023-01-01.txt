{{short description|Approach to machine translation using artificial neural networks}}
'''Neural machine translation''' ('''NMT''') is an approach to [[machine translation]] that uses an [[artificial neural network]] to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.

==Properties==
They require only a fraction of the memory needed by traditional [[statistical machine translation]] (SMT) models. Furthermore, unlike conventional translation systems, all parts of the neural translation model are trained jointly (end-to-end) to maximize the translation performance.<ref name="KalchbrennerBlunsom" /><ref name="sequence" /><ref name="Properties" />

==History==
Deep learning applications appeared first in [[speech recognition]] in the 1990s. The first scientific paper on using neural networks in machine translation appeared in 2014. This year Bahdanau et al.<ref group=R>Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. In: Proceedings of the 3rd International Conference on Learning Representations; 2015 May 7–9; San Diego, USA; 2015.</ref> and Sutskever
et al.<ref group=R>Sutskever I, Vinyals O, Le QV. Sequence to sequence learning with neural networks. In: Proceedings of the 27th International Conference on Neural Information Processing Systems; 2014 Dec 8–13; Montreal, QC, Canada; 2014.</ref>proposed end-to-end neural network translation models and formally used the term "neural machine translation". First large-scale NMT system was launched by Baidu in 2015. Next year Google launched an NMT system too, followed by others.<ref name=progr>Haifeng Wang, Hua Wu, Zhongjun He, Liang Huang, Kenneth Ward Church [https://reader.elsevier.com/reader/sd/pii/S2095809921002745?token=7DC2A458FA8F2BE948962C5B3805F32396358594C0634CC929527DC477C12DA0F5B9EE7A5E563A63A3744AF94287FFCC&originRegion=eu-west-1&originCreation=20220727183755 Progress in Machine Translation] // Engineering (2021), doi: https://doi.org/10.1016/j.eng.2021.03.023</ref> It was followed by a lot of advances in the following few years. (Large-vocabulary NMT, application to Image captioning, Subword-NMT, Multilingual NMT, Multi-Source NMT, Character-dec NMT, Zero-Resource NMT, Google, Fully Character-NMT, Zero-Shot NMT in 2017) In 2015 there was the first appearance of a NMT system in a public machine translation competition (OpenMT'15). WMT'15 also for the first time had a NMT contender; the following year it already had 90% of NMT systems among its winners.<ref name="WMT16" />

Since 2017, neural machine translation has been used by the European Patent Office to make information from the global patent system instantly accessible.<ref name="vid"/> The system, developed in collaboration with [[Google]], is paired with 31 languages, and as of 2018, the system has translated over nine million documents.<ref name="vid"/>

==Workings==
NMT departs from phrase-based [[statistical machine translation|statistical]] approaches that use separately engineered subcomponents.<ref name="Medical" /> Neural machine translation (NMT) is not a drastic step beyond what has been traditionally done in statistical machine translation (SMT). Its main departure is the use of vector representations ("embeddings", "continuous space representations") for words and internal states. The structure of the models is simpler than phrase-based models. There is no separate language model, translation model, and reordering model, but just a single sequence model that predicts one word at a time. However, this sequence prediction is conditioned on the entire source sentence and the entire already produced target sequence.
NMT models use [[deep learning]] and [[representation learning]].

The word sequence modeling was at first typically done using a [[recurrent neural network]] (RNN).
A bidirectional recurrent neural network, known as an ''encoder'', is used by the neural network to encode a source sentence for a second RNN, known as a ''decoder'', that is used to predict words in the [[target language (translation)|target language]].<ref name="align&translate" /> Recurrent neural networks face difficulties in encoding long inputs into a single vector. This can be compensated by an attention mechanism<ref name="attention" /> which allows the decoder to focus on different parts of the input while generating each word of the output. There are further Coverage Models addressing the issues in such attention mechanisms, such as ignoring of past alignment information leading to over-translation and under-translation.<ref>{{Cite arXiv|eprint=1601.04811|class=cs.CL|first1=Zhaopeng|last1=Tu|first2=Zhengdong|last2=Lu|title=Modeling Coverage for Neural Machine Translation|last3=Liu|first3=Yang|last4=Liu|first4=Xiaohua|last5=Li|first5=Hang|year=2016}}</ref>

Convolutional Neural Networks (Convnets) are in principle somewhat better for long continuous sequences, but were initially not used due to several weaknesses. These were successfully compensated for in 2017 by using "attention mechanisms".<ref name="DeepL" />

The [[Transformer (machine learning model)|Transformer]]<ref>{{cite arXiv|last1=Vaswani|first1=Ashish|last2=Shazeer|first2=Noam|last3=Parmar|first3=Niki|last4=Uszkoreit|first4=Jakob|last5=Jones|first5=Llion|last6=Gomez|first6=Aidan N.|last7=Kaiser|first7=Lukasz|last8=Polosukhin|first8=Illia|date=2017-12-05|title=Attention Is All You Need|class=cs.CL|eprint=1706.03762}},</ref> an attention-based model, remains the dominant architecture for several language pairs.<ref>{{Cite journal|last1=Barrault|first1=Loïc|last2=Bojar|first2=Ondřej|last3=Costa-jussà|first3=Marta R.|last4=Federmann|first4=Christian|last5=Fishel|first5=Mark|last6=Graham|first6=Yvette|last7=Haddow|first7=Barry|last8=Huck|first8=Matthias|last9=Koehn|first9=Philipp|last10=Malmasi|first10=Shervin|last11=Monz|first11=Christof|date=August 2019|title=Findings of the 2019 Conference on Machine Translation (WMT19)|url=https://www.aclweb.org/anthology/W19-5301|journal=Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)|location=Florence, Italy|publisher=Association for Computational Linguistics|pages=1–61|doi=10.18653/v1/W19-5301|doi-access=free}}</ref> The self-attention layers of the Transformer model learn the dependencies between words in a sequence by examining links between all the words in the paired sequences and by directly modeling those relationships.  It's a simpler approach than the gating mechanism that RNNs employ.  And its simplicity has enabled researchers to develop high-quality translation models with the Transformer model, even in low-resource settings.<ref name="sicilian">{{cite arXiv 
 | last       = Wdowiak
 | first      = Eryk
 | eprint     = 2110.01938
 | title      = Sicilian Translator: A Recipe for Low-Resource NMT
 | class      = cs.CL
 | date       = 2021-09-27
 }}</ref>

==Remarks==
<references group=R/>
==References==
{{reflist|refs=
<ref name="WMT16">{{cite journal|last1=Bojar|first1=Ondrej|last2=Chatterjee|first2=Rajen|last3=Federmann|first3=Christian|last4=Graham|first4=Yvette|last5=Haddow|first5=Barry|last6=Huck|first6=Matthias|last7=Yepes|first7=Antonio Jimeno|last8=Koehn|first8=Philipp|last9=Logacheva|first9=Varvara|last10=Monz|first10=Christof|last11=Negri|first11=Matteo|last12=Névéol|first12=Aurélie|last13=Neves|first13=Mariana|last14=Popel|first14=Martin|last15=Post|first15=Matt|last16=Rubino|first16=Raphael|last17=Scarton|first17=Carolina|last18=Specia|first18=Lucia|last19=Turchi|first19=Marco|last20=Verspoor|first20=Karin|last21=Zampieri|first21=Marcos|title=Findings of the 2016 Conference on Machine Translation|journal=ACL 2016 First Conference on Machine Translation (WMT16)|date=2016|pages=131–198|url=https://cris.fbk.eu/retrieve/handle/11582/307240/14326/W16-2301.pdf|publisher=The Association for Computational Linguistics|access-date=2018-01-27|archive-url=https://web.archive.org/web/20180127202851/https://cris.fbk.eu/retrieve/handle/11582/307240/14326/W16-2301.pdf|archive-date=2018-01-27|url-status=dead}}</ref>
<ref name="Medical">{{cite journal |last1=Wołk |first1=Krzysztof |last2=Marasek |first2=Krzysztof |title=Neural-based Machine Translation for Medical Text Domain. Based on European Medicines Agency Leaflet Texts |year=2015 |journal=Procedia Computer Science |volume=64 |issue=64 |pages=2–9 |doi=10.1016/j.procs.2015.08.456|bibcode=2015arXiv150908644W |arxiv=1509.08644|s2cid=15218663 }}</ref>
<ref name="attention">{{Cite arXiv|last1=Bahdanau|first1=Dzmitry|last2=Cho|first2=Kyunghyun|last3=Bengio|first3=Yoshua|date=2014-09-01|title=Neural Machine Translation by Jointly Learning to Align and Translate|eprint=1409.0473|class=cs.CL}}</ref>
<ref name="DeepL">{{Cite news|url=https://techcrunch.com/2017/08/29/deepl-schools-other-online-translators-with-clever-machine-learning/|title=DeepL schools other online translators with clever machine learning|last=Coldewey|first=Devin|work=TechCrunch|date=2017-08-29|access-date=2018-01-27}}</ref>
<ref name="KalchbrennerBlunsom">{{cite journal|last1=Kalchbrenner|first1=Nal|last2=Blunsom|first2=Philip|title=Recurrent Continuous Translation Models|journal=Proceedings of the Association for Computational Linguistics|pages=1700–1709|date=2013|url=http://www.aclweb.org/anthology/D13-1176}}</ref>
<ref name="sequence">{{cite arXiv|last1=Sutskever|first1=Ilya|last2=Vinyals|first2=Oriol|last3=Le|first3=Quoc Viet|title=Sequence to sequence learning with neural networks|eprint=1409.3215|class=cs.CL|year=2014}}</ref>
<ref name="Properties">{{cite arXiv | eprint = 1409.1259|author1=Kyunghyun Cho |author2=Bart van Merrienboer |author3=Dzmitry Bahdanau |author4=Yoshua Bengio | title = On the Properties of Neural Machine Translation: Encoder–Decoder Approaches | date = 3 September 2014 | class = cs.CL}}</ref>
<ref name="align&translate">{{cite arXiv | eprint = 1409.0473|author1=Dzmitry Bahdanau |author2=Cho Kyunghyun |author3=Yoshua Bengio | title = Neural Machine Translation by Jointly Learning to Align and Translate | year = 2014 | class = cs.CL}}</ref>
<ref name="vid">{{cite web|url=https://www.youtube.com/watch?v=-ZVplhqhyYM|title=Neural Machine Translation|date=16 July 2018|access-date=14 June 2021|publisher=European Patent Office}}</ref>
}}

{{Approaches to machine translation}}
{{Differentiable computing}}

[[Category:Applications of artificial intelligence]]
[[Category:Computational linguistics]]
[[Category:Machine translation]]
[[Category:Tasks of natural language processing]]