In [[machine learning]], '''hyperparameter optimization'''<ref>Matthias Feurer and Frank Hutter. [https://link.springer.com/content/pdf/10.1007%2F978-3-030-05318-5_1.pdf Hyperparameter optimization]. In: ''AutoML: Methods, Systems, Challenges'', pages 3–38.</ref> or tuning is the problem of choosing a set of optimal [[Hyperparameter (machine learning)|hyperparameters]] for a learning algorithm. A hyperparameter is a [[parameter]] whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.

The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined [[loss function]] on given independent data.<ref name=abs1502.02127>{{cite arXiv |eprint=1502.02127|last1=Claesen|first1=Marc|title=Hyperparameter Search in Machine Learning|author2=Bart De Moor|class=cs.LG|year=2015}}</ref>  The objective function takes a tuple of hyperparameters and returns the associated loss.<ref name=abs1502.02127/> [[Cross-validation (statistics)|Cross-validation]] is often used to estimate this generalization performance.<ref name="bergstra">{{cite journal|last1=Bergstra|first1=James|last2=Bengio|first2=Yoshua|year=2012|title=Random Search for Hyper-Parameter Optimization|url=http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf|journal=Journal of Machine Learning Research|volume=13|pages=281–305}}</ref>

== Approaches ==

[[File:Hyperparameter Optimization using Grid Search.svg|thumb|Grid search across different values of two hyperparameters. For each hyperparameter, 10 different values are considered, so a total of 100 different combinations are evaluated and compared. Blue contours indicate regions with strong results, whereas red ones show regions with poor results.]]

=== Grid search ===
The traditional way of performing hyperparameter optimization has been ''grid search'', or a ''parameter sweep'', which is simply an [[Brute-force search|exhaustive searching]] through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by [[Cross-validation (statistics)|cross-validation]] on the training set<ref>Chin-Wei Hsu, Chih-Chung Chang and Chih-Jen Lin (2010). [http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf A practical guide to support vector classification]. Technical Report, [[National Taiwan University]].</ref>
or evaluation on a hold-out validation set.<ref>{{cite journal 
| vauthors = Chicco D
| title = Ten quick tips for machine learning in computational biology 
| journal = BioData Mining
| volume = 10
| issue =  35
| pages = 35 
| date = December 2017 
| pmid = 29234465
| doi = 10.1186/s13040-017-0155-3
| pmc= 5721660}}</ref>

Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.

For example, a typical soft-margin [[support vector machine|SVM]] [[statistical classification|classifier]] equipped with an [[radial basis function kernel|RBF kernel]] has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant ''C'' and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of "reasonable" values for each, say

:<math>C \in \{10, 100, 1000\}</math>
:<math>\gamma \in \{0.1, 0.2, 0.5, 1.0\}</math>

Grid search then trains an SVM with each pair (''C'', γ) in the [[Cartesian product]] of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.

Grid search suffers from the [[curse of dimensionality]], but is often [[embarrassingly parallel]] because the hyperparameter settings it evaluates are typically independent of each other.<ref name="bergstra"/>

[[File:Hyperparameter Optimization using Random Search.svg|thumb|Random search across different combinations of values for two hyperparameters. In this example, 100 different random choices are evaluated. The green bars show that more individual values for each hyperparameter are considered compared to a grid search.]]

=== Random search ===
Random Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm.<ref name="bergstra" /> In this case, the optimization problem is said to have a low intrinsic dimensionality.<ref>{{Cite journal|last1=Ziyu|first1=Wang|last2=Frank|first2=Hutter|last3=Masrour|first3=Zoghi|last4=David|first4=Matheson|last5=Nando|first5=de Feitas|date=2016|title=Bayesian Optimization in a Billion Dimensions via Random Embeddings|journal=Journal of Artificial Intelligence Research|language=en|volume=55|pages=361–387|doi=10.1613/jair.4806|arxiv=1301.1942|s2cid=279236}}</ref> Random Search is also [[embarrassingly parallel]], and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample. Despite its simplicity, random search remains one of the important base-lines against which to compare the performance of new hyperparameter optimization methods.

[[File:Hyperparameter Optimization using Tree-Structured Parzen Estimators.svg|thumb|Methods such as Bayesian optimization smartly explore the space of potential choices of hyperparameters by deciding which combination to explore next based on previous observations.]]

=== Bayesian optimization ===
{{main|Bayesian optimization}}

Bayesian optimization is a global optimization method for noisy black-box functions.  Applied to hyperparameter optimization, Bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set. By iteratively evaluating a promising hyperparameter configuration based on the current model, and then updating it, Bayesian optimization aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters expected close to the optimum). In practice, Bayesian optimization has been shown<ref name="hutter">{{Citation
 | last1 = Hutter
 | first1 = Frank
 | last2 = Hoos
 | first2 = Holger
 | last3 = Leyton-Brown
 | first3 = Kevin
 | title = Sequential model-based optimization for general algorithm configuration
 | journal = Learning and Intelligent Optimization
 | volume = 6683
 | pages = 507–523
 | year = 2011
 | url = http://www.cs.ubc.ca/labs/beta/Projects/SMAC/papers/11-LION5-SMAC.pdf | doi = 10.1007/978-3-642-25566-3_40
 | citeseerx = 10.1.1.307.8813
 | series = Lecture Notes in Computer Science
 | isbn = 978-3-642-25565-6
 }}</ref><ref name="bergstra11">{{Citation
 | last1 = Bergstra
 | first1 = James
 | last2 = Bardenet
 | first2 = Remi
 | last3 = Bengio
 | first3 = Yoshua
 | last4 = Kegl
 | first4 = Balazs
 | title = Algorithms for hyper-parameter optimization
 | journal = Advances in Neural Information Processing Systems
 | year = 2011
 | url = http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf }}</ref><ref name="snoek">{{cite journal
 | last1 = Snoek
 | first1 = Jasper
 | last2 = Larochelle
 | first2 = Hugo
 | last3 = Adams
 | first3 = Ryan
 | title = Practical Bayesian Optimization of Machine Learning Algorithms
 | journal = Advances in Neural Information Processing Systems
 | volume =<!-- -->
 | pages =<!-- -->
 | year = 2012
 | url = http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf
 | bibcode = 2012arXiv1206.2944S
 | arxiv = 1206.2944
 }}</ref><ref name="thornton">{{cite journal
 | last1 = Thornton
 | first1 = Chris
 | last2 = Hutter
 | first2 = Frank
 | last3 = Hoos
 | first3 = Holger
 | last4 = Leyton-Brown
 | first4 = Kevin
 | title = Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms
 | journal = Knowledge Discovery and Data Mining
 | volume = <!-- -->
 | pages = <!-- -->
 | year = 2013
 | url = http://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/autoweka.pdf
 | bibcode = 2012arXiv1208.3719T
 | arxiv = 1208.3719
 }}</ref> to obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run.

=== Gradient-based optimization ===
For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks.<ref>{{cite journal |last1=Larsen|first1=Jan|last2= Hansen |first2=Lars Kai|last3=Svarer|first3=Claus|last4=Ohlsson|first4=M|title=Design and regularization of neural networks: the optimal use of a validation set|journal=Proceedings of the 1996 IEEE Signal Processing Society Workshop|date=1996|pages=62–71|doi=10.1109/NNSP.1996.548336|isbn=0-7803-3550-3|citeseerx=10.1.1.415.3266|s2cid=238874|url=http://orbit.dtu.dk/files/4545571/Svarer.pdf}}</ref> Since then, these methods have been extended to other models such as [[support vector machine]]s<ref>{{cite journal |author1=Olivier Chapelle |author2=Vladimir Vapnik |author3=Olivier Bousquet |author4=Sayan Mukherjee |title=Choosing multiple parameters for support vector machines |journal=Machine Learning |year=2002 |volume=46 |pages=131–159 |url=http://www.chapelle.cc/olivier/pub/mlj02.pdf | doi = 10.1023/a:1012450327387 |doi-access=free }}</ref> or logistic regression.<ref>{{cite journal |author1 =Chuong B|author2= Chuan-Sheng Foo|author3=Andrew Y Ng|journal = Advances in Neural Information Processing Systems 20|title = Efficient multiple hyperparameter learning for log-linear models|year =2008|url=http://papers.nips.cc/paper/3286-efficient-multiple-hyperparameter-learning-for-log-linear-models.pdf}}</ref>

A different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using  [[automatic differentiation]].<ref>{{cite journal|last1=Domke|first1=Justin|title=Generic Methods for Optimization-Based Modeling|journal=Aistats |date=2012|volume=22|url=http://www.jmlr.org/proceedings/papers/v22/domke12/domke12.pdf}}</ref><ref name=abs1502.03492>{{cite arXiv |last1=Maclaurin|first1=Douglas|last2=Duvenaud|first2=David|last3=Adams|first3=Ryan P.|eprint=1502.03492|title=Gradient-based Hyperparameter Optimization through Reversible Learning|class=stat.ML|date=2015}}</ref><ref>{{cite journal |last1=Franceschi |first1=Luca |last2=Donini |first2=Michele |last3=Frasconi |first3=Paolo |last4=Pontil |first4=Massimiliano |title=Forward and Reverse Gradient-Based Hyperparameter Optimization |journal=Proceedings of the 34th International Conference on Machine Learning |date=2017 |arxiv=1703.01785 |bibcode=2017arXiv170301785F |url=http://proceedings.mlr.press/v70/franceschi17a/franceschi17a-supp.pdf}}</ref><ref>Shaban, A., Cheng, C. A., Hatch, N., & Boots, B. (2019, April). [https://arxiv.org/pdf/1810.10667.pdf Truncated back-propagation for bilevel optimization]. In ''The 22nd International Conference on Artificial Intelligence and Statistics'' (pp. 1723-1732). PMLR.</ref> A more recent work along this direction uses the [[implicit function theorem]] to calculate hypergradients and proposes a stable approximation of the inverse Hessian. The method scales to millions of hyperparameters and requires constant memory.

In a different approach,<ref>Lorraine, J., & Duvenaud, D. (2018). [[arxiv:1802.09419|Stochastic hyperparameter optimization through hypernetworks]]. ''arXiv preprint arXiv:1802.09419''.</ref> a hypernetwork is trained to approximate the best response function. One of the advantages of this method is that it can handle discrete hyperparameters as well. Self-tuning networks<ref>MacKay, M., Vicol, P., Lorraine, J., Duvenaud, D., & Grosse, R. (2019). [[arxiv:1903.03088|Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions]]. ''arXiv preprint arXiv:1903.03088''.</ref> offer a memory efficient version of this approach by choosing a compact representation for the hypernetwork. More recently, Δ-STN<ref>Bae, J., & Grosse, R. B. (2020). [[arxiv:2010.13514|Delta-stn: Efficient bilevel optimization for neural networks using structured response jacobians]]. ''Advances in Neural Information Processing Systems'', ''33'', 21725-21737.</ref> has improved this method further by a slight reparameterization of the hypernetwork which speeds up training. Δ-STN also yields a better approximation of the best-response Jacobian by linearizing the network in the weights, hence removing unnecessary nonlinear effects of large changes in the weights.

Apart from hypernetwork approaches, gradient-based methods can be used to optimize discrete hyperparameters also by adopting a continuous relaxation of the parameters.<ref>Liu, H., Simonyan, K., & Yang, Y. (2018). [[arxiv:1806.09055|Darts: Differentiable architecture search]]. ''arXiv preprint arXiv:1806.09055''.</ref> Such methods have been extensively used for the optimization of architecture hyperparameters in [[neural architecture search]].

=== Evolutionary optimization ===
{{main|Evolutionary algorithm}}

Evolutionary optimization is a methodology for the global optimization of noisy black-box functions. In hyperparameter optimization, evolutionary optimization uses [[evolutionary algorithms]] to search the space of hyperparameters for a given algorithm.<ref name="bergstra11" /> Evolutionary hyperparameter optimization follows a [[Evolutionary algorithm#Implementation|process]] inspired by the biological concept of [[evolution]]:

# Create an initial population of random solutions (i.e., randomly generate tuples of hyperparameters, typically 100+)
# Evaluate the hyperparameters tuples and acquire their [[fitness function]] (e.g., 10-fold [[Cross-validation (statistics)|cross-validation]] accuracy of the machine learning algorithm with those hyperparameters)
# Rank the hyperparameter tuples by their relative fitness
# Replace the worst-performing hyperparameter tuples with new hyperparameter tuples generated through [[crossover (genetic algorithm)|crossover]] and [[mutation (genetic algorithm)|mutation]]
# Repeat steps 2-4 until satisfactory algorithm performance is reached or algorithm performance is no longer improving

Evolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms,<ref name="bergstra11" /> [[automated machine learning]], typical neural network <ref name="kousiouris1">{{cite journal |vauthors=Kousiouris G, Cuccinotta T, Varvarigou T | year = 2011 | title= The effects of scheduling, workload type and consolidation scenarios on virtual machine performance and their prediction through optimized artificial neural networks | url= https://www.sciencedirect.com/science/article/abs/pii/S0164121211000951 | journal    = Journal of Systems and Software | volume = 84 | issue = 8 | pages = 1270–1291| doi = 10.1016/j.jss.2011.04.013 | hdl = 11382/361472 }}</ref> and [[Deep learning#Deep neural networks|deep neural network]] architecture search,<ref name="miikkulainen1">{{cite arXiv | vauthors = Miikkulainen R, Liang J, Meyerson E, Rawal A, Fink D, Francon O, Raju B, Shahrzad H, Navruzyan A, Duffy N, Hodjat B | year = 2017 | title = Evolving Deep Neural Networks |eprint=1703.00548| class = cs.NE }}</ref><ref name="jaderberg1">{{cite arXiv | vauthors = Jaderberg M, Dalibard V, Osindero S, Czarnecki WM, Donahue J, Razavi A, Vinyals O, Green T, Dunning I, Simonyan K, Fernando C, Kavukcuoglu K | year = 2017 | title = Population Based Training of Neural Networks |eprint=1711.09846| class = cs.LG }}</ref> as well as training of the weights in deep neural networks.<ref name="such1">{{cite arXiv | vauthors = Such FP, Madhavan V, Conti E, Lehman J, Stanley KO, Clune J | year = 2017 | title = Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning |eprint=1712.06567| class = cs.NE }}</ref>

=== Population-based ===
Population Based Training (PBT) learns both hyperparameter values and network weights. Multiple learning processes operate independently, using different hyperparameters. As with evolutionary methods, poorly performing models are iteratively replaced with models that adopt modified hyperparameter values and weights based on the better performers. This replacement model warm starting is the primary differentiator between PBT and other evolutionary methods. PBT thus allows the hyperparameters to evolve and eliminates the need for manual hypertuning. The process makes no assumptions regarding model architecture, loss functions or training procedures.

PBT and its variants are adaptive methods: they update hyperparameters during the training of the models. On the contrary, non-adaptive methods have the sub-optimal strategy to assign a constant set of hyperparameters for the whole training.<ref>{{cite arXiv|last1=Li|first1=Ang|last2=Spyra|first2=Ola|last3=Perel|first3=Sagi|last4=Dalibard|first4=Valentin|last5=Jaderberg|first5=Max|last6=Gu|first6=Chenjie|last7=Budden|first7=David|last8=Harley|first8=Tim|last9=Gupta|first9=Pramod|date=2019-02-05|title=A Generalized Framework for Population Based Training|eprint=1902.01894|class=cs.AI}}</ref>

=== Early stopping-based ===
A class of early stopping-based hyperparameter optimization algorithms is purpose built for large search spaces of continuous and discrete hyperparameters, particularly when the computational cost to evaluate the performance of a set of hyperparameters is high. Irace implements the iterated racing algorithm, that focuses the search around the most promising configurations, using statistical tests to discard the ones that perform poorly.<ref name="irace">{{cite journal |last1=López-Ibáñez |first1=Manuel |last2=Dubois-Lacoste |first2=Jérémie |last3=Pérez Cáceres |first3=Leslie |last4=Stützle |first4=Thomas |last5=Birattari |first5=Mauro |date=2016 |title=The irace package: Iterated Racing for Automatic Algorithm Configuration |journal=Operations Research Perspective |volume=3 |issue=3 |pages=43–58 |doi=10.1016/j.orp.2016.09.002|doi-access=free }}</ref><ref name="race">{{cite journal |last1=Birattari |first1=Mauro |last2=Stützle |first2=Thomas |last3=Paquete |first3=Luis |last4=Varrentrapp |first4=Klaus |date=2002 |title=A Racing Algorithm for Configuring Metaheuristics |journal=Gecco 2002 |pages=11–18}}</ref>
Another early stopping hyperparameter optimization algorithm is successive halving (SHA),<ref>{{cite arXiv|last1=Jamieson|first1=Kevin|last2=Talwalkar|first2=Ameet|date=2015-02-27|title=Non-stochastic Best Arm Identification and Hyperparameter Optimization|eprint=1502.07943|class=cs.LG}}</ref> which begins as a random search but periodically prunes low-performing models, thereby focusing computational resources on more promising models.  Asynchronous successive halving (ASHA)<ref>{{cite arXiv|last1=Li|first1=Liam|last2=Jamieson|first2=Kevin|last3=Rostamizadeh|first3=Afshin|last4=Gonina|first4=Ekaterina|last5=Hardt|first5=Moritz|last6=Recht|first6=Benjamin|last7=Talwalkar|first7=Ameet|date=2020-03-16|title=A System for Massively Parallel Hyperparameter Tuning|class=cs.LG|eprint=1810.05934v5}}</ref> further improves upon SHA's resource utilization profile by removing the need to synchronously evaluate and prune low-performing models. Hyperband<ref>{{cite journal|last1=Li|first1=Lisha|last2=Jamieson|first2=Kevin|last3=DeSalvo|first3=Giulia|last4=Rostamizadeh|first4=Afshin|last5=Talwalkar|first5=Ameet|date=2020-03-16|title=Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization|journal=Journal of Machine Learning Research|volume=18|pages=1–52|arxiv=1603.06560}}</ref> is a higher level early stopping-based algorithm that invokes SHA or ASHA multiple times with varying levels of pruning aggressiveness, in order to be more widely applicable and with fewer required inputs.

=== Others ===
[[Radial basis function|RBF]]<ref name=abs1705.08520>{{cite arXiv |eprint=1705.08520|last1=Diaz|first1=Gonzalo|title=An effective algorithm for hyperparameter optimization of neural networks|last2=Fokoue|first2=Achille|last3=Nannicini|first3=Giacomo|last4=Samulowitz|first4=Horst|class=cs.AI|year=2017}}</ref> and [[spectral method|spectral]]<ref name=abs1706.00764>{{cite arXiv |eprint=1706.00764|last1=Hazan|first1=Elad|title=Hyperparameter Optimization: A Spectral Approach|last2=Klivans|first2=Adam|last3=Yuan|first3=Yang|class=cs.LG|year=2017}}</ref> approaches have also been developed.

== Open-source software ==

===Grid search===
*[https://github.com/determined-ai/determined Determined], a DL Training Platform includes grid search for PyTorch and TensorFlow (Keras and Estimator) models.
*[http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html H2O AutoML] provides grid search over algorithms in the H2O open source machine learning library.
*[https://github.com/kubeflow/katib Katib] is a Kubernetes-native system that includes grid search.
*[[scikit-learn]] is a Python package that includes [http://scikit-learn.sourceforge.net/modules/grid_search.html grid] search.
*[https://github.com/autonomio/talos Talos] includes grid search for [[Keras]].
*[https://ray.readthedocs.io/en/latest/tune.html Tune] is a Python library for distributed hyperparameter tuning and supports grid search.
*[https://orion.readthedocs.io/ Oríon] is an asynchronous framework for distributed black-box optimization and hyperparameter optimization that includes grid search.
*[https://github.com/awslabs/syne-tune Syne Tune] is a Python library for asynchronous hyperparameter and architecture optimization that supports grid search.

===Random search===
*[https://github.com/determined-ai/determined Determined] is a DL Training Platform that supports random search for PyTorch and TensorFlow (Keras and Estimator) models.
* [https://github.com/hyperopt/hyperopt hyperopt], also via [https://github.com/maxpumperla/hyperas hyperas] and [https://github.com/hyperopt/hyperopt-sklearn hyperopt-sklearn], are Python packages which include random search.
*[https://github.com/kubeflow/katib Katib] is a Kubernetes-native system that includes random search.
* [[scikit-learn]] is a Python package which includes [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html random] search.
* [[caret]] is a R package which includes [http://topepo.github.io/caret/random-hyperparameter-search.html grid & random] search.
*[https://github.com/autonomio/talos Talos] includes a customizable random search for [[Keras]].
*[https://ray.readthedocs.io/en/latest/tune.html Tune] is a Python library for distributed hyperparameter tuning and supports random search over arbitrary parameter distributions.
*[https://orion.readthedocs.io/ Oríon] is an asynchronous framework for distributed black-box optimization and hyperparameter optimization that includes random search.
*[https://github.com/awslabs/syne-tune Syne Tune] is a Python library for asynchronous hyperparameter and architecture optimization that supports random search.


===Bayesian===
* [https://github.com/automl/auto-sklearn Auto-sklearn]<ref name="autosklearn">{{cite journal | vauthors = Feurer M, Klein A, Eggensperger K, Springenberg J, Blum M, Hutter F | year = 2015 | title = Efficient and Robust Automated Machine Learning | url = https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning | journal = Advances in Neural Information Processing Systems 28 (NIPS 2015) | volume = 28 | pages = 2962–2970 }}</ref> is a Bayesian hyperparameter optimization layer on top of [[scikit-learn]].
* [https://github.com/facebook/Ax Ax]<ref name=AxBoTorch>{{cite web |url=https://ai.facebook.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/ |title=Open-sourcing Ax and BoTorch: New AI tools for adaptive experimentation |year=2019}}</ref> is a Python-based experimentation platform that supports Bayesian optimization and bandit optimization as exploration strategies.
* [https://github.com/baptistar/BOCS BOCS] is a Matlab package which uses [[semidefinite programming]] for minimizing a black-box function over discrete inputs.<ref name="arXiv:1806.08838">{{cite arXiv |year=2018 |title=Bayesian Optimization of Combinatorial Structures |eprint=1806.08838|last1=Baptista |first1=Ricardo |last2=Poloczek |first2=Matthias |class=stat.ML }}</ref> A Python 3 implementation is also included.
* [https://github.com/automl/HpBandSter HpBandSter] is a Python package which combines Bayesian optimization with bandit-based methods.<ref name="arXiv:1807.01774">{{cite arXiv |year=2018 |title=BOHB: Robust and Efficient Hyperparameter Optimization at Scale |eprint=1807.01774|last1=Falkner |first1=Stefan |last2=Klein |first2=Aaron |last3=Hutter |first3=Frank |class=stat.ML }}</ref>
*[https://github.com/kubeflow/katib Katib] is a Kubernetes-native system which includes bayesian optimization.
*[https://github.com/mlr-org/mlrMBO mlrMBO], also with [https://github.com/mlr-org/mlr mlr], is an [[R (programming language)|R]] package for model-based/Bayesian optimization of black-box functions.
*[https://optuna.readthedocs.io/en/latest/ optuna] is a Python package for black box optimization, compatible with arbitrary functions that need to be optimized.
* [https://github.com/scikit-optimize/scikit-optimize scikit-optimize] is a Python package or sequential model-based optimization with a scipy.optimize interface.<ref name=skopt>{{Cite web|url=https://scikit-optimize.github.io/|title=skopt API documentation|website=scikit-optimize.github.io}}</ref>
* [https://github.com/automl/SMAC3 SMAC] SMAC is a Python/Java library implementing Bayesian optimization.<ref name="SMAC">{{cite journal | vauthors = Hutter F, Hoos HH, Leyton-Brown K | title = Sequential Model-Based Optimization for General Algorithm Configuration | url = https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf | journal = Proceedings of the Conference on Learning and Intelligent OptimizatioN (LION 5)}}</ref>
* [https://github.com/PhilippPro/tuneRanger tuneRanger] is an R package for tuning random forests using model-based optimization.
* [https://github.com/google/vizier Open Source Vizier], a Python service which allows Bayesian Optimization development built upon [https://www.tensorflow.org/probability Tensorflow Probability].
*[https://orion.readthedocs.io/ Oríon] is a Python package that provides a built-in implementation of the Bayesian optimization algorithm TPE as well as integrations with [https://github.com/facebook/Ax Ax], [https://github.com/scikit-optimize/scikit-optimize scikit-optimize] and [https://github.com/automl/HpBandSter HpBandSter] for additional Bayesian optimization algorithms.
*[https://github.com/awslabs/syne-tune Syne Tune] is a Python library for asynchronous hyperparameter and architecture optimization that supports Bayesian optimization, as well as model-based multi-fidelity algorithms.


===Gradient-based optimization===
* [https://github.com/lucfra/FAR-HO FAR-HO] is a Python package containing Tensorflow implementations and wrappers for gradient-based hyperparameter optimization with forward and reverse mode algorithmic differentiation.
* [https://github.com/dmlc/xgboost XGBoost] is an open-source software library that provides a gradient boosting framework for C++, Java, Python, R, and Julia.

===Evolutionary===
* [https://github.com/DEAP/deap deap] is a Python framework for general evolutionary computation which is flexible and integrates with parallelization packages like [https://github.com/soravux/scoop scoop] and [[PySpark|pyspark]], and other Python frameworks like [[Scikit-learn|sklearn]] via [https://github.com/rsteca/sklearn-deap sklearn-deap].
*[https://github.com/determined-ai/determined Determined] is a DL Training Platform that supports PBT for optimizing PyTorch and TensorFlow (Keras and Estimator) models.
* [https://github.com/joeddav/devol devol] is a Python package that performs Deep Neural Network architecture search using [[genetic programming]].
* [https://github.com/facebookresearch/nevergrad nevergrad]<ref name=nevergrad_issue1/> is a Python package which includes [[Differential evolution]], [[Evolution strategy]], [[Bayesian optimization]], population control methods for the noisy case and [[Particle swarm optimization]].<ref name=nevergrad/>
*[https://ray.readthedocs.io/en/latest/tune.html Tune] is a Python library for distributed hyperparameter tuning and leverages [https://github.com/facebookresearch/nevergrad nevergrad] for evolutionary algorithm support.
* [https://github.com/google/vizier Open Source Vizier] is a Python service that supports algorithms such as NSGA-II<ref name="doi10.1109/4235.996017">{{Cite journal | doi = 10.1109/4235.996017| title = A fast and elitist multiobjective genetic algorithm: NSGA-II| journal = IEEE Transactions on Evolutionary Computation| volume = 6| issue = 2| pages = 182| year = 2002| last1 = Deb | first1 = K.| last2 = Pratap | first2 = A.| last3 = Agarwal | first3 = S.| last4 = Meyarivan | first4 = T.| citeseerx = 10.1.1.17.7771}}</ref> and integrated with [https://github.com/google/pyglove PyGlove] to allow evolutionary computing.
*[https://orion.readthedocs.io/ Oríon] is a Python package that includes the algorithm Population Based Training and Population Based Bandits, as well as an integration with [https://github.com/facebookresearch/nevergrad nevergrad] for additional evolutionary algorithms.
*[https://github.com/awslabs/syne-tune Syne Tune] is a Python library for asynchronous hyperparameter and architecture optimization that supports evolutionary algorithms.


===Early Stopping===
*[https://github.com/determined-ai/determined Determined] is a DL Training Platform that supports Hyperband for PyTorch and TensorFlow (Keras and Estimator) models.
* [https://iridia.ulb.ac.be/irace/ irace] is an R package that implements the iterated racing algorithm.<ref name="irace"/><ref name="race"/>
*[https://github.com/kubeflow/katib Katib] is a Kubernetes-native system that includes hyperband.
*[https://github.com/awslabs/syne-tune Syne Tune] is a Python library for asynchronous hyperparameter and architecture optimization that supports multi-fidelity algorithms (also known as early-stopping algorithms).


===Other===
*[https://github.com/determined-ai/determined Determined] is a DL Training Platform that supports random, grid, PBT, Hyperband and NAS approaches to hyperparameter optimization for PyTorch and TensorFlow (Keras and Estimator) models.
* [[dlib]]<ref name=dlib_github>{{Cite web|url=https://github.com/davisking/dlib|title=A toolkit for making real world machine learning and data analysis applications in C++: davisking/dlib|date=February 25, 2019|via=GitHub}}</ref> is a C++ package with a Python API which has a parameter-free optimizer based on [https://arxiv.org/abs/1703.02628 LIPO] and [[trust region]] optimizers working in tandem.<ref name=dlib_blog>{{cite web |last1=King |first1=Davis |title=A Global Optimization Algorithm Worth Using |url=http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html}}</ref>
* [https://github.com/callowbird/Harmonica Harmonica] is a Python package for spectral hyperparameter optimization.<ref name=abs1706.00764/>
* [https://github.com/hyperopt/hyperopt hyperopt], also via [https://github.com/maxpumperla/hyperas hyperas] and [https://github.com/hyperopt/hyperopt-sklearn hyperopt-sklearn], are Python packages which include [[tree of Parzen estimators]] based distributed hyperparameter optimization.
*[https://github.com/kubeflow/katib/ Katib] is a Kubernetes-native system which includes grid, random search, bayesian optimization, hyperband, and NAS based on reinforcement learning.
* [https://github.com/facebookresearch/nevergrad nevergrad]<ref name=nevergrad_issue1>{{Cite web|url=https://github.com/facebookresearch/nevergrad/issues/1|title=[QUESTION] How to use to optimize NN hyperparameters · Issue #1 · facebookresearch/nevergrad|website=GitHub}}</ref> is a Python package for gradient-free optimization using techniques such as differential evolution, sequential quadratic programming, fastGA, covariance matrix adaptation, population control methods, and particle swarm optimization.<ref name=nevergrad>{{Cite web|url=https://code.fb.com/ai-research/nevergrad/|title=Nevergrad: An open source tool for derivative-free optimization|date=December 20, 2018}}</ref>
* [[Neural Network Intelligence]] (NNI) is a Python package which includes hyperparameter tuning for neural networks in local and distributed environments. Its techniques include TPE, random, anneal, evolution, SMAC, batch, grid, and hyperband.
* [https://github.com/sherpa-ai/sherpa parameter-sherpa] is a similar Python package which includes several techniques grid search, Bayesian and genetic Optimization
* [https://github.com/wwu-mmll/photonai photonai] is a high level Python API for designing and optimizing machine learning pipelines based on grid, random search and bayesian optimization.
* [https://github.com/CMA-ES/pycma pycma] is a Python implementation of [[CMA-ES|Covariance Matrix Adaptation Evolution Strategy]].
* [https://github.com/coin-or/rbfopt rbfopt] is a Python package that uses a [[radial basis function]] model<ref name=abs1705.08520/>
* [https://ray.readthedocs.io/en/latest/tune.html Tune] is a Python library for hyperparameter tuning execution and integrates with/scales many existing hyperparameter optimization libraries such as [https://github.com/hyperopt/hyperopt hyperopt], [https://github.com/facebookresearch/nevergrad nevergrad], and [https://github.com/scikit-optimize/scikit-optimize scikit-optimize].
* [https://github.com/google/vizier Open Source Vizier] is a Python-based service for blackbox hyperparameter optimization and research, based on Google's internal Vizier service.<ref name=google_vizier>{{cite web |last1=Golovin |first1=Daniel |last2=Solnik |first2=Benjamin |last3=Subhodeep |first3=Moitra |last4=Kochanski |first4=Greg |last5=Karro |first5=John |last6=Sculley |first6=D. |title=Google Vizier: A Service for Black-Box Optimization |url=https://dl.acm.org/doi/10.1145/3097983.3098043}}</ref>
* [https://orion.readthedocs.io/ Oríon] is an asynchronous framework for distributed black-box optimization and hyperparameter optimization that includes built-in algorithms such as grid search, random search, MOFA, Hyperband, PBT, TPE and several integrations with hyperparameter optimization libraries such as [https://github.com/facebook/Ax Ax], [https://github.com/scikit-optimize/scikit-optimize scikit-optimize], [https://github.com/automl/HpBandSter HpBandSter], [https://github.com/huawei-noah/HEBO HEBO], [https://github.com/facebookresearch/nevergrad nevergrad] and [https://github.com/automl/DEHB DEHB].

== Commercial services ==
* [https://aws.amazon.com/sagemaker/automatic-model-tuning/ Amazon Sagemaker Automatic Model Tuning] offers grid search, random search, Gaussian process-based Bayesian optimization and asynchronous successive halving. 
* [https://bigml.com/api/optimls BigML OptiML] supports mixed search domains
* [https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning Google Cloud Vertex Vizier] supports mixed search domains, multiobjective, multifidelity, and safety constraints.
* [https://indiesolver.com Indie Solver] supports multiobjective, multifidelity and constraint optimization
* [https://mindfoundry.ai/OPTaaS Mind Foundry OPTaaS] supports mixed search domains, multiobjective, constraints, parallel optimization and surrogate models.
* [https://sigopt.com SigOpt] supports mixed search domains, multiobjective, multisolution, multifidelity, constraint (linear and black-box), and parallel optimization.

== See also ==
* [[Automated machine learning]]
* [[Neural architecture search]]
* [[Meta-optimization]]
* [[Model selection]]
* [[Self-tuning]]
* [[XGBoost]]

== References ==
{{Reflist|30em}}

{{Differentiable computing}}

[[Category:Machine learning]]
[[Category:Mathematical optimization]]
[[Category:Model selection]]