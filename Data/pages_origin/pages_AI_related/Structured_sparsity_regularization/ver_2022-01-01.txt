{{Orphan|date=December 2015}}

'''Structured sparsity regularization''' is a class of methods, and an area of research in [[statistical learning theory]], that extend and generalize sparsity regularization learning methods.<ref name="rosPoggio">{{cite document|last = Rosasco|first = Lorenzo|author2 = Poggio, Tomasso|title = A Regularization Tour of Machine Learning |work=MIT-9.520 Lectures Notes|date=December 2014}}</ref> Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable <math> Y </math> (i.e., response, or [[Dependent and independent variables#Dependent variable|dependent variable]]) to be learned can be described by a reduced number of variables in the input space <math> X </math> (i.e., the [[Domain of a function|domain]], space of [[Feature (machine learning)|features]] or [[Dependent and independent variables#Independent variable|explanatory variables]]). ''Sparsity regularization methods'' focus on selecting the input variables that best describe the output. ''Structured sparsity regularization methods'' generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in <math> X </math>.<ref name="groupLasso" /><ref name="latentLasso" />

Common motivation for the use of structured sparsity methods are model interpretability, [[Curse of dimensionality|high-dimensional learning]] (where dimensionality of <math> X </math> may be higher than the number of observations <math> n </math>), and reduction of [[Time complexity|computational complexity]].<ref name="LR18" /> Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups,<ref name="groupLasso" /> non-overlapping groups, and acyclic graphs.<ref name="latentLasso" /> Examples of uses of structured sparsity methods include face recognition,<ref name="face_recognition">{{cite journal|last = Jia|first = Kui|title = Robust and Practical Face Recognition via Structured Sparsity|year = 2012|display-authors=etal}}</ref> [[Magnetic resonance imaging|magnetic resonance image (MRI)]] processing,<ref name="MRI">{{cite book|last = Chen|first = Chen|chapter= Compressive Sensing MRI with Wavelet Tree Sparsity |chapter-url=https://papers.nips.cc/paper/4630-compressive-sensing-mri-with-wavelet-tree-sparsity |title= Proceedings of the 26th Annual Conference on Neural Information Processing Systems |pages = 1115–1123|year = 2012|display-authors=etal|publisher = Curran Associates}}</ref> socio-linguistic analysis in natural language processing,<ref name="sociolinguistic">{{cite journal|last = Eisenstein|first = Jacob|title = Discovering Sociolinguistic Associations with Structured Sparsity|journal = Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics|year = 2011|display-authors=etal}}</ref> and analysis of genetic expression in breast cancer.<ref name="genetic">{{cite journal|last = Jacob|first = Laurent|title = Group Lasso with Overlap and Graph Lasso|journal = Proceedings of the 26th International Conference on Machine Learning|year = 2009|display-authors=etal}}</ref>

== Definition and related concepts ==

=== Sparsity regularization ===
Consider the linear kernel [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem with a loss function <math> V(y_i, f(x)) </math>  and the <math>\ell_0</math> "norm" as the regularization penalty:
: <math>\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n V(y_i, \langle w,x_i\rangle)  + \lambda \|w\|_0, </math>
where <math> x, w \in \mathbb{R^d} </math>, and <math>\|w\|_0</math> denotes the <math>\ell_0</math> "norm", defined as the number of nonzero entries of the vector <math>w</math>. <math>f(x) = \langle w,x_i\rangle</math>  is said to be '''sparse if'''  <math>\|w\|_0 = s < d</math>. Which means that the output <math>Y</math> can be described by a small subset of input variables.

More generally, assume a dictionary <math> \phi_j : X \rightarrow \mathbb{R} </math> with <math>j = 1,...,p </math>  is given, such that the target function <math>f(x)</math> of a learning problem can be written as:
: <math>f(x) = \sum_{j=1}^p \phi_j(x) w_j</math>, <math> \forall x \in X </math> 
The <math>\ell_0</math> norm <math>\|f\|_0 = \|w\|_0</math>  as the number of non-zero components of <math>w</math> is defined as 
: <math>\|w\|_0 = | \{ j | w_j \neq 0, j \in\{ 1,...,p \}\} |</math>, where <math>|A|</math> is the cardinality of set <math>A</math>.
<math>f</math> is said to be sparse if <math>\|f\|_0 = \|w\|_0 = s < d</math>.

However, while using the <math>\ell_0</math> norm for regularization favors sparser solutions, it is computationally difficult to use and additionally is not convex. A computationally more feasible norm that favors sparser solutions is the <math>\ell_1</math> norm; this has been shown to still favor sparser solutions and is additionally convex.<ref name="LR18" />

=== Structured sparsity regularization ===
Structured sparsity regularization extends and generalizes the variable selection problem that characterizes sparsity regularization.<ref name="groupLasso">{{cite journal|last = Yuan|first = M.|author2 = Lin, Y.|title = Model selection and estimation in regression with grouped variables|journal = J. R. Stat. Soc. B|year = 2006|volume = 68|issue = 1|pages = 49–67|doi = 10.1111/j.1467-9868.2005.00532.x|citeseerx = 10.1.1.79.2062}}</ref><ref name="latentLasso" /> Consider the above [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem with a general kernel and associated feature map <math> \phi_j : X \rightarrow \mathbb{R} </math> with <math>j = 1,...,p </math>.
: <math>\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n V(y_i, \langle w,\Phi(x_i)\rangle)  + \lambda \|w\|_0, </math>
The regularization term <math>\lambda \|w\|_0 </math> penalizes each <math>w_j</math> component independently, which means that the algorithm will suppress input variables independently from each other.

In several situations we may want to impose more structure in the regularization process, so that, for example, input variables are suppressed according to predefined groups. '''Structured sparsity regularization methods''' allow to impose such structure by adding structure to the norms defining the regularization term.

== Structures and norms ==

=== Non-overlapping groups: group Lasso ===
The non-overlapping group case is the most basic instance of structured sparsity. In it, an ''a priori'' partition of the coefficient vector <math>w</math> in <math>G</math> non-overlapping groups is assumed. Let <math>w_g</math> be the vector of coefficients in group <math>g</math>, we can define a regularization term and its group norm as
: <math>\lambda R(w)=\lambda\sum _{{g=1}}^{G}\|w_{g}\|_{g} </math>,
where <math> \|w_{g}\|_{g}</math> is the group <math>\ell_2</math> norm <math> \|w_{g}\|_{g}= \sqrt{ \sum _{{j=1}}^{{|G_{g}|}}(w_{g}^{j})^{2}} </math> ,    <math>G_g</math> is group <math>g</math>, and <math>w_{g}^{j}</math> is the ''j-th'' component of group <math>G_g</math>.

The above norm is also referred to as '''group Lasso'''.<ref name="groupLasso" /> This regularizer will force entire coefficient groups towards zero, rather than individual coefficients. As the groups are non-overlapping, the set of non-zero coefficients can be obtained as the union of the groups that were not set to zero, and conversely for the set of zero coefficients.

=== Overlapping groups ===
Overlapping groups is the structure sparsity case where a variable can belong to more than one group <math>g</math>. This case is often of interest as it can represent a more general class of  relationships among variables than non-overlapping groups can, such as tree structures or other type of graphs.<ref name="latentLasso">{{cite arxiv |last1 = Obozinski|first1 = G.|last2 = Laurent | first2= J. | last3= Vert | first3= J.-P.|title = Group lasso with overlaps: the latent group lasso approach |year = 2011 |eprint = 1110.0413|class = stat.ML}}</ref><ref name="genetic" />

There are two types of overlapping group sparsity regularization approaches, which are used to model different types of input variable relationships:

==== Intersection of complements: group Lasso  ====
The ''intersection of complements'' approach is used in cases when we want to select only those input variables that have positive coefficients in all groups they belong to.  Consider again the '''group Lasso''' for a [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem:
: <math>\lambda R(w)=\lambda\sum _{{g=1}}^{G}\|w_{g}\|_{g} </math>,
where <math> \|w_{g}\|_{g}</math> is the group <math>\ell_2</math> norm,    <math>G_g</math> is group <math>g</math>, and <math>w_{g}^{j}</math> is the ''j-th'' component of group <math>G_g</math>.

As in the non-overlapping groups case, the ''group Lasso'' regularizer will potentially set entire groups of coefficients to zero. Selected variables are those with coefficients <math>w_j > 0</math>. However, as in this case groups may overlap, we take the '''intersection of the complements''' of those groups that are not set to zero.

This ''intersection of complements'' selection criteria implies the modeling choice that we allow some coefficients within a particular group <math>g</math> to be set to zero, while others within the same group <math>g</math> may remain positive. In other words, coefficients within a group may differ depending on the several group memberships that each variable within the group may have.

==== Union of groups: latent group Lasso  ====
A different approach is to consider union of groups for variable selection. This approach captures the modeling situation where variables can be selected as long as they belong at least to one group with positive coefficients. This modeling perspective implies that we want to preserve group structure.

The formulation of the union of groups approach is also referred to as '''latent group Lasso''', and requires to modify the group <math>\ell_2</math> norm considered above and introduce the following regularizer <ref name="latentLasso" />
: <math>R(w)=inf\left\{\sum _{g}\|w_{{g}}\|_{{g}}:w=\sum _{{g=1}}^{G}{\bar  {w}}_{g}\right\}</math>
where <math>w\in {\mathbb  {R^{d}}}</math>,  <math>w_{{g}}\in G_{g}</math> is the vector of coefficients of group g, and <math>{\bar  {w}}_{g}\in {\mathbb  {R^{d}}}</math> is a vector with coefficients <math>w_{g}^{j}</math> for all variables  <math>j</math>  in group  <math>g</math> , and  <math>0</math>  in all others, i.e., <math>{\bar  w}_{g}^{j}=w_{g}^{j}</math> if  <math>j</math>  in group  <math>g</math>  and <math>{\bar  w}_{g}^{j}=0</math> otherwise.

This regularizer can be interpreted as effectively replicating variables that belong to more than one group, therefore conserving group structure. As intended by the union of groups approach, requiring <math>w=\sum _{{g=1}}^{G}{\bar  {w}}_{g}</math> produces a vector of weights w that effectively sums up the weights of all variables across all groups they belong to.

=== Issues with Group Lasso regularization and alternative approaches ===
The objective function using group lasso consists of an error function, which is generally required to be convex but not necessarily strongly convex, and a group <math>\ell_1</math> regularization term.  An issue with this objective function is that it is convex but not necessarily strongly convex, and thus generally does not lead to unique solutions.<ref name=":0" />

An example of a way to fix this is to introduce the squared <math>\ell_2</math> norm of the weight vector as an additional regularization term while keeping the <math>\ell_1</math> regularization term from the group lasso approach.<ref name=":0" /> If the coefficient of the squared  <math>\ell_2</math> norm term is greater than <math>0</math>, then because the squared  <math>\ell_2</math> norm term is strongly convex, the resulting objective function will also be strongly convex.<ref name=":0" /> Provided that the  <math>\ell_2</math> coefficient is suitably small but still positive, the weight vector minimizing the resulting objective function is generally very close to a weight vector that minimizes the objective function that would result from removing the group  <math>\ell_2</math> regularization term altogether from the original objective function; the latter scenario corresponds to the group Lasso approach.<ref name=":0" /> Thus this approach allows for simpler optimization while maintaining sparsity.<ref name=":0" />

=== Norms based on the structure over Input variables ===
''See: [[Submodular set function]]''

Besides the norms discussed above, other norms used in structured sparsity methods include hierarchical norms and norms defined on grids. These norms arise from submodular functions and allow the incorporation of prior assumptions on the structure of the input variables. In the context of hierarchical norms, this structure can be represented as a [[directed acyclic graph]] over the variables while in the context of grid-based norms, the structure can be represented using a grid.<ref name=":2" /><ref name=":3" /><ref name=":4" /><ref name=":1" /><ref name=":5" /><ref name=":6" />

==== Hierarchical Norms ====
''See:'' [[Unsupervised learning]]

Unsupervised learning methods are often used to learn the parameters of [[latent variable model]]s. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. Often in such models, "hierarchies" are assumed between the variables of the system; this system of hierarchies can be represented using directed acyclic graphs.

Hierarchies of latent variables have emerged as a natural structure in several applications, notably to model text documents.<ref name=":3">Bengio, Y. "Learning deep architectures for AI". Foundations and Trends in Machine Learning, 2(1), 2009.</ref>  Hierarchical models using Bayesian non-parametric methods have been used to learn [[topic model]]s,<ref name=":2">Blei, D., Ng, A., and Jordan, M. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, 2003.</ref> which are statistical models for discovering the abstract "topics" that occur in a collection of documents. Hierarchies have also been considered in the context of kernel methods.<ref name=":1">{{Cite journal|arxiv=0904.3523|last1=Jenatton|first1=Rodolphe|title=Structured Variable Selection with Sparsity-Inducing Norms|journal=Journal of Machine Learning Research |volume=12|issue=2011|pages=2777–2824|last2=Audibert|first2=Jean-Yves|last3=Bach|first3=Francis|year=2011|bibcode=2009arXiv0904.3523J}}</ref> Hierarchical norms have been applied to bioinformatics,<ref name=":4">S. Kim and E. Xing. Tree-guided group Lasso for multi-task regression with structured sparsity. In Proc. ICML, 2010.</ref> computer vision and topic models.<ref name=":5">R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. In Proc. ICML, 2010.</ref>

==== Norms defined on grids ====
If the structure assumed over variables is in the form of a 1D, 2D or 3D grid, then submodular functions based on overlapping groups can be considered as norms, leading to stable sets equal to rectangular or convex shapes.<ref name=":1" /> Such methods have applications in computer vision<ref name=":6">R. Jenatton, G. Obozinski, and F. Bach. Structured sparse principal component analysis. In ''Proc. AISTATS'', 2009.</ref>

== Algorithms for computation ==

=== Best subset selection problem ===
The problem of choosing the best subset of input variables can be naturally formulated under a penalization framework as:<ref name="LR18">L. Rosasco. Lecture 10 of the Lecture Notes for 9.520: Statistical Learning Theory and Applications. Massachusetts Institute of Technology, Fall 2014. Available at https://www.mit.edu/~9.520/fall14/slides/class18/class18_sparsity.pdf</ref>
: <math>\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n V(y_i, w, x_i)  + \lambda \|w\|_0, </math>
Where <math>\|w\|_0</math> denotes the <math>\ell_0</math> "norm", defined as the number of nonzero entries of the vector <math>w</math>.

Although this formulation makes sense from a modeling perspective, it is computationally unfeasible, as it is equivalent to an exhaustive search evaluating all possible subsets of variables.<ref name="LR18" />

Two main approaches for solving the optimization problem are: 1) greedy methods, such as [[Stepwise regression|step-wise regression]] in statistics, or [[matching pursuit]] in [[signal processing]]; and 2) convex relaxation formulation approaches and [[Proximal gradient methods for learning|proximal gradient]] optimization methods.

=== Convex relaxation ===
A natural approximation for the best subset selection problem is the <math>\ell_1</math> norm regularization:<ref name="LR18" />
: <math> \min_{w\in\mathbb{R}^d}  \frac{1}{n}\sum_{i=1}^n V(y_i, w, x_i) + \lambda \|w\|_1</math>
Such a scheme is called [[basis pursuit]] or the [[Lasso (statistics)|Lasso]], which substitutes the <math>\ell_0</math> "norm" for the convex, non-differentiable <math>\ell_1</math> norm.

=== Proximal gradient methods ===
{{Main article|Proximal gradient methods for learning|l1=Proximal gradient methods}}

[[Proximal gradient methods for learning|Proximal gradient methods]], also called forward-backward splitting, are optimization methods useful for minimizing functions with a [[Convex function|convex]] and [[Differentiable function|differentiable]] component, and a convex potentially non-differentiable component.

As such, proximal gradient methods are useful for solving sparsity and structured sparsity regularization problems<ref name=":0">{{cite arXiv|last = Villa|first = S.|author2 = Rosasco, L.|author3 = Mosci, S.|author4 = Verri, A.|title = Proximal methods for the latent group lasso penalty|year = 2012|eprint = 1209.0368|class = math.OC}}</ref> of the following form: 
: <math> \min_{w\in\mathbb{R}^d}  \frac{1}{n}\sum_{i=1}^n V(y_i, w, x_i) + R(w)</math>
Where <math>V(y_i, w, x_i) </math> is a convex and differentiable [[loss function]] like the [[Loss function#Quadratic loss function|quadratic loss]], and <math>R(w) </math> is a convex potentially non-differentiable regularizer such as the <math>\ell_1</math> norm.

== Connections to Other Areas of Machine Learning ==

=== Connection to Multiple Kernel Learning ===
{{main article|Multiple kernel learning}}

Structured Sparsity regularization can be applied in the context of [[multiple kernel learning]].<ref name=":7">{{Cite journal|title = MIT 9.520 course notes Fall 2015, chapter 6|last = Rosasco|first = Lorenzo|date = Fall 2015|last2 = Poggio, Tomaso}}</ref> Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm.

In the algorithms mentioned above, a whole space was taken into consideration at once and was partitioned into groups, i.e. subspaces. A complementary point of view is to consider the case in which distinct spaces are combined to obtain a new one. It is useful to discuss this idea considering finite dictionaries. Finite dictionaries with linearly independent elements - these elements are also known as atoms - refer to finite sets of linearly independent basis functions, the linear combinations of which define hypothesis spaces. Finite dictionaries can be used to define specific kernels, as will be shown.<ref name=":7" /> Assume for this example that rather than only one dictionary, several finite dictionaries are considered.

For simplicity, the case in which there are only two dictionaries <math>A = \{a_j: X \rightarrow \R, j=1,...,p\} </math> and <math>B = \{b_t: X \rightarrow \R, t=1,...,q\} </math> where <math>q</math> and <math>p</math> are integers, will be considered. The atoms in <math>A</math> as well as the atoms in <math>B</math> are assumed to be linearly independent. Let <math>D = \{d_k: X \rightarrow \R, k=1,...,p+q\} = A \cup B </math> be the union of the two dictionaries. Consider the linear space of functions <math>H </math> given by linear combinations of the form

<math>f(x) = \sum_{i=1}^{p+q}{w^j d_j(x)} = \sum_{j=1}^{p}{w_A^j a_j(x)} + \sum_{t=1}^{q}{w_B^t b_t(x)}, x \in X </math>

for some coefficient vectors <math>w_A \in \R^p, w_B \in \R^q </math>, where <math>w=(w_A,w_B) </math>. Assume the atoms in <math>D </math> to still be linearly independent, or equivalently, that the map <math>w = (w_A, w_B) \mapsto f </math> is one to one. The functions in the space <math>H </math> can be seen as the sums of two components, one in the space <math>H_A </math>, the linear combinations of atoms in  <math>A</math> and one in <math>H_B </math>, the linear combinations of the atoms in <math>B</math>.

One choice of norm on this space is <math>||f|| = ||w_A|| + ||w_B|| </math>. Note that we can now view <math>H </math> as a function space in which  <math>H_A </math>,  <math>H_B </math> are subspaces. In view of the linear independence assumption, <math>H </math> can be identified with <math>\R^{p+q} </math> and <math>H_A, H_B </math> with <math>\R^p, \R^q </math> respectively. The norm mentioned above can be seen as the group norm in  <math>H </math>associated to the subspaces  <math>H_A </math>,  <math>H_B </math>, providing a connection to structured sparsity regularization.

Here, <math>H_A </math>,  <math>H_B </math> and <math>H </math> can be seen to be the reproducing kernel Hilbert spaces with corresponding feature maps <math>\Phi_A : X \rightarrow \R^p </math>, given by <math>\Phi_A(x) = (a_1(x),...,a_p(x)) </math>, <math>\Phi_B : X \rightarrow \R^q </math>, given by <math>\Phi_B(x) = (b_1(x),...,b_q(x)) </math>, and <math>\Phi: X \rightarrow \R^{p+q} </math>, given by the concatenation of <math>\Phi_A, \Phi_B </math>, respectively.

In the structured sparsity regularization approach to this scenario, the relevant groups of variables which the group norms consider correspond to the subspaces <math>H_A </math> and <math>H_B </math>. This approach promotes setting the groups of coefficients corresponding to these subspaces to zero as opposed to only individual coefficients, promoting sparse multiple kernel learning.

The above reasoning directly generalizes to any finite number of dictionaries, or feature maps. It can be extended to feature maps inducing infinite dimensional hypothesis

spaces.<ref name=":7" />

==== When Sparse Multiple Kernel Learning is useful ====
Considering sparse multiple kernel learning is useful in several situations including the following:
* Data fusion: When each kernel corresponds to a different kind of modality/feature.
* Nonlinear variable selection: Consider kernels <math>K_g</math> depending only one dimension of the input.

Generally sparse multiple kernel learning is particularly useful when there are many kernels and model selection and interpretability are important.<ref name=":7" />

== Additional uses and applications ==
Structured sparsity regularization methods have been used in a number of settings where it is desired to impose an ''a priori'' input variable structure to the regularization process. Some such applications are:
* [[Compressed sensing|Compressive sensing]] in [[magnetic resonance imaging]] (MRI), reconstructing MR images from a small number of measurements, potentially yielding significant reductions in MR scanning time<ref name="MRI" />
* Robust [[Facial recognition system|face recognition]] in the presence of misalignment, occlusion and illumination variation<ref name="face_recognition" />
* Uncovering [[Sociolinguistics|socio-linguistic]] associations between lexical frequencies used by Twitter authors, and the socio-demographic variables of their geographic communities<ref name="sociolinguistic" />
* Gene selection analysis of breast cancer data using priors of overlapping groups, e.g., biologically meaningful gene sets<ref name="genetic" />

== See also ==
* [[Statistical learning theory]]
* [[Regularization (mathematics)#Regularization in statistics and machine learning|Regularization]]
* [[Sparse approximation]]
* [[Proximal gradient method]]s
* [[Convex analysis]]
* [[Feature selection]]

== References ==
{{reflist}}

[[Category:Machine learning]]
[[Category:First order methods|First order methods]]
[[Category:Convex optimization]]