{{short description|Overview of the use of machine learning in several video games}}
In [[video game]]s, various [[artificial intelligence]] techniques have been used in a variety of ways, ranging from [[non-player character]] (NPC) control to [[Procedural generation|procedural content generation]] (PCG). [[Machine learning]] is a [[subset]] of artificial intelligence that focuses on using algorithms and statistical models to make machines act without specific programming. This is in sharp contrast to traditional methods of artificial intelligence such as [[search tree]]s and [[expert system]]s.

Information on machine learning techniques in the field of games is mostly known to public through [[Research|research projects]] as most gaming companies choose not to publish specific information about their [[intellectual property]]. The most publicly known application of machine learning in games is likely the use of [[deep learning]] [[Intelligent agent|agents]] that compete with professional human players in complex [[strategy game]]s. There has been a significant application of machine learning on games such as [[Atari]]/ALE, ''[[Doom (1993 video game)|Doom]]'', ''[[Minecraft]]'', ''[[StarCraft (video game)|StarCraft]]'', and car racing.<ref name="Justesen 2019 1">{{Cite journal|last1=Justesen|first1=Niels|last2=Bontrager|first2=Philip|last3=Togelius|first3=Julian|last4=Risi|first4=Sebastian|date=2019|title=Deep Learning for Video Game Playing|journal=IEEE Transactions on Games|volume=12|pages=1–20|doi=10.1109/tg.2019.2896986|issn=2475-1502|arxiv=1708.07902|s2cid=37941741}}</ref> Other games that did not originally exists as video games, such as chess and Go have also been affected by the machine learning.<ref name=":0">{{Cite journal|last1=Silver|first1=David|last2=Hubert|first2=Thomas|last3=Schrittwieser|first3=Julian|last4=Antonoglou|first4=Ioannis|last5=Lai|first5=Matthew|last6=Guez|first6=Arthur|last7=Lanctot|first7=Marc|last8=Sifre|first8=Laurent|last9=Kumaran|first9=Dharshan|date=2018-12-06|title=A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play|journal=Science|volume=362|issue=6419|pages=1140–1144|doi=10.1126/science.aar6404|pmid=30523106|issn=0036-8075|bibcode=2018Sci...362.1140S|s2cid=54457125|url=http://discovery.ucl.ac.uk/10069050/1/alphazero_preprint.pdf|doi-access=free}}</ref>

== Overview of relevant machine learning techniques ==
[[File:Colored neural network.svg|thumb|An artificial neural network is an interconnected group of nodes, akin to the vast network of [[neuron]]s in a [[brain]]. Here, each circular node represents an [[artificial neuron]] and an arrow represents a connection from the output of one artificial neuron to the input of another.]]

=== Deep learning ===
[[Deep learning]] is a subset of machine learning which focuses heavily on the use of [[artificial neural network]]s (ANN) that learn to solve complex tasks. Deep learning uses multiple layers of ANN and other techniques to progressively extract information from an input. Due to this complex layered approach, deep learning models often require powerful machines to train and run on.

==== Convolutional neural networks ====
[[Convolutional neural network]]s (CNN) are specialized ANNs that are often used to analyze image data. These types of networks are able to learn [[translation invariant]] patterns, which are patterns that are not dependent on location. CNNs are able to learn these patterns in a hierarchy, meaning that earlier convolutional layers will learn smaller local patterns while later layers will learn larger patterns based on the previous patterns.<ref name=":7">{{Cite book|last=Chollet, Francois|title=Deep learning with Python|date=2017-10-28|isbn=9781617294433|oclc=1019988472}}</ref> A CNN's ability to learn visual data has made it a commonly used tool for deep learning in games.<ref name=":2" /><ref name=":1" />

=== Recurrent neural network ===
[[Recurrent neural network]]s are a type of ANN that are designed to process sequences of data in order, one part at a time rather than all at once. An RNN runs over each part of a sequence, using the current part of the sequence along with memory of previous parts of the current sequence to produce an output. These types of ANN are highly effective at tasks such as [[speech recognition]] and other problems that depend heavily on temporal order. There are several types of RNNs with different internal configurations; the basic implementation suffers from a lack of [[Long-term memory|long term memory]] due to the [[vanishing gradient problem]], thus it is rarely used over newer implementations.<ref name=":7" />

==== Long short-term memory ====
A [[long short-term memory]] (LSTM) network is a specific implementation of a RNN that is designed to deal with the [[vanishing gradient problem]] seen in simple RNNs, which would lead to them gradually "forgetting" about previous parts of an inputted sequence when calculating the output of a current part. LSTMs solve this problem with the addition of an elaborate system that uses an additional input/output to keep track of long term data.<ref name=":7" /> LSTMs have achieved very strong results across various fields, and were used by several monumental deep learning agents in games.<ref name=":8">{{Cite web|url=https://openai.com/blog/openai-five/|title=OpenAI Five|date=2018-06-25|website=OpenAI|access-date=2019-06-04}}</ref><ref name=":2" />

=== Reinforcement learning ===
[[Reinforcement learning]] is the process of training an agent using rewards and/or punishments. The way an agent is rewarded or punished depends heavily on the problem; such as giving an agent a positive reward for winning a game or a negative one for losing. Reinforcement learning is used heavily in the field of machine learning and can be seen in methods such as [[Q-learning]], [[Direct policy search|policy search]], Deep Q-networks and others. It has seen strong performance in both the field of games and [[robotics]].<ref>{{Cite book|title=Artificial intelligence : a modern approach|last=Russell, Stuart J. (Stuart Jonathan)|others=Norvig, Peter|year=2015|isbn=9789332543515|edition= Third Indian |location=Noida, India|oclc=928841872}}</ref>

=== Neuroevolution ===
[[Neuroevolution]] involves the use of both neural networks and [[evolutionary algorithm]]s. Instead of using gradient descent like most neural networks, neuroevolution models make use of evolutionary algorithms to update neurons in the network. Researchers claim that this process is less likely to get stuck in a local minimum and is potentially faster than state of the art deep learning techniques.<ref>{{cite arxiv|last1=Clune|first1=Jeff|last2=Stanley|first2=Kenneth O.|last3=Lehman|first3=Joel|last4=Conti|first4=Edoardo|last5=Madhavan|first5=Vashisht|last6=Such|first6=Felipe Petroski|date=2017-12-18|title=Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning|eprint=1712.06567|class=cs.NE}}</ref>

== Deep learning agents ==
Machine learning [[Intelligent agent|agents]] have been used to take the place of a human player rather than function as NPCs, which are deliberately added into video games as part of designed [[gameplay]]. Deep learning agents have achieved impressive results when used in competition with both humans and other artificial intelligence agents.<ref name=":0" /><ref>{{Citation|last1=Zhen|first1=Jacky Shunjie|title=Neuroevolution for Micromanagement in the Real-Time Strategy Game Starcraft: Brood War|date=2013|work=Lecture Notes in Computer Science|pages=259–270|publisher=Springer International Publishing|isbn=9783319036793|last2=Watson|first2=Ian|doi=10.1007/978-3-319-03680-9_28|citeseerx=10.1.1.703.5110}}</ref>

=== Chess ===
[[Chess]] is a [[turn-based strategy]] game that is considered a difficult AI problem due to the [[computational complexity]] of its board space. Similar strategy games are often solved with some form of a [[Minimax]] Tree Search. These types of AI agents have been known to beat professional human players, such as the historic 1997 [[Deep Blue versus Garry Kasparov]] match. Since then, machine learning agents have shown ever greater success than previous AI agents.

=== Go ===
[[Go (game)|Go]] is another turn-based strategy game which is considered an even more difficult AI problem than chess. The state space of is Go is around 10^170 possible board states compared to the 10^120 board states for Chess. Prior to recent deep learning models, AI Go agents were only able to play at the level of a human amateur.<ref name=":1">{{Cite journal|last1=Silver|first1=David|last2=Huang|first2=Aja|last3=Maddison|first3=Chris J.|last4=Guez|first4=Arthur|last5=Sifre|first5=Laurent|last6=van den Driessche|first6=George|last7=Schrittwieser|first7=Julian|last8=Antonoglou|first8=Ioannis|last9=Panneershelvam|first9=Veda|date=January 2016|title=Mastering the game of Go with deep neural networks and tree search|journal=Nature|volume=529|issue=7587|pages=484–489|doi=10.1038/nature16961|pmid=26819042|issn=0028-0836|bibcode=2016Natur.529..484S|s2cid=515925}}</ref>

==== AlphaGo ====
Google's 2015 [[AlphaGo]] was the first AI agent to beat a professional Go player.<ref name=":1" /> AlphaGo used a deep learning model to train the weights of a [[Monte Carlo tree search]] (MCTS). The deep learning model consisted of 2 ANN, a policy network to predict the probabilities of potential moves by opponents, and a value network to predict the win chance of a given state. The deep learning model allows the agent to explore potential game states more efficiently than a vanilla MCTS. The network were initially trained on games of humans players and then were further trained by games against itself.

==== AlphaGo Zero ====
[[AlphaGo Zero]], another implementation of AlphaGo, was able to train entirely by playing against itself. It was able to quickly train up to the capabilities of the previous agent.<ref>{{Cite journal|last1=Silver|first1=David|last2=Schrittwieser|first2=Julian|last3=Simonyan|first3=Karen|last4=Antonoglou|first4=Ioannis|last5=Huang|first5=Aja|last6=Guez|first6=Arthur|last7=Hubert|first7=Thomas|last8=Baker|first8=Lucas|last9=Lai|first9=Matthew|date=October 2017|title=Mastering the game of Go without human knowledge|journal=Nature|volume=550|issue=7676|pages=354–359|doi=10.1038/nature24270|pmid=29052630|issn=0028-0836|bibcode=2017Natur.550..354S|s2cid=205261034|url=http://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf}}</ref>

=== ''StarCraft'' series ===
''[[StarCraft (video game)|StarCraft]]'' and its sequel ''[[StarCraft II]]'' are [[real-time strategy]] (RTS) video games that have become popular environments for AI research. [[Blizzard]] and [[DeepMind]] have worked together to release a public ''StarCraft 2'' environment for AI research to be done on.<ref>{{cite arxiv|last1=Tsing|first1=Rodney|last2=Repp|first2=Jacob|last3=Ekermo|first3=Anders|last4=Lawrence|first4=David|last5=Brunasso|first5=Anthony|last6=Keet|first6=Paul|last7=Calderone|first7=Kevin|last8=Lillicrap|first8=Timothy|last9=Silver|first9=David|date=2017-08-16|title=StarCraft II: A New Challenge for Reinforcement Learning|eprint=1708.04782|class=cs.LG}}</ref> Various deep learning methods have been tested on both games, though most agents usually have trouble outperforming the default AI with cheats enabled or skilled players of the game.<ref name="Justesen 2019 1"/>

==== Alphastar ====
[[AlphaStar (software)|Alphastar]] was the first AI agent to beat professional ''StarCraft 2'' players without any in-game advantages. The deep learning network of the agent initially received input from a simplified zoomed out version of the gamestate, but was later updated to play using a camera like other human players. The developers have not publicly released the code or architecture of their model, but have listed several state of the art machine learning techniques such as relational deep reinforcement learning, [[long short-term memory]], auto-regressive policy heads, pointer networks, and centralized value baseline.<ref name=":2">{{Cite web|url=https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/|title=AlphaStar: Mastering the Real-Time Strategy Game StarCraft II|website=DeepMind|access-date=2019-06-04}}</ref> Alphastar was initially trained with supervised learning, it watched replays of many human games in order to learn basic strategies. It then trained against different versions of itself and was improved through reinforcement learning. The final version was hugely successful, but only trained to play on a specific map in a protoss mirror matchup.

=== ''Dota 2'' ===
''[[Dota 2]]'' is a [[multiplayer online battle arena]] (MOBA) game. Like other complex games, traditional AI agents have not been able to compete on the same level as professional human player. The only widely published information on AI agents attempted on ''Dota 2'' is [[OpenAI]]'s deep learning Five agent.

==== OpenAI Five ====
[[OpenAI Five]] utilized separate [[Long short-term memory|LSTM]] networks to learn each hero. It trained using a [[reinforcement learning]] technique known as Proximal Policy Learning running on a system containing 256 [[Graphics processing unit|GPUs]] and 128,000 [[Multi-core processor|CPU cores]].<ref name=":8" /> Five trained for months, accumulating 180 years of game experience each day, before facing off with professional players.<ref name=":3">{{Cite web|url=https://openai.com/five/|title=OpenAI Five|website=OpenAI|access-date=2019-06-04}}</ref><ref name=":4">{{Cite web|url=https://openai.com/blog/how-to-train-your-openai-five/|title=How to Train Your OpenAI Five|date=2019-04-15|website=OpenAI|access-date=2019-06-04}}</ref> It was eventually able to beat the 2018 ''Dota 2'' [[esports]] champion team in a 2019 series of games.

=== ''Planetary Annihilation'' ===
''[[Planetary Annihilation]]'' is a real-time strategy game which focuses on massive scale war. The developers use ANNs in their default AI agent.<ref>{{Cite web|url=https://www.engadget.com/2014/06/06/meet-the-computer-thats-learning-to-kill-and-the-man-who-progra/|title=Meet the computer that's learning to kill and the man who programmed the chaos|last=xavdematos|website=Engadget|access-date=2019-06-04}}</ref>

=== Supreme Commander 2 ===
[[Supreme Commander 2]] is a [[real-time strategy]] (RTS) video game. 
The game uses [[Multilayer perceptron|Multilayer Perceptrons]] (MLPs) to control a platoon’s reaction to encountered enemy units. Total of four MLPs are used, one for each platoon type: land, naval, bomber, and fighter.
<ref>http://www.gameaipro.com/GameAIPro/GameAIPro_Chapter30_Using_Neural_Networks_to_Control_Agent_Threat_Response.pdf</ref>

=== Generalized games ===
There have been attempts to make machine learning agents that are able to play more than one game. These "general" gaming agents are trained to understand games based on shared properties between them.

==== AlphaZero ====
[[AlphaZero]] is a modified version of [[AlphaGo Zero]] which is able to play [[Shogi]], [[chess]], and [[Go (game)|Go]]. The modified agent starts with only basic rules of the game, and is also trained entirely through self-learning. DeepMind was able to train this generalized agent to be competitive with previous versions of itself on Go, as well as top agents in the other two games.<ref name=":0"/>

=== Strengths and weaknesses of deep learning agents ===
Machine learning agents are often not covered in many game design courses. Previous use of machine learning agents in games may not have been very practical, as even the 2015 version of AlphaGo took hundreds of CPUs and GPUs to train to a strong level.<ref name=":0" /> This potentially limits the creation of highly effective deep learning agents to large corporations or extremely wealthy individuals. The extensive training time of neural network based approaches can also take weeks on these powerful machines.<ref name=":2" />

The problem of effectively training ANN based models extends beyond powerful hardware environments; finding a good way to represent data and learn meaningful things from it is also often a difficult problem. ANN models often overfit to very specific data and perform poorly in more generalized cases. AlphaStar shows this weakness, despite being able to beat professional players, it is only able to do so on a single map when playing a mirror protoss matchup.<ref name=":2" /> OpenAI Five also shows this weakness, it was only able to beat professional player when facing a very limited hero pool out of the entire game.<ref name=":4" /> This example show how difficult it can be to train a deep learning agent to perform in more generalized situations.

Machine learning agents have shown great success in a variety of different games.<ref name=":3" /><ref name=":0" /><ref name=":2" /> However, agents that are too competent also risk making games too difficult for new or casual players. Research has shown that challenge that is too far above a player's skill level will ruin lower player enjoyment.<ref>{{Cite journal|last1=Sweetser|first1=Penelope|last2=Wyeth|first2=Peta|date=2005-07-01|title=GameFlow|journal=Computers in Entertainment|volume=3|issue=3|pages=3|doi=10.1145/1077246.1077253|s2cid=2669730|issn=1544-3574}}</ref> These highly trained agents are likely only desirable against very skilled human players who have many of hours of experience in a given game. Given these factors, highly effective deep learning agents are likely only a desired choice in games that have a large competitive scene, where they can function as an alternative practice option to a skilled human player.

== Computer vision-based players ==
[[Computer vision]] focuses on training computers to gain a high-level understanding of digital images or videos. Many computer vision techniques also incorporate forms of machine learning, and have been applied on various video games. This application of computer vision focuses on interpreting game events using visual data. In some cases, artificial intelligence agents have used [[model-free]] techniques to learn to play games without any direct connection to internal game logic, solely using video data as input.

=== ''Pong'' ===
[[Andrej Karpathy]] has demonstrated that relatively trivial neural network with just one hidden layer is capable of being trained to play ''[[Pong]]'' based on screen data alone.<ref name=IBMdeveloper/><ref>{{Cite web|url=http://karpathy.github.io/2016/05/31/rl/|title=Deep Reinforcement Learning: Pong from Pixels|website=karpathy.github.io|access-date=2020-02-03}}</ref>

=== Atari games ===
In 2013, a team at [[DeepMind]] demonstrated the use of [[deep Q-learning]] to play a variety of [[Atari]] video games &mdash; ''[[Beamrider]]'', ''[[Breakout (video game)|Breakout]]'', ''[[Enduro (video game)|Enduro]]'', ''[[Pong]]'', ''[[Q*bert]]'', ''[[Seaquest (video game)|Seaquest]]'', and ''[[Space Invaders]]'' &mdash; from screen data.<ref>{{cite arxiv|last1=Mnih|first1=Volodymyr|last2=Kavukcuoglu|first2=Koray|last3=Silver|first3=David|last4=Graves|first4=Alex|last5=Antonoglou|first5=Ioannis|last6=Wierstra|first6=Daan|last7=Riedmiller|first7=Martin|date=2013-12-19|title=Playing Atari with Deep Reinforcement Learning|eprint=1312.5602|class=cs.LG}}</ref> The team expanded their work to create a learning algorithm called MuZero that was able the "learn" the rules and develop winning strategies for over 50 different Atari games based on screen data.<ref>{{cite web | url = https://www.engadget.com/deepmind-muzero-160024950.html | title = DeepMind's latest AI can master games without being told their rules | first = Igor | last = Bonifacic | date =December 23, 2020 | access-date = December 23, 2020 | work = [[Engadget]] }}</ref><ref>{{cite journal | title = Mastering Atari, Go, chess and shogi by planning with a learned model | doi = 10.1038/s41586-020-03051-4 | journal = [[Nature (journal)|Nature]] | volume =  588 | pages = 604–609 | year = 2020 | first1= Julian |last1=Schrittwieser| first2= Ioannis |last2=Antonoglou| first3= Thomas |last3=Hubert| first4= Karen |last4=Simonyan| first5= Laurent |last5=Sifre| first6= Simon |last6=Schmitt| first7= Arthur |last7=Guez| first8= Edward |last8=Lockhart | first9=  Demis |last9=Hassabis | first10= Thore |last10=Graepel | first11= Timothy |last11=Lillicrap | first12 = David |last12=Silver | issue = 7839 | pmid = 33361790 | arxiv= 1911.08265 | bibcode = 2020Natur.588..604S | s2cid = 208158225 }}</ref>

=== ''Doom'' ===
''[[Doom (1993 video game)|Doom]]'' (1993) is a first-person shooter (FPS) game. Student researchers from [[Carnegie Mellon University]] used computer vision techniques to create an agent that could play the game using only image pixel input from the game. The students used [[convolutional neural network]] (CNN) layers to interpret incoming image data and output valid information to a [[recurrent neural network]] which was responsible for outputting game moves.<ref>{{Cite journal|last1=Lample|first1=Guillaume|last2=Chaplot|first2=Devendra Singh|date=2017|title=Playing FPS Games with Deep Reinforcement Learning|url=http://dl.acm.org/citation.cfm?id=3298483.3298548|journal=Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence|series=AAAI'17|location=San Francisco, California, USA|publisher=AAAI Press|pages=2140–2146|bibcode=2016arXiv160905521L|arxiv=1609.05521}}</ref>

=== ''Super Mario'' ===
Other uses of vision-based [[deep learning]] techniques for playing games have included playing ''[[Super Mario Bros.]]'' only using image input, using [[deep Q-learning]] for training.<ref name=IBMdeveloper>{{Cite web|url=https://developer.ibm.com/technologies/artificial-intelligence/articles/machine-learning-and-gaming/|title=Machine learning and gaming|website=IBM Developer|language=en-US|access-date=2020-02-03|first=M. Tim|last=Jones|date=June 7, 2019}}</ref>

== Machine learning for procedural content generation in games ==
Machine learning has seen research for use in content recommendation and generation. [[Procedural generation|Procedural content generation]] is the process of creating data algorithmically rather than manually. This type of content is used to add replayability to games without relying on constant additions by human developers. PCG has been used in various games for different types of content generation, examples of which include weapons in ''[[Borderlands 2]]'',<ref>{{Cite web|url=https://www.eurogamer.net/articles/2012-07-16-how-many-weapons-are-in-borderlands-2|title=How many weapons are in Borderlands 2?|last=Yin-Poole|first=Wesley|date=2012-07-16|website=Eurogamer|access-date=2019-06-04}}</ref> all world layouts in [[Minecraft]]<ref>{{Cite web|url=https://notch.tumblr.com/post/3746989361/terrain-generation-part-1|title=Terrain generation, Part 1|website=The Word of Notch|access-date=2019-06-04}}</ref> and entire universes in ''[[No Man's Sky]]''.<ref>{{Cite web|url=https://www.technologyreview.com/s/529136/no-mans-sky-a-vast-game-crafted-by-algorithms/|title=A Science Fictional Universe Created by Algorithms|last=Parkin|first=Simon|website=MIT Technology Review|access-date=2019-06-04}}</ref> Common approaches to PCG include techniques that involve [[Formal grammar|grammars]], [[Search algorithm|search-based algorithms]], and [[logic programming]].<ref>{{Citation|last1=Togelius|first1=Julian|title=Introduction|date=2016|work=Procedural Content Generation in Games|pages=1–15|publisher=Springer International Publishing|isbn=9783319427140|last2=Shaker|first2=Noor|last3=Nelson|first3=Mark J.|doi=10.1007/978-3-319-42716-4_1}}</ref> These approaches require humans to manually define the range of content possible, meaning that a human developer decides what features make up a valid piece of generated content. Machine learning is theoretically capable of learning these features when given examples to train off of, thus greatly reducing the complicated step of developers specifying the details of content design.<ref name=":5">{{Cite journal|last1=Summerville|first1=Adam|last2=Snodgrass|first2=Sam|last3=Guzdial|first3=Matthew|last4=Holmgard|first4=Christoffer|last5=Hoover|first5=Amy K.|last6=Isaksen|first6=Aaron|last7=Nealen|first7=Andy|last8=Togelius|first8=Julian|date=September 2018|title=Procedural Content Generation via Machine Learning (PCGML)|journal=IEEE Transactions on Games|volume=10|issue=3|pages=257–270|doi=10.1109/tg.2018.2846639|issn=2475-1502|arxiv=1702.00539|s2cid=9950600}}</ref> Machine learning techniques used for content generation include [[Long short-term memory|Long Short-Term Memory]] (LSTM) [[Recurrent neural network|Recurrent Neural Networks]] (RNN), [[Generative adversarial network|Generative Adversarial networks]] (GAN), and [[K-means clustering]]. Not all of these techniques make use of ANNs, but the rapid development of deep learning has greatly increased the potential of techniques that do.<ref name=":5" />

=== ''Galactic Arms Race'' ===
''[[Galactic Arms Race]]'' is a space shooter video game that uses [[neuroevolution]] powered PCG to generate unique weapons for the player. This game was a finalist in the 2010 Indie Game Challenge and its related research paper won the Best Paper Award at the 2009 IEEE Conference on Computational Intelligence and Games. The developers use a form of neuroevolution called cgNEAT to generate new content based on each player's personal preferences.<ref>{{Cite journal|last1=Hastings|first1=Erin J.|last2=Guha|first2=Ratan K.|last3=Stanley|first3=Kenneth O.|date=September 2009|title=Evolving content in the Galactic Arms Race video game|journal=2009 IEEE Symposium on Computational Intelligence and Games|pages=241–248|publisher=IEEE|doi=10.1109/cig.2009.5286468|isbn=9781424448142|s2cid=16598064|url=http://eplex.cs.ucf.edu/papers/hastings_cig09.pdf}}</ref>

Each generated item is represented by a special ANN known as a [[Compositional pattern-producing network|Compositional Pattern Producing Network]] (CPPNs). During the evolutionary phase of the game cgNEAT calculates the fitness of current items based on player usage and other gameplay metrics, this fitness score is then used decide which CPPNs will reproduce to create a new item. The ending result is the generation of new weapon effects based on the player's preference.

=== ''Super Mario Bros.'' ===
''[[Super Mario Bros.]]'' has been used by several researchers to simulate PCG level creation. Various attempts having used different methods. A version in 2014 used n-grams to generate levels similar to the ones it trained on, which was later improved by making use of MCTS to guide generation.<ref>{{Cite web|url=https://www.aaai.org/ocs/index.php/AIIDE/AIIDE15/paper/view/11569|title=MCMCTS PCG 4 SMB: Monte Carlo Tree Search to Guide Platformer Level Generation|last=Summerville|first=Adam|website=www.aaai.org|access-date=2019-06-04}}</ref> These generations were often not optimal when taking gameplay metrics such as player movement into account, a separate research project in 2017 tried to resolve this problem by generating levels based on player movement using Markov Chains.<ref>{{Cite journal|last1=Snodgrass|first1=Sam|last2=Ontañón|first2=Santiago|date=August 2017|title=Player Movement Models for Video Game Level Generation|journal=Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence|pages=757–763|location=California|publisher=International Joint Conferences on Artificial Intelligence Organization|doi=10.24963/ijcai.2017/105|isbn=9780999241103|doi-access=free}}</ref> These projects were not subjected to human testing and may not meet human playability standards.

=== ''The Legend of Zelda'' ===
PCG level creation for ''[[The Legend of Zelda (video game)|The Legend of Zelda]]'' has been attempted by researchers at the University of California, Santa Cruz. This attempt made use of a Bayesian Network to learn high level knowledge from existing levels, while Principal Component Analysis (PCA) was used to represent the different low level features of these levels.<ref>{{Cite web|url=https://www.aaai.org/ocs/index.php/AIIDE/AIIDE15/paper/view/11570|title=Sampling Hyrule: Multi-Technique Probabilistic Level Generation for Action Role Playing Games|last=Summerville|first=James|website=www.aaai.org|access-date=2019-06-04}}</ref> The researchers used PCA to compare generated levels to human made levels and found that they were considered very similar. This test did not include playability or human testing of the generated levels.

== Music generation ==
Music is often seen in video games and can be a crucial element for influencing the mood of different situations and story points. Machine learning has seen use in the experimental field of music generation; it is uniquely suited to processing raw [[unstructured data]] and forming high level representations that could be applied to the diverse field of music.<ref name=":6">{{cite arxiv|last1=Pachet|first1=François-David|last2=Hadjeres|first2=Gaëtan|last3=Briot|first3=Jean-Pierre|date=2017-09-05|title=Deep Learning Techniques for Music Generation - A Survey|eprint=1709.01620|class=cs.SD}}</ref> Most attempted methods have involved the use of ANN in some form. Methods include the use of basic [[feedforward neural network]]s, [[autoencoder]]s, [[Restricted Boltzmann machine|restricted boltzmann machines]], [[recurrent neural network]]s, [[convolutional neural network]]s, [[generative adversarial network]]s (GANs), and compound architectures that use multiple methods.<ref name=":6" />

=== VRAE video game melody symbolic music generation system ===
The 2014 research paper on "Variational Recurrent Auto-Encoders" attempted to generate music based on songs from 8 different video games. This project is one of the few conducted purely on video game music. The neural network in the project was able to generate data that was very similar to the data of the games it trained off of.<ref>{{cite arxiv|last1=van Amersfoort|first1=Joost R.|last2=Fabius|first2=Otto|date=2014-12-20|title=Variational Recurrent Auto-Encoders|eprint=1412.6581|class=stat.ML}}</ref> The generated data did not translate into good quality music.

== References ==
{{Reflist}}

== External links ==


{{Differentiable computing}}

[[Category:Machine learning]]
[[Category:Game artificial intelligence]]