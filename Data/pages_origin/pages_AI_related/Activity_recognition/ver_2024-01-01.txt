{{Short description|Recognition of events from videos or sensors}}
{{research paper|date=March 2018|reason=academic unencyclopedic phrasings, excessive in-text attribution, unsourced / irrelevant claims of importance and other statements of opinion}}
'''Activity recognition''' aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several [[computer science]] communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine, [[Human–computer interaction|human-computer interaction]], or sociology.

Due to its multifaceted nature, different fields may refer to activity recognition as plan recognition, goal recognition, intent recognition, behavior recognition, location estimation and [[location-based service]]s.

==Types==

===Sensor-based, single-user activity recognition===
[[Sensor]]-based activity recognition integrates the emerging area of sensor networks with novel [[data mining]] and [[machine learning]] techniques to model a wide range of human activities.<ref>Tanzeem Choudhury, [[Gaetano Borriello]], et al. The Mobile Sensing Platform: An Embedded System for Activity Recognition. Appears in the IEEE Pervasive Magazine – Special Issue on Activity-Based Computing, April 2008.</ref><ref>Nishkam Ravi, Nikhil Dandekar, Preetham Mysore, Michael Littman. [http://www.aaai.org/Papers/AAAI/2005/IAAI05-013.pdf Activity Recognition from Accelerometer Data]. Proceedings of the Seventeenth Conference on Innovative Applications of Artificial Intelligence (IAAI/AAAI 2005).</ref> Mobile devices (e.g. smart phones) provide sufficient sensor data and calculation power to enable physical activity recognition to provide an estimation of the energy consumption during everyday life. Sensor-based activity recognition researchers believe that by empowering [[ubiquitous computing|ubiquitous computers]] and sensors to monitor the behavior of agents (under consent), these computers will be better suited to act on our behalf. Visual sensors that incorporate color and depth information, such as the [[Kinect]], allow more accurate automatic action recognition and fuse many emerging applications such as interactive education<ref>{{cite journal |last1=Yang |first1=Yang |last2=Leung |first2=Howard |last3=Shum |first3=Hubert P. H. |last4=Li |first4=Jiao |last5=Zeng |first5=Lanling |last6=Aslam |first6=Nauman |last7=Pan |first7=Zhigeng |title=CCESK: A Chinese Character Educational System Based on Kinect |journal=IEEE Transactions on Learning Technologies |date=2018 |volume=11 |issue=3 |pages=342–347 |doi=10.1109/TLT.2017.2723888|s2cid=52899136 }}</ref> and smart environments.<ref>{{cite journal |last1=Ho |first1=Edmond S. L. |last2=Chan |first2=Jacky C. P. |last3=Chan |first3=Donald C. K. |last4=Shum |first4=Hubert P. H. |last5=Cheung |first5=Yiu-ming |last6=Yuen |first6=P. C. |title=Improving Posture Classification Accuracy for Depth Sensor-based Human Activity Monitoring in Smart Environments |journal=Computer Vision and Image Understanding |date=2016 |volume=148 |pages=97–110 |doi=10.1016/j.cviu.2015.12.011|s2cid=207060860 |doi-access=free }}</ref> Multiple views of visual sensor enable the development of machine learning for automatic view invariant action recognition.<ref>{{cite journal |last1=Zhang |first1=Jingtian |last2=Shum |first2=Hubert P. H. |last3=Han |first3=Jungong |last4=Shao |first4=Ling |title=Action Recognition from Arbitrary Views Using Transferable Dictionary Learning |journal=IEEE Transactions on Image Processing |date=2018 |volume=27 |issue=10 |pages=4709–4723 |doi=10.1109/TIP.2018.2836323|pmid=29994770 |bibcode=2018ITIP...27.4709Z |s2cid=49536771 |doi-access=free }}</ref> More advanced sensors used in 3D [[motion capture]] systems allow highly accurate automatic recognition, in the expenses of more complicated hardware system setup.<ref>{{cite journal |last1=Shen |first1=Yijun |last2=Yang |first2=Longzhi |last3=Ho |first3=Edmond S. L. |last4=Shum |first4=Hubert P. H. |title=Interaction-based Human Activity Comparison |journal=IEEE Transactions on Visualization and Computer Graphics |date=2020 |volume=26 |issue=8 |pages=115673–115684 |doi=10.1109/TVCG.2019.2893247|pmid=30703028 |s2cid=73447673 |doi-access=free }}</ref> 

====Levels of sensor-based activity recognition====
Sensor-based activity recognition is a challenging task due to the inherent noisy nature of the input. Thus, [[statistical modeling]] has been the main thrust in this direction in layers, where the recognition at several intermediate levels is conducted and connected. At the lowest level where the sensor data are collected, statistical learning concerns how to find the detailed locations of agents from the received signal data. At an intermediate level, [[statistical inference]] may be concerned about how to recognize individuals' activities from the inferred location sequences and environmental conditions at the lower levels. Furthermore, at the highest level, a major concern is to find out the overall goal or subgoals of an agent from the activity sequences through a mixture of logical and statistical reasoning.

===Sensor-based, multi-user activity recognition===
Recognizing activities for multiple users using on-body sensors first appeared in the work by ORL using active badge systems<ref>Want R., Hopper A., Falcao V., Gibbons J.: The Active Badge Location System, ACM Transactions on Information, Systems, Vol. 40, No. 1, pp. 91–102, January 1992</ref> in the early 1990s. Other sensor technology such as acceleration sensors were used for identifying group activity patterns during office scenarios.<ref>Bieber G., Kirste T., Untersuchung des gruppendynamischen Aktivitaetsverhaltes im Office-Umfeld, 7. Berliner Werkstatt Mensch-Maschine-Systeme, Berlin, Germany, 2007</ref> Activities of Multiple Users in intelligent environments are addressed in Gu ''et al''.<ref>Tao Gu, Zhanqing Wu, Liang Wang, Xianping Tao, and Jian Lu. [https://ieeexplore.ieee.org/abstract/document/5326404/ Mining Emerging Patterns for Recognizing Activities of Multiple Users in Pervasive Computing]. In Proc. of the 6th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services (MobiQuitous '09), Toronto, Canada, July 13–16, 2009.</ref> In this work, they investigate the fundamental problem of recognizing activities for multiple users from sensor readings in a home environment, and propose a novel pattern mining approach to recognize both single-user and multi-user activities in a unified solution.

===Sensor-based group activity recognition===
Recognition of group activities is fundamentally different from single, or multi-user activity recognition in that the goal is to recognize the behavior of the group as an entity, rather than the activities of the individual members within it.<ref>Dawud Gordon, Jan-Hendrik Hanne, Martin Berchtold, Ali Asghar Nazari Shirehjini, Michael Beigl: [http://www.teco.edu/~gordon/publications/MONE_GAR.pdf Towards Collaborative Group Activity Recognition Using Mobile Devices], Mobile Networks and Applications 18(3), 2013, pp. 326–340</ref>  Group behavior is emergent in nature, meaning that the properties of the behavior of the group are fundamentally different than the properties of the behavior of the individuals within it, or any sum of that behavior.<ref>Lewin, K. Field theory in social science: selected theoretical papers. Social science paperbacks. Harper, New York, 1951.</ref>  The main challenges are in modeling the behavior of the individual group members, as well as the roles of the individual within the group dynamic<ref>Hirano, T., and Maekawa, T. [http://www-nishio.ist.osaka-u.ac.jp/~maekawa/paper/maekawa-ISWC2013.pdf A hybrid unsupervised/supervised model for group activity recognition]. In Proceedings of the 2013 International Symposium on Wearable Computers, ISWC ’13, ACM (New York, NY, USA, 2013), 21–24</ref> and their relationship to emergent behavior of the group in parallel.<ref>Brdiczka, O., Maisonnasse, J., Reignier, P., and Crowley, J. L. [http://www-prima.imag.fr/Prima/Homepages/jlc/papers/BrdiczkaAppliedIntelligence.pdf Detecting small group activities from multimodal observations]. Applied Intelligence 30, 1 (July 2007), 47–57.</ref>  Challenges which must still be addressed include quantification of the behavior and roles of individuals who join the group, integration of explicit models for role description into inference algorithms, and scalability evaluations for very large groups and crowds.  Group activity recognition has applications for crowd management and response in emergency situations, as well as for [[Social networking service|social networking]] and [[Quantified Self]] applications.<ref>Dawud Gordon, [https://www.researchgate.net/profile/Dawud_Gordon/publication/260342656_Recognizing_Group_Activities_Using_Wearable_Sensors/links/54a82b2e0cf257a6360bdd30.pdf Group Activity Recognition Using Wearable Sensing Devices], Dissertation, Karlsruhe Institute of Technology, 2014</ref>

==Approaches==

===Activity recognition through logic and reasoning===
Logic-based approaches keep track of all [[logically consistent]] explanations of the observed actions. Thus, all possible and consistent plans or goals must be considered. Kautz provided a formal theory of plan recognition. He described plan recognition as a logical inference process of circumscription. All actions and plans are uniformly referred to as goals, and a recognizer's knowledge is represented by a set of first-order statements, called event hierarchy.  Event hierarchy is encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events.<ref>H. Kautz. "[https://urresearch.rochester.edu/fileDownloadForInstitutionalItem.action?itemId=5934&itemFileId=9297 A formal theory of plan recognition]". In PhD thesis, University of Rochester, 1987.</ref>

Kautz's general framework for plan recognition has an exponential time complexity in worst case, measured in the size of the input hierarchy. Lesh and Etzioni went one step further and presented methods in scaling up goal recognition to scale up his work computationally. In contrast to Kautz's approach where the plan library is explicitly represented, Lesh and Etzioni's approach enables automatic plan-library construction from domain primitives. Furthermore, they introduced compact representations and efficient algorithms for goal recognition on large plan libraries.<ref>N. Lesh and O. Etzioni. "[http://www.ijcai.org/Proceedings/95-2/Papers/088.pdf A sound and fast goal recognizer]". In ''Proceedings of the International Joint Conference on Artificial Intelligence'', 1995.</ref>

Inconsistent plans and goals are repeatedly pruned when new actions arrive. Besides, they also presented methods for adapting a goal recognizer to handle individual idiosyncratic behavior given a sample of an individual's recent behavior. Pollack et al. described a direct argumentation model that can know about the relative strength of several kinds of arguments for belief and intention description.

A serious problem of logic-based approaches is their inability or inherent infeasibility to represent uncertainty. They offer no mechanism for preferring one consistent approach to another and are incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed. There is also a lack of learning ability associated with logic based methods.

Another approach to logic-based activity recognition is to use stream reasoning based on [[answer set programming]],<ref>{{cite book|last=Do|first=Thang|author2=Seng W. Loke|author3=Fei Liu |title=Advances in Artificial Intelligence |chapter=Answer Set Programming for Stream Reasoning |year=2011|volume=6657|pages=104–109|doi=10.1007/978-3-642-21043-3_13|series=Lecture Notes in Computer Science|isbn=978-3-642-21042-6|citeseerx=10.1.1.453.2348}}</ref> and has been applied to recognising activities for health-related applications,<ref>{{cite journal|last=Do|first=Thang|author2=Seng W. Loke|author3=Fei Liu|title=HealthyLife: an Activity Recognition System with Smartphone using Logic-Based Stream Reasoning|journal=Proceedings of the 9th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, (Mobiquitous 2012)|year=2012|url=http://homepage.cs.latrobe.edu.au/sloke/papers/Mobiquitous2012.pdf}}</ref>  which uses weak constraints to model a degree of ambiguity/uncertainty.

===Activity recognition through probabilistic reasoning===
Probability theory and statistical learning models are more recently applied in activity recognition to reason about actions, plans and goals under uncertainty.<ref>E. Charniak and R.P. Goldman. "[https://www.sciencedirect.com/science/article/pii/000437029390060O A Bayesian model of plan recognition]". ''Artificial Intelligence'', 64:53–79, 1993.</ref> In the literature, there have been several approaches which explicitly represent uncertainty in reasoning about an agent's plans and goals.

Using sensor data as input, Hodges and Pollack designed machine learning-based systems for identifying individuals as they perform routine daily activities such as making coffee.<ref>M.R. Hodges and M.E. Pollack. "[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.5487&rep=rep1&type=pdf An 'object-use fingerprint': The use of electronic sensors for human identification]". In ''Proceedings of the 9th International Conference on Ubiquitous Computing'', 2007.</ref> [[Intel Research Lablets|Intel Research (Seattle) Lab]] and University of Washington at Seattle have done some important works on using sensors to detect human plans.<ref>Mike Perkowitz, Matthai Philipose, Donald J. Patterson, and Kenneth P. Fishkin. "[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.103.7512&rep=rep1&type=pdf Mining models of human activities from the web]". In ''Proceedings of the Thirteenth International World Wide Web Conference (WWW 2004), pages 573–582, May 2004.</ref><ref>Matthai Philipose, Kenneth P. Fishkin, Mike Perkowitz, Donald J. Patterson, Dieter Fox, Henry Kautz, and Dirk Hähnel. "[https://web.archive.org/web/20181001030913/https://ftp.cs.rochester.edu/u/kautz/papers/pervasive_proact_final.pdf Inferring activities from interactions with objects] ". In ''IEEE Pervasive Computing'', pages 50–57, October 2004.</ref><ref>Dieter Fox Lin Liao, Donald J. Patterson and Henry A. Kautz. "[http://www.aaai.org/Papers/AAAI/2004/AAAI04-056.pdf Learning and inferring transportation routines]". ''Artif. Intell.'', 171(5–6):311–331, 2007.</ref> Some of these works infer user transportation modes from readings of radio-frequency identifiers (RFID) and global positioning systems (GPS).

The use of temporal probabilistic models has been shown to perform well in activity recognition and generally outperform non-temporal models.<ref>TLM van Kasteren, Gwenn Englebienne, BJA Kröse. "[https://www.researchgate.net/profile/Tim_Van_Kasteren/publication/227017257_Human_Activity_Recognition_from_Wireless_Sensor_Network_Data_Benchmark_and_Software/links/0deec52b531429802c000000/Human-Activity-Recognition-from-Wireless-Sensor-Network-Data-Benchmark-and-Software.pdf Human activity recognition from wireless sensor network data: Benchmark and software]." Activity Recognition in Pervasive Intelligent Environments, 165–186, Atlantis Press</ref> Generative models such as the Hidden Markov Model (HMM) and the more generally formulated Dynamic Bayesian Networks (DBN) are popular choices in modelling activities from sensor data.<ref>Piyathilaka, L.; Kodagoda, S., "[https://www.researchgate.net/profile/Piyathilaka_Lasitha/publication/258341803_Gaussian_Mixture_Based_HMM_for_Human_Daily_Activity_Recognition_Using_3D_Skeleton_Features/links/0deec527f42f76c8f4000000/Gaussian-Mixture-Based-HMM-for-Human-Daily-Activity-Recognition-Using-3D-Skeleton-Features.pdf Gaussian mixture based HMM for human daily activity recognition using 3D skeleton features]," Industrial Electronics and Applications (ICIEA), 2013 8th IEEE Conference on, vol., no., pp.567,572, 19–21 June 2013</ref><ref name=":0">TLM van Kasteren, Gwenn Englebienne, Ben Kröse" [https://sites.google.com/site/tim0306/amiKasterenHierch.pdf?attredirects=0 Hierarchical Activity Recognition Using Automatically Clustered Actions]", 2011, Ambient Intelligence, 82–91, Springer Berlin/Heidelberg</ref><ref>Daniel Wilson and Chris Atkeson. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.3498&rep=rep1&type=pdf Simultaneous tracking and activityrecognition (star) using many anonymous binary sensors]. In Proceedings of the 3rd international conference on Pervasive Computing, Pervasive, pages 62–79, Munich, Germany, 2005.</ref><ref>[[Nuria Oliver]], Barbara Rosario and [[Alex Pentland]] "A Bayesian Computer Vision System for Modeling Human Interactions"
Appears in PAMI Special Issue on Visual Surveillance and Monitoring, Aug 00</ref>
Discriminative models such as Conditional Random Fields (CRF) are also commonly applied and also give good performance in activity recognition.<ref>TLM Van Kasteren, Athanasios Noulas, Gwenn Englebienne, Ben Kröse, "[https://dl.acm.org/citation.cfm?id=1409637 Accurate activity recognition in a home setting]", 2008/9/21, Proceedings of the 10th international conference on Ubiquitous computing, 1–9, ACM</ref><ref>Derek Hao Hu, Sinno Jialin Pan, Vincent Wenchen Zheng, Nathan NanLiu, and Qiang Yang. [http://www3.ntu.edu.sg/home/sinnopan/publications/%5BUbiComp2008%5DReal%20World%20Activity%20Recognition%20with%20Multiple%20Goals.pdf Real world activity recognition with multiple goals]. In Proceedings of the 10th international conference on Ubiquitous computing, Ubicomp, pages 30–39, New York, NY, USA, 2008. ACM.</ref>

Generative and discriminative models both have their pros and cons and the ideal choice depends on their area of application. A dataset together with implementations of a number of popular models (HMM, CRF) for activity recognition can be found [https://sites.google.com/site/tim0306/datasets here].

Conventional temporal probabilistic models such as the hidden Markov model (HMM) and conditional random fields (CRF) model directly model the correlations between the activities and the observed sensor data. In recent years, increasing evidence has supported the use of hierarchical models which take into account the rich hierarchical structure that exists in human behavioral data.<ref name=":0" /><ref name=":1">[[Nuria Oliver]], Ashutosh Garg, and Eric Horvitz. [https://pdfs.semanticscholar.org/6dbc/ecce4f3f642fa2b5cb907c9c165ef4dd6661.pdf Layered representations for learning and inferring office activity from multiple sensory channels]. Comput. Vis. Image Underst., 96(2):163–180, 2004.</ref><ref>Amarnag Subramanya, Alvin Raj, Jeff Bilmes, and Dieter Fox. [http://ssli.ee.washington.edu/~bilmes/mypapers/mmsp2006.pdf Hierarchical models for activity recognition]. In Proceedings of the international conference on Multimedia Signal Processing, MMSP, Victoria, CA, October 2006.</ref> The core idea here is that the model does not directly correlate the activities with the sensor data, but instead breaks the activity into sub-activities (sometimes referred to as actions) and models the underlying correlations accordingly. An example could be the activity of preparing a stir fry, which can be broken down into the subactivities or actions of cutting vegetables, frying the vegetables in a pan and serving it on a plate. Examples of such a hierarchical model are Layered Hidden Markov Models (LHMMs)<ref name=":1" /> and the hierarchical hidden Markov model (HHMM), which have been shown to significantly outperform its non-hierarchical counterpart in activity recognition.<ref name=":0" />

===Data mining based approach to activity recognition===
Different from traditional machine learning approaches, an approach based on data mining has been recently proposed. In the work of Gu et al., the problem of activity recognition is formulated as a pattern-based classification problem. They proposed a data mining approach based on discriminative patterns which describe significant changes between any two activity classes of data to recognize sequential, interleaved and concurrent activities in a unified solution.<ref>Tao Gu, Zhanqing Wu, Xianping Tao, Hung Keng Pung, and Jian Lu. [https://ieeexplore.ieee.org/abstract/document/4912776/ epSICAR: An Emerging Patterns based Approach to Sequential, Interleaved and Concurrent Activity Recognition]. In Proc. of the 7th Annual IEEE International Conference on Pervasive Computing and Communications (Percom '09), Galveston, Texas, March 9–13, 2009.</ref> Gilbert ''et al.'' use 2D corners in both space and time. These are grouped spatially and temporally using a hierarchical process, with an increasing search area. At each stage of the hierarchy, the most distinctive and descriptive features are learned efficiently through data mining (Apriori rule).<ref>Gilbert A, Illingworth J, Bowden R. [http://epubs.surrey.ac.uk/531449/1/PAMIActRec_pre_print.pdf Action Recognition using Mined Hierarchical Compound Features]. IEEE Trans Pattern Analysis and Machine Learning</ref>

===GPS-based activity recognition===
Location-based activity recognition can also rely on [[Global positioning system|GPS]] data to recognize activities.<ref>Liao, Lin, Dieter Fox, and Henry Kautz. "[http://ants.iis.sinica.edu.tw/3bkmj9ltewxtsrrvnoknfdxrm3zfwrr/80/places-isrr-05.pdf Hierarchical conditional random fields for GPS-based activity recognition]." Robotics Research. Springer, Berlin, Heidelberg, 2007. 487–506.</ref><ref>Liao, Lin, Dieter Fox, and Henry Kautz. "[http://papers.nips.cc/paper/2911-location-based-activity-recognition.pdf Location-based activity recognition]." Advances in Neural Information Processing Systems. 2006.</ref>

==Sensor usage==

===Vision-based activity recognition===
It is a very important and challenging problem to track and understand the behavior of agents through videos taken by various cameras. The primary technique employed is [[Computer Vision]]. Vision-based activity recognition has found many applications such as human-computer interaction, user interface design, [[robot learning]], and surveillance, among others.
Scientific conferences where vision based activity recognition work often appears are [[ICCV]] and [[CVPR]].

In vision-based activity recognition, a great deal of work has been done. Researchers have attempted a number of methods such as [[optical flow]], [[Kalman filtering]], [[Hidden Markov model]]s, etc., under different modalities such as single camera, [[computer stereo vision|stereo]], and infrared. In addition, researchers have considered multiple aspects on this topic, including single pedestrian tracking, group tracking, and detecting dropped objects.

Recently some researchers have used [[RGBD camera]]s like Microsoft Kinect to detect human activities.<ref name="ho16improving">{{cite journal |last1=Ho |first1=Edmond S. L. |last2=Chan |first2=Jacky C. P. |last3=Chan |first3=Donald C. K. |last4=Shum |first4=Hubert P. H. |last5=Cheung |first5=Yiu-ming |last6=Yuen |first6=P. C. |title=Improving Posture Classification Accuracy for Depth Sensor-Based Human Activity Monitoring in Smart Environments |journal=Computer Vision and Image Understanding |date=2016 |volume=148 |pages=97–110 |doi=10.1016/j.cviu.2015.12.011 |issn=1077-3142 |publisher=Elsevier |doi-access=free }}</ref> Depth cameras add extra dimension i.e. depth which normal 2d camera fails to provide. Sensory information from these depth cameras have been used to generate real-time skeleton model of humans with different body positions. <ref name="shum13realtime">{{cite journal |last1=Shum |first1=Hubert P. H. |last2=Ho |first2=Edmond S. L. |last3=Jiang |first3=Yang |last4=Takagi |first4=Shu |title=Real-Time Posture Reconstruction for Microsoft Kinect |journal=IEEE Transactions on Cybernetics |date=2013 |volume=43 |issue=5 |pages=1357–1369 |doi=10.1109/TCYB.2013.2275945 |issn=2168-2267 |publisher=IEEE |url=http://doi.org/10.1109/TCYB.2013.2275945 }}</ref> This skeleton information provides meaningful information that researchers have used to model human activities which are trained and later used to recognize unknown activities.<ref>Piyathilaka, L.; Kodagoda, S., "[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6566433&isnumber=6566328 Gaussian mixture based HMM for human daily activity recognition using 3D skeleton features]," Industrial Electronics and Applications (ICIEA), 2013 8th IEEE Conference on, vol., no., pp.567,572, 19–21 June 2013 URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6566433&isnumber=6566328</ref><ref>Piyathilaka, L. and Kodagoda, S., 2015. Human activity recognition for domestic robots. In Field and Service Robotics (pp. 395–408). Springer, Cham.[https://www.researchgate.net/publication/258341805_Human_Activity_Recognition_for_Domestic_Robots "Human Activity Recognition for Domestic Robots"]</ref>

With the recent emergency of deep learning, RGB video based activity recognition has seen rapid development. It uses videos captured by RGB cameras as input and perform several tasks, including: video classification, detection of activity start and end in videos, and spatial-temporal localization of activity and the people performing the activity. <ref name="qiao22geometric">{{cite conference |last1=Qiao |first1=Tanqiu |last2=Men |first2=Qianhui |last3=Li |first3=Frederick W. B. |last4=Kubotani |first4=Yoshiki |last5=Morishima |first5=Shigeo |last6=Shum |first6=Hubert P. H. |title=Geometric Features Informed Multi-Person Human-Object Interaction Recognition in Videos |series=Lecture Notes in Computer Science |date=2022 |volume=13664 |pages=474–491 |doi=10.1007/978-3-031-19772-7_28 |isbn=978-3-031-19772-7 |publisher=Springer |arxiv=2207.09425 }}</ref> Pose estimation methods <ref name="huang20highspeed">{{cite journal |last1=Huang |first1=Ying |last2=Shum |first2=Hubert P. H. |last3=Ho |first3=Edmond S. L. |last4=Aslam |first4=Nauman |title=High-Speed Multi-Person Pose Estimation with Deep Feature Transfer |journal=Computer Vision and Image Understanding |date=2020 |volume=197-198 |pages=103010 |doi=10.1016/j.cviu.2020.103010 |issn=1077-3142 |publisher=Elsevier |url=http://doi.org/10.1016/j.cviu.2020.103010 }}</ref> allow extracting more representative skeletal features for action recognition. <ref name="men23focalized">{{cite journal |last1=Men |first1=Qianhui |last2=Ho |first2=Edmond S. L. |last3=Shum |first3=Hubert P. H. |last4=Leung |first4=Howard |title=Focalized Contrastive View-Invariant Learning for Self-Supervised Skeleton-Based Action Recognition |journal=Neurocomputing |date=2023 |volume=537 |pages=198–209 |doi=10.1016/j.neucom.2023.03.070 |issn=0925-2312 |publisher=Elsevier |arxiv=2304.00858 }}</ref> That said, it has been discovered that deep learning based action recognition may suffer from adversarial attacks, where an attacker alter the input insignificantly to fool an action recognition system. <ref name="lu23hard">{{cite conference |last1=Lu |first1=Zhengzhi |last2=Wang |first2=He |last3=Chang |first3=Ziyi |last4=Yang |first4=Guoan |last5=Shum |first5=Hubert P. H. |title=Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient |date=2023 |publisher=IEEE/CVF |arxiv=2308.05681 }}</ref>

Despite remarkable progress of vision-based activity recognition, its usage for most actual visual surveillance applications remains a distant aspiration.<ref>{{cite journal |last1=Bux |first1=Allah |last2=Angelov |first2=Plamen |last3=Habib |first3=Zulfiqar |title=A comprehensive review on handcrafted and learning-based action representation approaches for human activity recognition |journal=Applied Sciences |date=2017 |volume=7 |issue=1 |page=110 |doi=10.3390/app7010110|doi-access=free }}</ref> Conversely, the human brain seems to have perfected the ability to recognize human actions. This capability relies not only on acquired knowledge, but also on the aptitude of extracting information relevant to a given context and logical reasoning. Based on this observation, it has been proposed to enhance vision-based activity recognition systems by integrating [[commonsense reasoning]] and, contextual and [[Commonsense knowledge (artificial intelligence)|commonsense knowledge]].

'''Hierarchical Human Activity (HAR) Recognition'''

Hierarchical human activity recognition is a technique within computer vision and machine learning. It aims to identify and comprehend human actions or behaviors from visual data. This method entails structuring activities hierarchically, creating a framework that represents connections and interdependencies among various actions.<ref>{{Cite journal |last1=Aggarwal |first1=J.K. |last2=Ryoo |first2=M.S. |date=2011-04-29 |title=Human activity analysis: A review |url=https://doi.org/10.1145/1922649.1922653 |journal=ACM Computing Surveys |volume=43 |issue=3 |pages=16:1–16:43 |doi=10.1145/1922649.1922653 |s2cid=5388357 |issn=0360-0300}}</ref> HAR techniques can be used to understand data correlations and model fundamentals to improve models, to balance accuracy and privacy concerns in sensitive application areas, and to identify and manage trivial labels that have no relevance in specific use cases.<ref>{{Cite journal |last1=Altın |first1=Mahsun |last2=Gürsoy |first2=Furkan |last3=Xu |first3=Lina |date=2021 |title=Machine-Generated Hierarchical Structure of Human Activities to Reveal How Machines Think |journal=IEEE Access |volume=9 |pages=18307–18317 |doi=10.1109/ACCESS.2021.3053084 |bibcode=2021IEEEA...918307A |issn=2169-3536|doi-access=free }}</ref>

====Levels of vision-based activity recognition====
In vision-based activity recognition, the computational process is often divided into four steps, namely human detection, human tracking, human activity recognition and then a high-level activity evaluation.

====Fine-grained action localization====
{{main|Object co-segmentation}}

In [[computer vision]]-based activity recognition, fine-grained action localization typically provides per-image segmentation masks delineating the human object and its action category (e.g., ''Segment-Tube''<ref name="Wang Duan Zhang Niu p=1657">{{cite journal | last1=Wang | first1=Le | last2=Duan | first2=Xuhuan | last3=Zhang | first3=Qilin | last4=Niu | first4=Zhenxing | last5=Hua | first5=Gang | last6=Zheng | first6=Nanning | title=Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation | journal=Sensors | volume=18 | issue=5 | date=2018-05-22 | issn=1424-8220 | doi=10.3390/s18051657 | pmid=29789447 | pmc=5982167 | page=1657 | bibcode=2018Senso..18.1657W | url=https://qilin-zhang.github.io/_pages/pdfs/Segment-Tube_Spatio-Temporal_Action_Localization_in_Untrimmed_Videos_with_Per-Frame_Segmentation.pdf| doi-access=free }} [[File:CC-BY icon.svg|50px]].</ref>). Techniques such as dynamic [[Markov random field|Markov Networks]], [[Convolutional neural network|CNN]] and [[Long short-term memory|LSTM]] are often employed to exploit the semantic correlations between consecutive video frames. Geometric fine-grained features such as objective bounding boxes and human poses facilitate activity recognition with [[graph neural network]].<ref name="qiao22geometric">{{cite conference |last1=Qiao |first1=Tanqiu |last2=Men |first2=Qianhui |last3=Li |first3=Frederick W. B. |last4=Kubotani |first4=Yoshiki |last5=Morishima |first5=Shigeo |last6=Shum |first6=Hubert P. H. |title=Geometric Features Informed Multi-person Human-object Interaction Recognition in Videos |series=Lecture Notes in Computer Science |date=2022 |volume=13664 |pages=474–491 |doi=10.1007/978-3-031-19772-7_28 |isbn=978-3-031-19772-7 |publisher=Springer |arxiv=2207.09425 }}</ref><ref name="zhang22towards">{{cite conference |last1=Zhang |first1=Xiatian |last2=Moubayed |first2=Noura Al |last3=Shum |first3=Hubert P. H. |title=2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI) |chapter=Towards Graph Representation Learning Based Surgical Workflow Anticipation |date=2022 |pages=01–04 |doi=10.1109/BHI56158.2022.9926801 |publisher=IEEE |arxiv=2208.03824 |isbn=978-1-6654-8791-7 }}</ref>

====Automatic gait recognition====
{{Main|Gait recognition}}

One way to identify specific people is by how they walk. Gait-recognition software can be used to record a person's gait or gait feature profile in a database for the purpose of recognizing that person later, even if they are wearing a disguise.

===Wi-Fi-based activity recognition===
When activity recognition is performed indoors and in cities using the widely available [[Wi-Fi]] signals and [[802.11]] access points, there is much noise and uncertainty. These uncertainties can be modeled using a dynamic [[Bayesian network]] model.<ref>Jie Yin, Xiaoyong Chai and Qiang Yang, "[http://www.aaai.org/Papers/AAAI/2004/AAAI04-092.pdf High-level Goal Recognition in a Wireless LAN]". In ''Proceedings of the Nineteenth National Conference on Artificial Intelligence'' (AAAI-04), San Jose, CA USA, July 2004. Pages 578–584</ref> In a multiple goal model that can reason about user's interleaving goals, a [[deterministic]] state transition model is applied.<ref>Xiaoyong Chai and Qiang Yang, "[http://www.aaai.org/Papers/AAAI/2005/AAAI05-001.pdf Multiple-Goal Recognition From Low-level Signals]". ''Proceedings of the Twentieth National Conference on Artificial Intelligence'' (AAAI 2005), Pittsburgh, PA USA, July 2005. Pages 3–8.</ref> Another possible method models the concurrent and interleaving activities in a probabilistic approach.<ref>Derek Hao Hu, Qiang Yang. "[http://www.aaai.org/Papers/AAAI/2008/AAAI08-216.pdf CIGAR: Concurrent and Interleaving Goal and Activity Recognition]", to appear in AAAI 2008</ref> A user action discovery model could segment Wi-Fi signals to produce possible actions.<ref>Jie Yin, Dou Shen, Qiang Yang and Ze-nian Li "[http://www.aaai.org/Papers/AAAI/2005/AAAI05-005.pdf Activity Recognition through Goal-Based Segmentation]". ''Proceedings of the Twentieth National Conference on Artificial Intelligence'' (AAAI 2005), Pittsburgh, PA USA, July 2005. Pages 28–33.</ref>

==== Basic models of Wi-Fi recognition ====

One of the primary thought of Wi-Fi activity recognition is that when the signal goes through the human body during transmission; which causes reflection, diffraction, and scattering. Researchers can get information from these signals to analyze the activity of the human body.

===== Static transmission model =====

As shown in,<ref>D. Zhang, J. Ma, Q. Chen, and L. M. Ni, ¡°[https://ieeexplore.ieee.org/abstract/document/4144758/ An rf-based system for tracking transceiver-free objects],¡±. Proceedings of the Pervasive Computing and Communications. White Plains, USA, 2007: 135¨C144.</ref> when wireless signals are transmitted indoors, obstacles such as walls, the ground, and the human body cause various effects such as reflection, scattering, diffraction, and diffraction. Therefore, receiving end receives multiple signals from different paths at the same time, because surfaces reflect the signal during the transmission, which is known as [[Multipath propagation|multipath effect]].

The static model is based on these two kinds of signals: the direct signal and the reflected signal. Because there is no obstacle in the direct path, direct signal transmission can be modeled by [[Friis transmission equation]]:
: <math>P_r=\frac{P_t G_t G_r\lambda^2}{(4\pi)^2 d^2}</math>
: <math>P_t</math> is the power fed into the transmitting antenna input terminals;
: <math>P_r</math> is the power available at receiving antenna output terminals;
: <math>d</math> is the distance between antennas;
: <math>G_t</math> is transmitting antenna gain;
: <math>G_r</math> is receiving antenna gain;
: <math>\lambda</math> is the wavelength of the radio frequency

If we consider the reflected signal, the new equation is:
: <math>P_r=\frac{P_t G_t G_r\lambda^2}{(4\pi)^2(d+4h)^2}</math>
: <math>h</math> is the distance between reflection points and direct path.

When human shows up, we have a new transmission path. Therefore, the final equation is:
: <math>P_r=\frac{P_t G_t G_r\lambda^2}{(4\pi)^2(d+4h+\Delta)^2}</math>
<math>\Delta</math> is the approximate difference of the path caused by human body.

===== Dynamic transmission model =====
In this model, we consider the human motion, which causes the signal transmission path to change continuously. We can use Doppler Shift to describe this effect, which is related to the motion speed.
: <math>\Delta f=\frac{2v\cos\theta}{c}f</math>
By calculating the Doppler Shift of the receiving signal, we can figure out the pattern of the movement, thereby further identifying human activity. For example, in,<ref>Q. Pu, S. Gupta, S. Gollakota, and S. Patel, “[https://dl.acm.org/doi/pdf/10.1145/2486001.2491687 Whole-home gesture recognition using wireless signals],”. Proceedings of the 19th Annual
International Conference on Mobile Computing and Networking, New York, USA, 2013: 27–38.</ref> the Doppler shift is used as a fingerprint to achieve high-precision identification for nine different movement patterns.

===== [[Fresnel zone]] =====

The Fresnel zone was initially used to study the interference and
diffraction of the light, which is later used to construct the wireless signal transmission model. Fresnel zone is a series of elliptical intervals whose foci are the positions of the sender and receiver.

When a person is moving across different Fresnel zones, the signal path formed by the reflection of the human body changes, and if people move vertically through Fresnel zones, the change of signal will be periodical. In the paper,<ref name ="paper33">D. Wu, D. Zhang, C. Xu, Y. Wang, and H. Wang.“[https://dl.acm.org/doi/abs/10.1145/2971648.2971658 Wider: Walking direction estimation using wireless signals],”.Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, New York, USA, 2016:351–362.</ref> and,<ref name="paper41">H. Wang, D. Zhang, J. Ma, Y. Wang, Y. Wang, D. Wu, T. Gu, and B. Xie, “[http://www-public.imtbs-tsp.eu/~zhang_da/pub/Daqing%202016%20UbiComp%20respiration.pdf Human respiration detection with commodity wifi devices: Do user location and body orientation matter?]”. Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, New York, USA, 2016:25–36.</ref> they have applied the Fresnel model to the activity recognition task and got a more accurate result.

===== Modeling of the human body =====

In some tasks, we should consider modeling the human body accurately to achieve better results. For example,<ref name="paper41"/> described the human body as concentric cylinders for breath detection. The outside of the cylinder denotes the rib cage when people inhale, and the inside denotes that when people exhale. So the difference between the radius of that two cylinders represents the moving distance during breathing.  
The change of the signal phases can be expressed in the following equation:
: <math>\theta=2\pi\frac{2\,\Delta d}{\lambda}</math>
: <math>\theta</math> is the change of the signal phases;
: <math>\lambda</math> is the wavelength of the radio frequency;
: <math>\Delta d</math> is moving distance of rib cage;

== Datasets ==
There are some popular datasets that are used for benchmarking activity recognition or action recognition algorithms.

* '''UCF-101:''' It consists of 101 human action classes, over 13k clips and 27 hours of video data. Action classes include applying makeup, playing dhol, cricket shot, shaving beard, etc.<ref>{{Cite web|date=2021|title=UCF101 – Action Recognition Data Set|url=https://www.crcv.ucf.edu/research/data-sets/ucf101/|url-status=live|archive-url=https://web.archive.org/web/20200123021629/https://www.crcv.ucf.edu/research/data-sets/ucf101/ |archive-date=2020-01-23 }}</ref> 
* '''HMDB51:''' This is a collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,849 video clips from 51 action categories (such as “jump”, “kiss” and “laugh”), with each category containing at least 101 clips.<ref>{{Cite web|title=Papers with Code – HMDB51 Dataset|url=https://paperswithcode.com/dataset/hmdb51|access-date=2021-08-23|website=paperswithcode.com|language=en}}</ref>
* '''Kinetics:''' This is a significantly larger dataset than the previous ones. It contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. This dataset was created by DeepMind.<ref>{{cite arXiv|last1=Kay|first1=Will|last2=Carreira|first2=Joao|last3=Simonyan|first3=Karen|last4=Zhang|first4=Brian|last5=Hillier|first5=Chloe|last6=Vijayanarasimhan|first6=Sudheendra|last7=Viola|first7=Fabio|last8=Green|first8=Tim|last9=Back|first9=Trevor|last10=Natsev|first10=Paul|last11=Suleyman|first11=Mustafa|date=2017-05-19|title=The Kinetics Human Action Video Dataset|class=cs.CV|eprint=1705.06950}}</ref>

==Applications==

By automatically monitoring human activities, home-based rehabilitation can be provided for people suffering from traumatic brain injuries. One can find applications ranging from security-related applications and logistics support to [[location-based service]]s.<ref>[[Martha E. Pollack|Pollack, M.E.]], and et al., L. E. B. 2003. "[http://www.math.ntua.gr/aarg/projects/Thalis_files/Tsamardinos_2.pdf Autominder: an intelligent cognitive orthotic system for people with memory impairment] {{Webarchive|url=https://web.archive.org/web/20170810011434/http://www.math.ntua.gr/aarg/projects/Thalis_files/Tsamardinos_2.pdf |date=2017-08-10 }}". ''Robotics and Autonomous Systems'' 44(3–4):273–282.</ref> Activity recognition systems have been developed for [[wildlife observation]]<ref>Gao, Lianli, et al. "[http://www.academia.edu/download/33116711/Gao_et_al_2012_EN2324_EcolInform.pdf A Web-based semantic tagging and activity recognition system for species' accelerometry data]{{dead link|date=July 2022|bot=medic}}{{cbignore|bot=medic}}." Ecological Informatics 13 (2013): 47–56.</ref> and [[energy conservation]] in buildings.<ref>Nguyen, Tuan Anh, and Marco Aiello. "[http://www.rug.nl/research/portal/files/2357318/2013EnergyBuildingsNguyen.pdf Energy intelligent buildings based on user activity: A survey]." Energy and buildings 56 (2013): 244–257.</ref>

==See also==
* [[AI effect]]
* [[Applications of artificial intelligence]]
* [[Conditional random field]]
* [[Gesture recognition]]
* [[Hidden Markov model]]
* [[Motion analysis]]
* [[Naive Bayes classifier]]
* [[Support vector machines]]
* [[Object co-segmentation]]
* [[Outline of artificial intelligence]]
* [[Video content analysis]]

==References==
{{reflist}}

{{DEFAULTSORT:Activity Recognition}}
[[Category:Human–computer interaction]]
[[Category:Applications of artificial intelligence]]
[[Category:Applied machine learning]]
[[Category:Motion in computer vision]]