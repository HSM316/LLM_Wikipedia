The '''Bradley–Terry model''' is a [[probability theory|probability model]] that can predict the outcome of a paired comparison. Given a pair of individuals {{mvar|i}} and {{mvar|j}} drawn from some [[Population (statistics)|population]], it estimates the probability that the [[pairwise comparison]] {{math|''i'' > ''j''}} turns out true, as

:<math>P(i > j) = \frac{p_i}{p_i + p_j}</math>

where {{mvar|p<sub>i</sub>}} is a positive [[real number|real-valued]] score assigned to individual {{mvar|i}}. The comparison {{math|''i'' > ''j''}} can be read as "{{mvar|i}} is preferred to {{mvar|j}}", "{{mvar|i}} ranks higher than {{mvar|j}}", or "{{mvar|i}} beats {{mvar|j}}", depending on the application.

For example, {{mvar|p<sub>i</sub>}} may represent the skill of a team in a sports tournament, estimated from the number of times {{mvar|i}} has won a match. <math>P(i>j)</math> then represents the probability that {{mvar|i}} will win a match against {{mvar|j}}.<ref name="hunter" /><ref name="agresti" /> Another example used to explain the model's purpose is that of scoring products in a certain category by quality. While it's hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking.<ref name="agresti" />

== History and applications ==
The model is named after R. A. Bradley and M. E. Terry,<ref>{{cite encyclopedia |author=E.E.M. van Berkum |title=Bradley-Terry model |encyclopedia=Encyclopedia of Mathematics |url=http://www.encyclopediaofmath.org/index.php?title=Bradley-Terry_model&oldid=22181 |accessdate=18 November 2014}}</ref> who presented it in 1952,<ref>{{Cite journal | doi = 10.2307/2334029| jstor = 2334029| title = Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons| journal = Biometrika| volume = 39| issue = 3/4| pages = 324-345| year = 1952| last1 = Bradley | first1 = Ralph Allan | last2 = Terry | first2 = Milton E. }}</ref> although it had already been studied by [[Ernst Zermelo|Zermelo]] in the 1920s.<ref name="hunter">{{Cite journal| first = David R. | last = Hunter| title = MM algorithms for generalized Bradley–Terry models| journal = The Annals of Statistics| volume = 32 | issue = 1| year = 2004| pages = 384–406| jstor = 3448514| url = http://projecteuclid.org/euclid.aos/1079120141| citeseerx = 10.1.1.110.7878| doi=10.1214/aos/1079120141}}</ref><ref>{{cite journal |last=Zermelo |first=Ernst |title=Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeitsrechnung |journal=[[Mathematische Zeitschrift]] |volume=29 |number=1 |year=1929 |pages=436–460|doi=10.1007/BF01180541}}</ref><ref>{{citation |title=Ernst Zermelo: An Approach to His Life and Work |author=Heinz-Dieter Ebbinghaus |year=2007 |isbn=9783540495536 |pages=268–269}}</ref>

Real-world applications of the model include estimation of the influence of [[Statistics|statistical]] [[scientific journal|journals]], or ranking documents by relevance in [[Learning to rank|machine-learned]] [[search engine]]s.<ref>{{cite conference |last1=Szummer |first1=Martin |first2=Emine |last2=Yilmaz |title=Semi-supervised learning to rank with preference regularization |conference=CIKM |year=2011 |url=http://research.microsoft.com/pubs/154323/SzummerYilmaz-semisupervised-ranking-cikm11.pdf}}</ref>
In the latter application, <math>P(i > j)</math> may reflect that document {{mvar|i}} is more relevant to the user's [[Web search query|query]] than document {{mvar|j}}, so it should be displayed earlier in the results list. The individual {{mvar|p<sub>i</sub>}} then express the relevance of the document, and can be estimated from the frequency with which users click particular "hits" when presented with a result list.<ref>{{cite conference |first1=Filip |last1=Radlinski |first2=Thorsten |last2=Joachims |title=Active Exploration for Learning Rankings from Clickthrough Data |conference=KDD '07 Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining |year=2007 |url=http://www.cs.cornell.edu/People/tJ/publications/radlinski_joachims_07a.pdf|doi=10.1145/1281192.1281254|pages=570–579 }}</ref>

== Definition ==
The Bradley–Terry model can be parametrized in various ways. One way to do so is to pick a single parameter per observation, leading to a model of {{mvar|n}} parameters {{math|''p''<sub>1</sub>, ..., ''p<sub>n</sub>''}}.<ref name="wu">{{cite conference |title=Ranking Optimization with Constraints |conference=CIKM '14 Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management |author1=Fangzhao Wu |author2=Jun Xu |author3=Hang Li |author4=Xin Jiang |year=2014|doi=10.1145/2661829.2661895|pages=1049–1058 }}</ref>
Another variant, in fact the version considered by Bradley and Terry,<ref name="agresti">{{cite book |last=Agresti |first=Alan |title=Categorical Data Analysis |publisher=John Wiley & Sons |year=2014 |pages=436–439}}</ref> uses exponential score functions <math>p_i = e^{\beta_i}</math> so that

:<math>P(i > j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}</math>

or, using the [[logit]] (and disallowing ties),<ref name="hunter" />

:<math>\operatorname{logit}(P(i > j)) = \log\left(\frac{P(i > j)}{1 - P(i > j)}\right) = \log\left(\frac{P(i > j)}{P(j > i)}\right) = \beta_i - \beta_j</math>

reducing the model to [[logistic regression]] on pairs of individuals.

=== Estimating the parameters ===
The following [[algorithm]] computes the parameters {{mvar|p<sub>i</sub>}} of the basic version of the model from a sample of observations. Formally, it computes a [[maximum likelihood estimation|maximum likelihood estimate]], i.e., it maximizes the [[likelihood]] of the observed data. The algorithm dates back to the work of Zermelo.<ref name="hunter" />

The observations required are the outcomes of previous comparisons, for example, pairs {{math|(''i'', ''j'')}} where {{mvar|i}} beats {{mvar|j}}. Summarizing these outcomes as {{mvar|w<sub>ij</sub>}}, the number of times {{mvar|i}} has beaten {{mvar|j}}, we obtain the [[log-likelihood]] of the parameter vector {{math|'''p''' {{=}} ''p''<sub>1</sub>, ..., ''p<sub>n</sub>''}} as<ref name="hunter" />

:<math>L(\mathbf{p}) = \sum_i^n \sum_j^n [w_{ij} \ln p_i - w_{ij} \ln(p_i + p_j)].</math>

Denote the number of comparisons "won" by {{mvar|i}} as {{mvar|W<sub>i</sub>}}. Starting from an arbitrary vector {{math|'''p'''}}, the algorithm iteratively performs the update

:<math>p'_i = W_i \left( \sum_{j \ne i} \frac{w_{ij} + w_{ji}}{p_i + p_j} \right)^{-1}</math>

for all {{mvar|i}}. After computing all of the new parameters, they should be renormalized,

:<math>p_i \leftarrow \frac{p'_i}{\sum_{j=1}^n p'_j}.</math>

This estimation procedure improves the log-likelihood in every iteration, and eventually converges to a unique maximum.

==== Worked Example of Iterated Procedure ====
Suppose there are 4 teams who have played a total of 21 games among themselves. Each team's wins are given in the rows of the table below and the opponents are given as the columns. For example, Team A has beat Team B two times and lost to team B three times; not played team C at all; won once and lost four times against team D. 
{| class="wikitable"
|+Results
!
!A
!B
!C
!D
|-
|'''A'''
| -
|2
|0
|1
|-
|'''B'''
|3
| -
|5
|0
|-
|'''C'''
|0
|3
| -
|1
|-
|'''D'''
|4
|0
|3
| -
|}
We would like to compute the relative strengths of the teams; that is, we want to compute one parameter per team, with higher parameters indicating greater prowess. We initialize the 4 entries in the parameter vector {{math|'''p'''}} arbitrarily, for example, assigning the value 1 to each team: {{math|[1, 1, 1, 1]}}.

<math>p_1 = \frac{W_1}{\sum_{j \ne 1}\frac{w_{1j}+w_{j1}}{p_1+p_j}} = 
\frac{2 + 0 + 1}{\frac{2+3}{1+1}+\frac{0+0}{1+1} + \frac{1 + 4}{1+1}} = 0.6</math>

<math>p_2 = \frac{W_2}{\sum_{j \ne 2}\frac{w_{2j}+w_{j2}}{p_2+p_j}} = 
\frac{3 + 5 + 0}{\frac{3+2}{1+1}+\frac{5+3}{1+1} + \frac{0 + 0}{1+1}} = 1.231</math>

<math>p_3 = \frac{W_3}{\sum_{j \ne 3}\frac{w_{3j}+w_{j3}}{p_3+p_j}} = 
\frac{0 + 3 + 1}{\frac{0+0}{1+1}+\frac{3+5}{1+1} + \frac{1 + 3}{1+1}} = 0.667</math>

<math>p_4 = \frac{W_4}{\sum_{j \ne 4}\frac{w_{4j}+w_{j4}}{p_4+p_j}} = 
\frac{4 + 0 + 3}{\frac{4+1}{1+1}+\frac{0+0}{1+1} + \frac{3 + 1}{1+1}} = 1.556</math>

Then, we normalize all the parameters by dividing them by <math>0.6 + 1.231 + 0.667 + 1.556 = 4.053</math> to get the estimated parameters {{math|'''p''' {{=}} [0.148, 0.304, 0.164, 0.384]}}.

To get better estimates, we can repeat the process, using the new {{math|'''p'''}} values. For example,

<math>p_1 = \frac{W_1}{\sum_{j \ne 1}\frac{w_{1j}+w_{j1}}{p_1+p_j}} = 
\frac{2 + 0 + 1}{\frac{2+3}{0.148+0.304}+\frac{0+0}{0.148+0.164} + \frac{1 + 4}{0.148+0.384}} = 0.147</math>

Repeating this for the remaining parameters and normalizing, we get {{math|'''p''' {{=}} [0.145, 0.280, 0.162, 0.413]}}. Repeating this process a total of 20 times will show rapid convergence to {{math|'''p''' {{=}} [0.139, 0.226, 0.143, 0.492]}}. This shows that Team D is the strongest, Team B is the second strongest, and Team A and Team C are nearly equal in strength, but less than Teams B and D. The Bradley-Terry model lets us extrapolate the relationship between all 4 teams, even though all teams haven't played each other.

== See also ==
*[[Ordinal regression]]
*[[Rasch model]]
*[[Scale (social sciences)]]
*[[Elo rating system]]
*[[Thurstonian model]]

== References ==
{{reflist|30em}}

{{DEFAULTSORT:Bradley-Terry model}}
[[Category:Machine learning]]
[[Category:Statistical models]]
[[Category:Logistic regression]]
[[Category:Regression models]]