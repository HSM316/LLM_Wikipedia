The '''Bradley–Terry model''' is a [[probability theory|probability model]] that can predict the outcome of a paired comparison. Given a pair of individuals {{mvar|i}} and {{mvar|j}} drawn from some [[Population (statistics)|population]], it estimates the probability that the [[pairwise comparison]] {{math|''i'' > ''j''}} turns out true, as

:<math>P(i > j) = \frac{p_i}{p_i + p_j}</math>

where {{mvar|p<sub>i</sub>}} is a positive [[real number|real-valued]] score assigned to individual {{mvar|i}}. The comparison {{math|''i'' > ''j''}} can be read as "{{mvar|i}} is preferred to {{mvar|j}}", "{{mvar|i}} ranks higher than {{mvar|j}}", or "{{mvar|i}} beats {{mvar|j}}", depending on the application.

For example, {{mvar|p<sub>i</sub>}} may represent the skill of a team in a sports tournament, estimated from the number of times {{mvar|i}} has won a match. <math>P(i>j)</math> then represents the probability that {{mvar|i}} will win a match against {{mvar|j}}.<ref name="hunter" /><ref name="agresti" /> Another example used to explain the model's purpose is that of scoring products in a certain category by quality. While it's hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking.<ref name="agresti" />

== History and applications ==
The model is named after R. A. Bradley and M. E. Terry,<ref>{{cite encyclopedia |author=E.E.M. van Berkum |title=Bradley-Terry model |encyclopedia=Encyclopedia of Mathematics |url=http://www.encyclopediaofmath.org/index.php?title=Bradley-Terry_model&oldid=22181 |accessdate=18 November 2014}}</ref> who presented it in 1952,<ref>{{Cite journal | doi = 10.2307/2334029| jstor = 2334029| title = Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons| journal = Biometrika| volume = 39| issue = 3/4| pages = 324-345| year = 1952| last1 = Bradley | first1 = Ralph Allan | last2 = Terry | first2 = Milton E. }}</ref> although it had already been studied by [[Ernst Zermelo|Zermelo]] in the 1920s.<ref name="hunter">{{Cite journal| first = David R. | last = Hunter| title = MM algorithms for generalized Bradley–Terry models| journal = The Annals of Statistics| volume = 32 | issue = 1| year = 2004| pages = 384–406| jstor = 3448514| url = http://projecteuclid.org/euclid.aos/1079120141| citeseerx = 10.1.1.110.7878| doi=10.1214/aos/1079120141}}</ref><ref>{{cite journal |last=Zermelo |first=Ernst |title=Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeitsrechnung |journal=[[Mathematische Zeitschrift]] |volume=29 |number=1 |year=1929 |pages=436–460|doi=10.1007/BF01180541}}</ref><ref>{{citation |title=Ernst Zermelo: An Approach to His Life and Work |author=Heinz-Dieter Ebbinghaus |year=2007 |isbn=9783540495536 |pages=268–269}}</ref>

Real-world applications of the model include estimation of the influence of [[Statistics|statistical]] [[scientific journal|journals]], or ranking documents by relevance in [[Learning to rank|machine-learned]] [[search engine]]s.<ref>{{cite conference |last1=Szummer |first1=Martin |first2=Emine |last2=Yilmaz |title=Semi-supervised learning to rank with preference regularization |conference=CIKM |year=2011 |url=http://research.microsoft.com/pubs/154323/SzummerYilmaz-semisupervised-ranking-cikm11.pdf}}</ref>
In the latter application, <math>P(i > j)</math> may reflect that document {{mvar|i}} is more relevant to the user's [[Web search query|query]] than document {{mvar|j}}, so it should be displayed earlier in the results list. The individual {{mvar|p<sub>i</sub>}} then express the relevance of the document, and can be estimated from the frequency with which users click particular "hits" when presented with a result list.<ref>{{cite conference |first1=Filip |last1=Radlinski |first2=Thorsten |last2=Joachims |title=Active Exploration for Learning Rankings from Clickthrough Data |conference=KDD '07 Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining |year=2007 |url=http://www.cs.cornell.edu/People/tJ/publications/radlinski_joachims_07a.pdf|doi=10.1145/1281192.1281254|pages=570–579 }}</ref>

== Definition ==
The Bradley–Terry model can be parametrized in various ways. One way to do so is to pick a single parameter per observation, leading to a model of {{mvar|n}} parameters {{math|''p''<sub>1</sub>, ..., ''p<sub>n</sub>''}}.<ref name="wu">{{cite conference |title=Ranking Optimization with Constraints |conference=CIKM '14 Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management |author1=Fangzhao Wu |author2=Jun Xu |author3=Hang Li |author4=Xin Jiang |year=2014|doi=10.1145/2661829.2661895|pages=1049–1058 }}</ref>
Another variant, in fact the version considered by Bradley and Terry,<ref name="agresti">{{cite book |last=Agresti |first=Alan |title=Categorical Data Analysis |publisher=John Wiley & Sons |year=2014 |pages=436–439}}</ref> uses exponential score functions <math>p_i = e^{\beta_i}</math> so that

:<math>P(i > j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}</math>

or, using the [[logit]] (and disallowing ties),<ref name="hunter" />

:<math>\operatorname{logit}(P(i > j)) = \log\left(\frac{P(i > j)}{1 - P(i > j)}\right) = \log\left(\frac{P(i > j)}{P(j > i)}\right) = \beta_i - \beta_j</math>

reducing the model to [[logistic regression]] on pairs of individuals.

=== Estimating the parameters ===
The following [[algorithm]] computes the parameters {{mvar|p<sub>i</sub>}} of the basic version of the model from a sample of observations. Formally, it computes a [[maximum likelihood estimation|maximum likelihood estimate]], i.e., it maximizes the [[likelihood]] of the observed data. The algorithm dates back to the work of Zermelo.<ref name="hunter" />

The observations required are the outcomes of previous comparisons, for example, pairs {{math|(''i'', ''j'')}} where {{mvar|i}} beats {{mvar|j}}. Summarizing these outcomes as {{mvar|w<sub>ij</sub>}}, the number of times {{mvar|i}} has beaten {{mvar|j}}, we obtain the [[log-likelihood]] of the parameter vector {{math|'''p''' {{=}} ''p''<sub>1</sub>, ..., ''p<sub>n</sub>''}} as<ref name="hunter" />

:<math>L(\mathbf{p}) = \sum_i^n \sum_j^n w_{ij} \ln p_i - w_{ij} \ln(p_i + p_j).</math>

Denote the number of comparisons "won" by {{mvar|i}} as {{mvar|W<sub>i</sub>}}. Starting from an arbitrary vector {{math|'''p'''}}, the algorithm iteratively performs the update

:<math>p'_i = W_i \left( \sum_{j \ne i} \frac{w_{ij} + w_{ji}}{p_i + p_j} \right)^{-1}</math>

for all {{mvar|i}}. After computing all of the new parameters, they should be renormalized,

:<math>p_i \leftarrow \frac{p'_i}{\sum_{j=1}^n p'_j}.</math>

This estimation procedure improves the log-likelihood in every iteration, and eventually converges to a unique maximum.

== See also ==
*[[Ordinal regression]]
*[[Rasch model]]
*[[Scale (social sciences)]]
*[[Elo rating system]]
*[[Thurstonian model]]

== References ==
{{reflist|30em}}

{{DEFAULTSORT:Bradley-Terry model}}
[[Category:Machine learning]]
[[Category:Statistical models]]
[[Category:Logistic regression]]
[[Category:Regression models]]