{{Short description|A paradigm in machine learning}}
'''Weak supervision''' is a paradigm in [[machine learning]], the relevance and notability of which increased with the advent of [[large language model]]s due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-[[labeled data]] (exclusively used in more expensive and time-consuming [[supervised learning]] paradigm), followed by a large amount of unlabeled data (used exclusively in [[unsupervised learning]] paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing [[Cluster analysis|clustering]] and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.

== Problem ==
[[File:Task-guidance.png|thumb|left|300px|Tendency for a task to employ supervised vs. unsupervised methods. Task names straddling circle boundaries is intentional. It shows that the classical division of imaginative tasks (left) employing unsupervised methods is blurred in today's learning schemes.]]
{{Machine learning|Paradigms}}
The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.

== Technique ==
{{See also| Active learning (machine learning)}}
[[File:Example of unlabeled data in semisupervised learning.png|thumb|left|194px|An example of the influence of unlabeled data in semi-supervised learning. The top panel shows a decision boundary we might adopt after seeing only one positive (white circle) and one negative (black circle) example. The bottom panel shows a decision boundary we might adopt if, in addition to the two labeled examples, we were given a collection of unlabeled data (gray circles).]]

More formally, semi-supervised learning assumes a set of <math>l</math> [[Independent identically distributed|independently identically distributed]] examples <math>x_1,\dots,x_l \in X</math> with corresponding labels <math>y_1,\dots,y_l \in Y</math> and <math>u</math> unlabeled examples <math>x_{l+1},\dots,x_{l+u} \in X</math> are processed. Semi-supervised learning combines this information to surpass the [[Statistical classification|classification]] performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.

Semi-supervised learning may refer to either [[Transduction (machine learning)|transductive learning]] or [[Inductive reasoning|inductive learning]].<ref>{{Citation |title=Semi-Supervised Learning Literature Survey, Page 5|citeseerx = 10.1.1.99.9681|year = 2007}}</ref> The goal of transductive learning is to infer the correct labels for the given unlabeled data <math>x_{l+1},\dots,x_{l+u}</math> only. The goal of inductive learning is to infer the correct mapping from <math>X</math> to <math>Y</math>.

It is unnecessary (and, according to [[Vapnik's principle]], imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.

== Assumptions ==
In order to make any use of unlabeled data, some relationship to the underlying distribution of data must exist. Semi-supervised learning algorithms make use of at least one of the following assumptions:{{sfn|Chapelle|Schölkopf|Zien|2006}}

=== Continuity / smoothness assumption ===
''Points that are close to each other are more likely to share a label.'' This is also generally assumed in supervised learning and yields a preference for geometrically simple [[decision boundary|decision boundaries]]. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes.<ref>Chawla, N., Bowyer, K., Hall, L.O., & Kegelmeyer, W.P. (2002). SMOTE: Synthetic Minority Over-sampling Technique. ArXiv, abs/1106.1813.</ref>

=== Cluster assumption ===
''The data tend to form discrete clusters, and points in the same cluster are more likely to share a label'' (although data that shares a label may spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to [[feature learning]] with clustering algorithms.

=== Manifold assumption ===
{{Main|Manifold hypothesis}}
''The data lie approximately on a [[manifold]] of much lower dimension than the input space.'' In this case learning the manifold using both the labeled and unlabeled data can avoid the [[curse of dimensionality]]. Then learning can proceed using distances and densities defined on the manifold.

The manifold assumption is practical when high-dimensional data are generated by some process that may be hard to model directly, but which has only a few degrees of freedom. For instance, human voice is controlled by a few vocal folds,<ref name = "StevensKN">{{Cite book|title=Acoustic phonetics|last=Stevens |first=Kenneth N.|date=1998|publisher=MIT Press|isbn=0-585-08720-2|location=Cambridge, Mass.|oclc=42856189}}</ref> and images of various facial expressions are controlled by a few muscles. In these cases, it is better to consider distances and smoothness in the natural space of the generating problem, rather than in the space of all possible acoustic waves or images, respectively.

== History ==
The heuristic approach of ''self-training'' (also known as ''self-learning'' or ''self-labeling'') is historically the oldest approach to semi-supervised learning,{{sfn|Chapelle|Schölkopf|Zien|2006}} with examples of applications starting in the 1960s.<ref>{{Cite journal|last=Scudder|first=H.|date=July 1965|title=Probability of error of some adaptive pattern-recognition machines|journal=IEEE Transactions on Information Theory|volume=11|issue=3|pages=363–371|doi=10.1109/TIT.1965.1053799|issn=1557-9654}}</ref>

The transductive learning framework was formally introduced by [[Vladimir Vapnik]] in the 1970s.<ref>{{cite book|last1=Vapnik |first1=V. |last2= Chervonenkis |first2=A.|title= Theory of Pattern Recognition |language= ru |publisher= Nauka |location=Moscow |year=1974}} cited in {{harvnb|Chapelle|Schölkopf|Zien|2006|p= 3}}</ref> Interest in inductive learning using generative models also began in the 1970s. A [[Probably approximately correct learning|''probably approximately correct'' learning]] bound for semi-supervised learning of a [[Gaussian]] mixture was demonstrated by Ratsaby and Venkatesh in 1995.<ref name="Ratsaby">{{cite web|last1=Ratsaby |first1=J.|last2= Venkatesh|first2= S. |url=http://www.ariel.ac.il/sites/ratsaby/Publications/PDF/colt95.pdf |title=Learning from a mixture of labeled and unlabeled examples with parametric side information}} in {{Cite book|title=Proceedings of the eighth annual conference on Computational learning theory - COLT '95|date=1995|publisher=ACM Press|isbn=0-89791-723-5|location=New York, New York, USA|pages= 412–417 |doi=10.1145/225298.225348|s2cid=17561403}}. Cited in {{harvnb|Chapelle|Schölkopf|Zien|2006|p=4}}</ref>

== Methods ==

=== Generative models ===
Generative approaches to statistical learning first seek to estimate <math>p(x|y)</math>,{{disputed inline|date=November 2017}} the distribution of data points belonging to each class. The probability <math>p(y|x)</math> that a given point <math>x</math> has label <math>y</math> is then proportional to <math>p(x|y)p(y)</math> by [[Bayes' theorem|Bayes' rule]]. Semi-supervised learning with [[generative model]]s can be viewed either as an extension of supervised learning (classification plus information about <math>p(x)</math>) or as an extension of unsupervised learning (clustering plus some labels).

Generative models assume that the distributions take some particular form <math>p(x|y,\theta)</math> parameterized by the vector <math>\theta</math>. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone.<ref>{{Citation|last1=Fabio|first1=Cozman|title=Risks of Semi-Supervised Learning: How Unlabeled Data Can Degrade Performance of Generative Classifiers|date=2006-09-22|work=Semi-Supervised Learning|pages=56–72|publisher=The MIT Press|isbn=978-0-262-03358-9|last2=Ira|first2=Cohen|doi=10.7551/mitpress/9780262033589.003.0004}} In: {{harvnb|Chapelle|Schölkopf|Zien|2006}}</ref> 
However, if the assumptions are correct, then the unlabeled data necessarily improves performance.<ref name = "Ratsaby"/>

The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.

The parameterized [[joint distribution]] can be written as <math>p(x,y|\theta)=p(y|\theta)p(x|y,\theta)</math> by using the [[Chain rule (probability)|chain rule]]. Each parameter vector <math>\theta</math> is associated with a decision function <math>f_\theta(x) = \underset{y}{\operatorname{argmax}}\ p(y|x,\theta)</math>. 
The parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by <math>\lambda</math>:

:<math>\underset{\Theta}{\operatorname{argmax}}\left( \log p(\{x_i,y_i\}_{i=1}^l | \theta) + \lambda \log p(\{x_i\}_{i=l+1}^{l+u}|\theta)\right) </math><ref name="SSL_EoML">Zhu, Xiaojin. [http://pages.cs.wisc.edu/~jerryzhu/pub/SSL_EoML.pdf Semi-Supervised Learning] University of Wisconsin-Madison.</ref>

=== Low-density separation ===
Another major class of methods attempts to place boundaries in regions with few data points (labeled or unlabeled). One of the most commonly used algorithms is the [[Support vector machine#Transductive support vector machines|transductive support vector machine]], or TSVM (which, despite its name, may be used for inductive learning as well). Whereas [[support vector machines]] for supervised learning seek a decision boundary with maximal [[Margin (machine learning)|margin]] over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard [[hinge loss]] <math>(1-yf(x))_+</math> for labeled data, a loss function <math>(1-|f(x)|)_+</math> is introduced over the unlabeled data by letting <math>y=\operatorname{sign}{f(x)}</math>. TSVM then selects <math>f^*(x) = h^*(x) + b</math> from a [[reproducing kernel Hilbert space]] <math>\mathcal{H}</math> by minimizing the [[Regularization (mathematics)|regularized]] [[Empirical risk minimization|empirical risk]]:

:<math>f^* = \underset{f}{\operatorname{argmin}}\left( 
\displaystyle \sum_{i=1}^l(1-y_if(x_i))_+ + \lambda_1 \|h\|_\mathcal{H}^2 + \lambda_2 \sum_{i=l+1}^{l+u} (1-|f(x_i)|)_+
\right) </math>

An exact solution is intractable due to the non-[[convex function|convex]] term <math>(1-|f(x)|)_+</math>, so research focuses on useful approximations.<ref name="SSL_EoML"/>

Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).

=== Laplacian regularization ===

Laplacian regularization has been historically approached through graph-Laplacian.
Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its <math>k</math> nearest neighbors or to examples within some distance <math>\epsilon</math>. The weight <math>W_{ij}</math> of an edge between <math>x_i</math> and <math>x_j</math> is then set to <math>e^{-\|x_i-x_j\|^2 / \epsilon^2}</math>.

Within the framework of [[manifold regularization]],<ref>{{cite journal|author1=M. Belkin |author2=P. Niyogi |title=Semi-supervised Learning on Riemannian Manifolds|journal=Machine Learning|volume=56|issue=Special Issue on Clustering|pages=209–239|year=2004|url=http://booksc.org/dl/11288633/421f61|doi=10.1023/b:mach.0000033120.25363.1e|doi-access=free}}</ref><ref>M. Belkin, P. Niyogi, V. Sindhwani. On Manifold Regularization. AISTATS 2005.</ref> the graph serves as a proxy for the manifold. A term is added to the standard [[Tikhonov regularization]] problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes

:<math>\underset{f\in\mathcal{H}}{\operatorname{argmin}}\left(
\frac{1}{l}\displaystyle\sum_{i=1}^l V(f(x_i),y_i) + 
\lambda_A \|f\|^2_\mathcal{H} + 
\lambda_I \int_\mathcal{M}\|\nabla_\mathcal{M} f(x)\|^2dp(x)
\right) </math><ref name="SSL_EoML"/>

where <math>\mathcal{H}</math> is a reproducing kernel [[Hilbert space]] and <math>\mathcal{M}</math> is the manifold on which the data lie. The regularization parameters <math>\lambda_A</math> and <math>\lambda_I</math> control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the [[Laplacian matrix|graph Laplacian]] <math>L = D - W</math> where <math>D_{ii} = \sum_{j=1}^{l+u} W_{ij}</math> and <math>\mathbf{f}</math> is the vector <math>[f(x_1)\dots f(x_{l+u})]</math>, we have

:<math>\mathbf{f}^T L \mathbf{f} = \displaystyle\sum_{i,j=1}^{l+u}W_{ij}(f_i-f_j)^2 \approx \int_\mathcal{M}\|\nabla_\mathcal{M} f(x)\|^2dp(x)</math>.

The graph-based approach to Laplacian regularization is to put in relation with [[finite difference method]].{{Clarify|date=July 2022}}{{Cn|date=July 2022}}

The Laplacian can also be used to extend the supervised learning algorithms: [[regularized least squares]] and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.

=== Heuristic approaches ===
Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples <math>x_1,\dots,x_{l+u}</math> may inform a choice of representation, [[distance metric]], or [[Kernel (statistics)|kernel]] for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples. In this vein, some methods learn a low-dimensional representation using the supervised data and then apply either low-density separation or graph-based methods to the learned representation.<ref>{{cite book |last1=Iscen |first1=Ahmet |last2=Tolias |first2=Giorgos |last3=Avrithis |first3=Yannis |last4=Chum |first4=Ondrej |title=2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) |chapter=Label Propagation for Deep Semi-Supervised Learning |date=2019 |pages=5065–5074 |doi=10.1109/CVPR.2019.00521 |chapter-url=https://ieeexplore.ieee.org/document/8954421 |access-date=26 March 2021|arxiv=1904.04717 |isbn=978-1-7281-3293-8 |s2cid=104291869 }}</ref><ref>{{cite journal |last1=Burkhart |first1=Michael C. |last2=Shan |first2=Kyle |title=Deep Low-Density Separation for Semi-supervised Classification |journal=International Conference on Computational Science (ICCS) |series=Lecture Notes in Computer Science |date=2020 |volume=12139 |pages=297–311 |doi=10.1007/978-3-030-50420-5_22 |isbn=978-3-030-50419-9 |doi-access=free }}</ref> Iteratively refining the representation and then performing semi-supervised learning on said representation may further improve performance.

''Self-training'' is a wrapper method for semi-supervised learning.<ref>{{Cite journal|title = Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study|journal = Knowledge and Information Systems|date = 2013-11-26|issn = 0219-1377|pages = 245–284|volume = 42|issue = 2|doi = 10.1007/s10115-013-0706-y|language = en|first1 = Isaac|last1 = Triguero|first2 = Salvador|last2 = García|first3 = Francisco|last3 = Herrera|s2cid = 1955810}}</ref> First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident in are added at each step.<ref>{{Cite journal|title = Self-Trained LMT for Semisupervised Learning|journal = Computational Intelligence and Neuroscience|date = 2015-12-29|pages = 3057481|volume = 2016|doi = 10.1155/2016/3057481|pmid = 26839531|pmc = 4709606|language = en|first1 = Nikos|last1 = Fazakis|first2 = Stamatis|last2 = Karlos|first3 = Sotiris|last3 = Kotsiantis|first4 = Kyriakos|last4 = Sgarbas|doi-access = free}}</ref> In natural language processing, a common self-training algorithm is the [[Yarowsky algorithm]] for problems like word sense disambiguation, accent restoration, and spelling correction.<ref>{{cite journal| last1      = Yarowsky| first1     = David| date       = 1995| title      = Unsupervised Word Sense Disambiguation Rivaling Supervised Methods| url        = https://aclanthology.org/P95-1026/| journal    = Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics| pages      = 189–196| doi        = 10.3115/981658.981684| location              = Cambridge, MA| publisher             = Association for Computational Linguistics| access-date = 1 November 2022| doi-access= free}}</ref>

[[Co-training]] is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.<ref>{{Cite book|title = Analysis of Co-training Algorithm with Very Small Training Sets|publisher = Springer Berlin Heidelberg|date = 2012-11-07|isbn = 9783642341656|pages = 719–726|series = Lecture Notes in Computer Science|language = en|first1 = Luca|last1 = Didaci|first2 = Giorgio|last2 = Fumera|first3 = Fabio|last3 = Roli|editor-first = Georgy|editor-last = Gimel’farb|editor-first2 = Edwin|editor-last2 = Hancock|editor-first3 = Atsushi|editor-last3 = Imiya|editor-first4 = Arjan|editor-last4 = Kuijper|editor-first5 = Mineichi|editor-last5 = Kudo|editor-first6 = Shinichiro|editor-last6 = Omachi|editor-first7 = Terry|editor-last7 = Windeatt|editor-first8 = Keiji|editor-last8 = Yamada|doi = 10.1007/978-3-642-34166-3_79| s2cid=46063225 }}</ref>

== In human cognition ==
Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data.<ref name="ZhuGoldberg">
{{Cite book|title=Introduction to semi-supervised learning|last=Zhu|first=Xiaojin|date=2009|publisher=Morgan & Claypool Publishers|others=Goldberg, A. B. (Andrew B.)|isbn=978-1-59829-548-1|location=[San Rafael, Calif.]|oclc=428541480}}</ref> More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human [[concept learning]] involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).

Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces.<ref>{{cite journal |author1=Younger B. A. |author2=Fearing D. D. | year = 1999 | title = Parsing Items into Separate Categories: Developmental Change in Infant Categorization | journal = Child Development | volume = 70 | issue = 2| pages = 291–303 | doi=10.1111/1467-8624.00022}}</ref> Infants and children take into account not only unlabeled examples, but the [[sampling (statistics)|sampling]] process from which labeled examples arise.<ref>{{cite journal|author1=Xu, F.  |author2=Tenenbaum, J. B. |name-list-style=amp |year=2007|title=Sensitivity to sampling in Bayesian word learning|volume=10|issue=3 |pages=288–297|doi=10.1111/j.1467-7687.2007.00590.x|pmid=17444970 |journal=Developmental Science|citeseerx=10.1.1.141.7505 }}</ref><ref>{{cite journal|author=Gweon, H., Tenenbaum J.B., and Schulz L.E|year=2010|title=Infants consider both the sample and the sampling process in inductive generalization|journal=Proc Natl Acad Sci U S A|volume=107|issue=20|pages=9066–71|doi=10.1073/pnas.1003095107|pmid=20435914|pmc=2889113|bibcode=2010PNAS..107.9066G|doi-access=free}}</ref>

==See also==
*[[PU learning]]

==References==
<references responsive="0" />

== Sources ==
* {{Cite book  | last1 = Chapelle | first1 = Olivier | last2 = Schölkopf | first2 = Bernhard | last3 = Zien | first3 = Alexander | title = Semi-supervised learning | year = 2006 | publisher = MIT Press | location = Cambridge, Mass. | isbn = 978-0-262-03358-9 }}

==External links==
* [http://manifold.cs.uchicago.edu/manifold_regularization/software.html Manifold Regularization] A freely available [[MATLAB]] implementation of the graph-based semi-supervised algorithms Laplacian support vector machines and Laplacian regularized least squares.
* [http://sci2s.ugr.es/keel/algorithms.php#sub10 KEEL: A software tool to assess evolutionary algorithms for Data Mining problems (regression, classification, clustering, pattern mining and so on)] KEEL module for semi-supervised learning.
* [http://pages.cs.wisc.edu/~jerryzhu/ssl/software.html Semi-Supervised Learning Software]
* [https://scikit-learn.org/stable/modules/semi_supervised.html Semi-Supervised learning — scikit-learn documentation] Semi-supervised learning in [[scikit-learn]].

{{Differentiable computing}}

[[Category:Machine learning]]