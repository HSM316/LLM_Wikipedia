'''Weak supervision''' is a branch of [[machine learning]] where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of [[Training, validation, and test sets|training data]] in a [[supervised learning]] setting.<ref name=":0">{{cite web |url=http://ai.stanford.edu/blog/weak-supervision/ |title=Weak Supervision: A New Programming Paradigm for Machine Learning|last=Alex Ratner, Stephen Bach, Paroma Varma, Chris Ré And referencing work by many other members of Hazy Research |website=The Stanford AI Lab Blog |access-date=2022-03-22}}</ref> This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model.<ref name=":10">{{Cite journal|url=https://www.sciencedirect.com/science/article/pii/S0020025520309609|title=Ground truthing from multi-rater labeling with three-way decision and possibility theory|last1=Campagner|first1=Andrea|last2=Ciucci|first2=Davide|last3=Svensson|first3=Carl Magnus|last4=Figge|first4=Marc Thilo|last5=Cabitza|first5=Federico|journal=Information Sciences|year=2021|volume=545|pages=771–790|doi=10.1016/j.ins.2020.09.049|s2cid=225116425}}</ref><ref name=":2">{{Cite journal|url=http://pdfs.semanticscholar.org/3adc/fd254b271bcc2fb7e2a62d750db17e6c2c08.pdf|archive-url=https://web.archive.org/web/20190222174426/http://pdfs.semanticscholar.org/3adc/fd254b271bcc2fb7e2a62d750db17e6c2c08.pdf|url-status=dead|archive-date=22 February 2019|title=A Brief Introduction to Weakly Supervised Learning|last=Zhou|first=Zhi-Hua|journal=National Science Review|year=2018|volume=5|pages=44–53|doi=10.1093/NSR/NWX106|s2cid=44192968|access-date=4 June 2019|doi-access=free}}</ref><ref>{{cite book |last1=Nodet |first1=Pierre |last2=Lemaire |first2=Vincent |last3=Bondu |first3=Alexis |last4=Cornuéjols |first4=Antoine |last5=Ouorou |first5=Adam |title=2021 International Joint Conference on Neural Networks (IJCNN)  |chapter=From Weakly Supervised Learning to Biquality Learning: An Introduction |year=2021 |pages=1–10 |doi=10.1109/IJCNN52387.2021.9533353 |arxiv=2012.09632|isbn=978-1-6654-3900-8 |s2cid=237450775 }}</ref>

== Problem of labeled training data ==
Machine learning models and techniques are increasingly accessible to researchers and developers; the real-world usefulness of these models, however, depends on access to high-quality labeled training data.<ref>{{Cite web|url=http://www.spacemachine.net/views/2016/3/datasets-over-algorithms|title=Datasets Over Algorithms|website=Space Machine|access-date=2019-06-05}}</ref> This need for labeled training data often proves to be a significant obstacle to the application of machine learning models within an organization or industry.<ref name=":0" />{{Dead link|date=August 2021}} This bottleneck effect manifests itself in various ways, including the following examples:

'''Insufficient quantity of labeled data'''

When machine learning techniques are initially used in new applications or industries, there is often not enough training data available to apply traditional processes.<ref name=":1">{{Cite arXiv|title=A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective|last=Roh|first=Yuji|date=8 Nov 2018 |eprint = 1811.03402|class = cs.LG}}</ref> Some industries have the benefit of decades' worth of training data readily available; those that do not are at a significant disadvantage. In such cases, obtaining training data may be impractical, expensive, or impossible without waiting years for its accumulation.

'''Insufficient subject-matter expertise to label data'''

When labeling training data requires specific relevant expertise, creation of a usable training data set can quickly become prohibitively expensive.<ref name=":1" /> This issue is likely to occur, for example, in [[Biomedicine|biomedical]] or [[National security|security-related]] applications of machine learning.

'''Insufficient time to label and prepare data'''

Most of the time required to implement machine learning is spent in preparing data sets.<ref name=":1" /> When an industry or research field deals with problems that are, by nature, rapidly evolving, it can be impossible to collect and prepare data quickly enough for results to be useful in real-world applications. This issue could occur, for example, in [[fraud detection]] or [[Computer security|cybersecurity]] applications.

Other areas of machine learning exist that are likewise motivated by the demand for increased quantity and quality of labeled training data but employ different high-level techniques to approach this demand. These other approaches include [[Active learning (machine learning)|active learning]], [[semi-supervised learning]], and [[transfer learning]].<ref name=":0" />{{Dead link|date=August 2021}}

== Types of weak labels ==
Weak labels are intended to decrease the cost and increase the efficiency of human efforts expended in hand-labeling data. They can take many forms, and might be categorized into three types:

* '''Global statistics on groups of inputs:''' This setting consists in accessing global information on bags of samples — ''e.g.'' knowing that half of the labels of a given subset of samples. Examples of global statistics supervision include multiple-instance learning<ref>{{cite journal |last1=Dietterich |first1=Thomas G. |last2=Lathrop |first2=Richard H. |last3=Lozano-Pérez |first3=Tomás |title=Solving the multiple instance problem with axis-parallel rectangles |url=https://www.sciencedirect.com/science/article/pii/S0004370296000343 |journal=Artificial Intelligence |pages=31–71 |language=en |doi=10.1016/S0004-3702(96)00034-3 |date=1 January 1997|volume=89 |issue=1–2 }}</ref> and learning from label proportion.<ref>{{cite web |last1=Quadrianto |first1=Novi |last2=Smola |first2=Alex J. |last3=Caetano |first3=Tibério S. |last4=Le |first4=Quoc V. |title=Estimating Labels from Label Proportions |url=https://www.jmlr.org/papers/v10/quadrianto09a.html |website=Journal of Machine Learning Research |pages=2349–2374 |date=2009}}</ref>
* '''Weak classifiers:'''  A second approach consists in assuming the access to many weak classifiers that weakly correlate with the function to learn. Those classifiers might model labelers from a crowdsourcing platform, experts, noisy measurements or [[Heuristic (computer science)|heuristic rules]]. More generally, developers may take advantage of existing resources (such as knowledge bases, alternative data sets, or pre-trained models<ref name=":0" />) to create labels that are helpful, though not perfectly suited for the given task.<ref name="Data Programming 1605">{{Cite arXiv|last1=Ré|first1=Christopher|last2=Selsam|first2=Daniel|last3=Wu|first3=Sen|last4=De Sa|first4=Christopher|last5=Ratner|first5=Alexander|date=2016-05-25|title=Data Programming: Creating Large Training Sets, Quickly|eprint=1605.07723v3|class=stat.ML}}</ref>
* '''Incomplete annotation:''' Finally, weak supervision might be understood as the access to partial knowledge on each label. This partial knowledge can be thought of as a corruption process.<ref>{{cite web |last1=Rooyen |first1=Brendan van |last2=Williamson |first2=Robert C. |title=A Theory of Learning with Corrupted Labels |url=https://jmlr.org/papers/v18/16-315.html |website=Journal of Machine Learning Research |pages=1–50 |date=2018}}</ref> In some instances, partial observation can be cast as a set of potential labels that are compatible with this partial observation, which is the setting of partial supervision.<ref>{{Cite journal|url=https://www.sciencedirect.com/science/article/pii/S0888613X13001722|title=Learning from imprecise and fuzzy observations: Data disambiguation through generalized loss minimization|last=Hüllermeier|first=Eyke|journal=International Journal of Approximate Reasoning|year=2014|volume=55|issue=7|pages=1519–1534|doi=10.1016/j.ijar.2013.09.003|arxiv=1305.0698}}</ref><ref>{{cite web |last1=Cabannes |first1=Vivien |last2=Rudi |first2=Alessandro |last3=Bach |first3=Francis |title=Structured Prediction with Partial Labelling through the Infimum Loss |url=https://proceedings.mlr.press/v119/cabannnes20a.html |website=International Conference on Machine Learning |publisher=PMLR |pages=1230–1239 |language=en |date=21 November 2020}}</ref> Partial supervision is a generalization of [[semi-supervised learning]], which has been the classical approach to overcome the bottleneck of data annotation.

Beyond those three settings, limitations that motivates weakly supervised learning might be tackled by leveraging human knowledge under the form of priors<ref>{{cite web |last1=Mann |first1=Gideon S. |last2=McCallum |first2=Andrew |title=Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data |url=https://jmlr.org/beta/papers/v11/mann10a.html |website=Journal of Machine Learning Research |pages=955–984 |language=en |date=2010}}</ref> or of function architectures, reviving old approaches of artificial intelligence such as [[inductive logic programming]].

== Applications of weak supervision ==
Applications of weak supervision are numerous and varied within the machine learning research community.

In 2014, researchers from [[UC Berkeley]] made use of the principles of weak supervision to propose an iterative learning algorithm that solely depends on labels generated by heuristics and alleviates the need of collecting any ground-truth labels.<ref>{{Cite journal|last1=Jin|first1=Ming|last2=Jia|first2=Ruoxi|last3=Kang|first3=Zhaoyi|last4=Konstantakopoulos|first4=Ioannis|last5=Spanos|first5=Costas|date=2014|title=PresenceSense: zero-training algorithm for individual presence detection based on power monitoring|journal=Proceedings of the 1st ACM Conference on Embedded Systems for Energy-Efficient Buildings|pages=1–10|doi= 10.1145/2674061.2674073|s2cid=46950525}}</ref><ref>{{Cite journal|last1=Jin|first1=Ming|last2=Jia|first2=Ruoxi|last3=Spanos|first3=Costas|date=2017|title=Virtual occupancy sensing: using smart meters to indicate your presence|journal=IEEE Transactions on Mobile Computing|volume=16|number=11|pages=3264–3277|doi= 10.1109/TMC.2017.2684806|arxiv=1407.4395|s2cid=1997078}}</ref> The algorithm was applied to smart meter data to learn about the household's occupancy without ever asking for the occupancy data, which has raised issues of privacy and security as covered by an article in IEEE Spectrum.<ref>{{Cite web|url=https://spectrum.ieee.org/view-from-the-valley/energy/the-smarter-grid/what-does-your-smart-meter-know-about-you|title=What does smart meter know about you?|website=IEEE Spectrum}}</ref>

Researchers from the [[University of Southern California]] showed in 2017 that a very deep neural network can be trained to estimate a 3D face shape from a single image, using weakly supervised learning.<ref>{{Cite journal|last1=Tuan Tran|first1=Anh|last2=Hassner|first2=Tal|last3=Masi|first3=Iacopo|last4=Medioni|first4=Gerard|date=2017|title=Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network|journal=Computer Vision and Pattern Recognition (CVPR)|arxiv=1612.04904}}</ref> The researchers noted the challenge of obtaining large amounts of face photos, in the wild, with their accompanying 3D, ground-truth, face shapes. Instead of collecting this data, they proposed generating labels, automatically, for faces in an existing face dataset. In their work, they used a classical, pre-deep learning method to estimate (noisy) 3D face shapes for the face images. They then average pooled the multiple 3D estimates thus obtained for different photos of the same person. These pooled estimates, generated for photos of different people, were used as weak supervision when training a deep network to regress 3D face shapes from single input face images. Importantly, their analysis showed that the network's 3D predictions were more accurate than estimates produced by the method they used to generate their proxy training labels, demonstrating the effectiveness of this approach. The same team later successfully applied similar weakly supervised methods,<ref>{{Cite journal|last1=Chang|first1=FengJu|last2=Tuan Tran|first2=Anh|last3=Hassner|first3=Tal|last4=Masi|first4=Iacopo|last5=Nevatia|first5=Ram|last6=Medioni|first6=Gerard|date=2019|title=Deep, landmark-free FAME: Face alignment, modeling, and expression estimation|journal=International Journal of Computer Vision}}</ref> employing pre deep learning methods to automatically generate proxy labels for training networks to estimate [[Six degrees of freedom]] (6DoF) head poses<ref>{{Cite journal|last1=Chang|first1=FengJu|last2=Tuan Tran|first2=Anh|last3=Hassner|first3=Tal|last4=Masi|first4=Iacopo|last5=Nevatia|first5=Ram|last6=Medioni|first6=Gerard|date=2017|title=FacePoseNet: Making a Case for Landmark-Free Face Alignment|journal=International Conference on Computer Vision (CVPR) Workshops|arxiv=1708.07517}}</ref> and 3D facial deformations.<ref>{{Cite journal|last1=Chang|first1=FengJu|last2=Tuan Tran|first2=Anh|last3=Hassner|first3=Tal|last4=Masi|first4=Iacopo|last5=Nevatia|first5=Ram|last6=Medioni|first6=Gerard|date=2018|title=ExpNet: Landmark-free, deep, 3D facial expressions|journal=Conference on Automatic Face and Gesture Recognition (FG)|arxiv=1802.00542}}</ref>   

In 2018, researchers from [[UC Riverside]] proposed a method to localize actions/events in videos using only weak supervision, i.e., video-level labels, without any information about the start and end time of the events while training. Their work <ref>{{Cite journal|last1=Paul|first1=Sujoy|last2=Roy|first2=Sourya|last3=Roy-Chowdhury|first3=Amit K.|date=2018|title=W-TALC: Weakly-supervised Temporal Activity Localization and Classification|journal=European Conference on Computer Vision (ECCV)|arxiv=1807.10418}}</ref> introduced an attention-based similarity between two videos, which acts as a regularizer for learning with weak labels. Thereafter in 2019, they introduced a new problem <ref>{{Cite journal|last1=Mithun|first1=Niluthpol Chowdhury|last2=Paul|first2=Sujoy|last3=Roy-Chowdhury|first3=Amit K.|date=2019|title=Weakly Supervised Video Moment Retrieval From Text Queries|journal=Computer Vision and Pattern Recognition (CVPR)|arxiv=1904.03282}}</ref> of event localization in videos using text queries from users, but with weak annotations while training. Later in a collaboration with [[NEC Laboratories America]] a similar attention-based alignment mechanism with weak labels was introduced for adapting a source semantic segmentation model to a target domain.<ref>{{Cite journal|last1=Paul|first1=Sujoy|last2=Tsai|first2=Yi-Hsuan|last3=Schulter|first3=Samuel|last4=Roy-Chowdhury|first4=Amit K.|last5=Chandraker|first5=Manmohan|date=2020|title=Domain Adaptive Semantic Segmentation Using Weak Labels|journal=European Conference on Computer Vision (ECCV)|arxiv=2007.15176}}</ref> When the weak labels of the target images are estimated using the source model, it is unsupervised [[domain adaptation]], requiring no target annotation cost, and when the weak labels are acquired from an annotator, it incurs a very small amount of annotation cost and falls under the category of weakly-supervised domain adaptation, which is first introduced in this work for semantic segmentation.

[[Stanford University]] researchers created Snorkel, an open-source system for quickly assembling training data through weak supervision.<ref>{{Cite web|url=https://dawn.cs.stanford.edu/2017/05/08/snorkel/|title=Snorkel and The Dawn of Weakly Supervised Machine Learning · Stanford DAWN|website=dawn.cs.stanford.edu|access-date=2019-06-05}}</ref> Snorkel employs the central principles of the data programming paradigm,<ref name="Data Programming 1605"/>  in which developers create labeling functions, which are then used to programmatically label data, and employs supervised learning techniques to assess the accuracy of those labeling functions.<ref>{{Cite web|url=https://hazyresearch.github.io/snorkel/|title=Snorkel by HazyResearch|website=hazyresearch.github.io|access-date=2019-06-05}}</ref> In this way, potentially low-quality inputs can be used to create high-quality models. Afterward, the Stanford AI Lab researchers created Snorkel AI, which originated from the Snorkel project, using state-of-the-art programmatic data labeling and weak supervision approaches, successfully decreasing AI development costs and time significantly.<ref>{{Cite web|title=Snorkel AI scores $35M Series B to automate data labeling in machine learning|url=https://social.techcrunch.com/2021/04/07/snorkel-ai-scores-35m-series-b-to-automate-data-labeling-in-machine-learning-apps/|access-date=2021-10-08|website=TechCrunch|language=en-US}}</ref>

In a joint work with [[Google]], Stanford researchers showed that existing organizational knowledge resources could be converted into weak supervision sources and used to significantly decrease development costs and time.<ref>{{Cite journal|last1=Malkin|first1=Rob|last2=Ré|first2=Christopher|last3=Kuchhal|first3=Rahul|last4=Alborzi|first4=Houman|last5=Hancock|first5=Braden|last6=Ratner|first6=Alexander|last7=Sen|first7=Souvik|last8=Xia|first8=Cassandra|last9=Shao|first9=Haidong|date=2018-12-02|title=Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale|journal=Proceedings. ACM-SIGMOD International Conference on Management of Data|volume=2019|pages=362–375|arxiv=1812.00417|pmid=31777414|doi=10.1145/3299869.3314036|pmc=6879379|bibcode=2018arXiv181200417B}}</ref>

In 2019, [[Massachusetts Institute of Technology]] and [[Google]] researchers released cleanlab, the first standardized [[Python (programming language)|Python]] package for machine learning and [[deep learning]] with noisy labels.<ref>{{Cite web|url=https://l7.curtisnorthcutt.com/cleanlab-python-package|title=Announcing cleanlab: a Python Package for ML and Deep Learning on Datasets with Label Errors|website=l7.curtisnorthcutt.com|language=en|access-date=2020-02-04}}</ref> Cleanlab implements [[confident learning]],<ref>{{Cite web|url=https://l7.curtisnorthcutt.com/confident-learning|title=An Introduction to Confident Learning: Finding and Learning with Label Errors in Datasets|website=l7.curtisnorthcutt.com|access-date=2020-02-04}}</ref><ref>{{cite arXiv|last1=Northcutt|first1=Curtis G.|last2=Jiang|first2=Lu|last3=Chuang|first3=Isaac L.|date=2019-10-31|title=Confident Learning: Estimating Uncertainty in Dataset Labels|eprint=1911.00068|class=stat.ML}}</ref> a framework of theory and algorithms for dealing with uncertainty in dataset labels, to (1) find label errors in datasets, (2) characterize label noise, and (3) standardize and simplify research in weak supervision and learning with noisy labels.<ref>{{Cite web|url=https://github.com/cgnorthcutt/cleanlab|title=CleanLab for Finding and Learning with Noisy Labels|last=Northcutt|first=Curtis|website=[[GitHub]]|access-date=9 October 2019}}</ref>

Researchers at [[University of Massachusetts Amherst]] propose augmenting traditional [[Active learning (machine learning)|active learning]] approaches by soliciting labels on features rather than instances within a data set.<ref>{{Cite web|url=http://gregorydruck.name/pubs/druck09active.pdf|title=Active Learning by Labeling Features|last=Druck|first=Gregory|access-date=4 June 2019}}</ref>

Researchers at [[Johns Hopkins University]] propose reducing the cost of labeling data sets by having annotators provide rationales supporting each of their data annotations, then using those rationales to train both discriminative and generative models for labeling additional data.<ref>{{Cite web|url=http://www.cs.jhu.edu/~ozaidan/rationales/Zaidan_etal_rationales-nips2008.pdf|title=Machine Learning with Annotator Rationales to Reduce Annotation Cost|last=Zaidan|first=Omar|access-date=4 June 2019}}</ref>

Researchers at [[University of Alberta Faculty of Engineering|University of Alberta]] propose a method that applies traditional active learning approaches to enhance the quality of the imperfect labels provided by weak supervision.<ref>{{Cite journal|last1=Nashaat|first1=Mona|last2=Ghosh|first2=Aindrila|last3=Miller|first3=James|last4=Quader|first4=Shaikh|last5=Marston|first5=Chad|last6=Puget|first6=Jean-Francois|date=December 2018|title=Hybridization of Active Learning and Data Programming for Labeling Large Industrial Datasets|journal=2018 IEEE International Conference on Big Data (Big Data)|location=Seattle, WA, USA|publisher=IEEE|pages=46–55|doi=10.1109/BigData.2018.8622459|isbn=9781538650356|s2cid=59233854}}</ref>

==Semi-supervised learning==
[[File:Example of unlabeled data in semisupervised learning.png|thumb|194px|An example of the influence of unlabeled data in semi-supervised learning. The top panel shows a decision boundary we might adopt after seeing only one positive (white circle) and one negative (black circle) example. The bottom panel shows a decision boundary we might adopt if, in addition to the two labeled examples, we were given a collection of unlabeled data (gray circles). This could be viewed as performing [[Cluster analysis|clustering]] and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.]]
'''Semi-supervised learning''' is a special instance of weak supervision that combines a small amount of [[labeled data]] with a large amount of unlabeled data during training. Semi-supervised learning falls between [[unsupervised learning]] (with no labeled training data) and [[supervised learning]] (with only labeled training data).

Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.

A set of <math>l</math> [[Independent identically distributed|independently identically distributed]] examples <math>x_1,\dots,x_l \in X</math> with corresponding labels <math>y_1,\dots,y_l \in Y</math> and <math>u</math> unlabeled examples <math>x_{l+1},\dots,x_{l+u} \in X</math> are processed. Semi-supervised learning combines this information to surpass the [[Statistical classification|classification]] performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.

Semi-supervised learning may refer to either [[Transduction (machine learning)|transductive learning]] or [[Inductive reasoning|inductive learning]].<ref>{{Cite journal|title=Semi-Supervised Learning Literature Survey, Page 5|citeseerx = 10.1.1.99.9681|year = 2007}}</ref> The goal of transductive learning is to infer the correct labels for the given unlabeled data <math>x_{l+1},\dots,x_{l+u}</math> only. The goal of inductive learning is to infer the correct mapping from <math>X</math> to <math>Y</math>.

Intuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam.

It is unnecessary (and, according to [[Vapnik's principle]], imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.

===Assumptions ===
In order to make any use of unlabeled data, some relationship to the underlying distribution of data must exist. Semi-supervised learning algorithms make use of at least one of the following assumptions:{{sfn|Chapelle|Schölkopf|Zien|2006}}

====Continuity / smoothness assumption====
''Points that are close to each other are more likely to share a label.'' This is also generally assumed in supervised learning and yields a preference for geometrically simple [[decision boundary|decision boundaries]]. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes.{{Citation needed|reason=This is not clear a priori!|date=August 2021}}

====Cluster assumption====
''The data tend to form discrete clusters, and points in the same cluster are more likely to share a label'' (although data that shares a label may spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to [[feature learning]] with clustering algorithms.

====Manifold assumption====
{{Main|Manifold hypothesis}}
''The data lie approximately on a [[manifold]] of much lower dimension than the input space.'' In this case learning the manifold using both the labeled and unlabeled data can avoid the [[curse of dimensionality]]. Then learning can proceed using distances and densities defined on the manifold.

The manifold assumption is practical when high-dimensional data are generated by some process that may be hard to model directly, but which has only a few degrees of freedom. For instance, human voice is controlled by a few vocal folds,<ref name = "StevensKN">{{Cite book|title=Acoustic phonetics|last=Stevens, Kenneth N., 1924-|date=1998|publisher=MIT Press|isbn=0-585-08720-2|location=Cambridge, Mass.|oclc=42856189}}</ref> and images of various facial expressions are controlled by a few muscles. In these cases, it is better to consider distances and smoothness in the natural space of the generating problem, rather than in the space of all possible acoustic waves or images, respectively.

===History===
The heuristic approach of ''self-training'' (also known as ''self-learning'' or ''self-labeling'') is historically the oldest approach to semi-supervised learning,{{sfn|Chapelle|Schölkopf|Zien|2006}} with examples of applications starting in the 1960s.<ref>{{Cite journal|last=Scudder|first=H.|date=July 1965|title=Probability of error of some adaptive pattern-recognition machines|journal=IEEE Transactions on Information Theory|volume=11|issue=3|pages=363–371|doi=10.1109/TIT.1965.1053799|issn=1557-9654}}</ref>

The transductive learning framework was formally introduced by [[Vladimir Vapnik]] in the 1970s.<ref>{{cite book|last1=Vapnik |first1=V. |last2= Chervonenkis |first2=A.|title= Theory of Pattern Recognition |language= ru |publisher= Nauka |location=Moscow |year=1974}} cited in {{harvnb|Chapelle|Schölkopf|Zien|2006|p= 3}}</ref> Interest in inductive learning using generative models also began in the 1970s. A [[Probably approximately correct learning|''probably approximately correct'' learning]] bound for semi-supervised learning of a [[Gaussian]] mixture was demonstrated by Ratsaby and Venkatesh in 1995.<ref name="Ratsaby">{{cite web|last1=Ratsaby |first1=J.|last2= Venkatesh|first2= S. |url=http://www.ariel.ac.il/sites/ratsaby/Publications/PDF/colt95.pdf |title=Learning from a mixture of labeled and unlabeled examples with parametric side information}} in {{Cite book|title=Proceedings of the eighth annual conference on Computational learning theory - COLT '95|date=1995|publisher=ACM Press|isbn=0-89791-723-5|location=New York, New York, USA|pages= 412–417 |doi=10.1145/225298.225348|s2cid=17561403}}. Cited in {{harvnb|Chapelle|Schölkopf|Zien|2006|p=4}}</ref>

Semi-supervised learning has recently{{When|date=July 2022}} become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are available—e.g. text on websites, protein sequences, or images.<ref name="survey">{{cite web|last=Zhu|first= Xiaojin |url=http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf |title=Semi-supervised learning literature survey |publisher= University of Wisconsin-Madison |year=2008}}</ref>

===Methods===

====Generative models====
Generative approaches to statistical learning first seek to estimate <math>p(x|y)</math>,{{disputed inline|date=November 2017}} the distribution of data points belonging to each class. The probability <math>p(y|x)</math> that a given point <math>x</math> has label <math>y</math> is then proportional to <math>p(x|y)p(y)</math> by [[Bayes' theorem|Bayes' rule]]. Semi-supervised learning with [[generative model]]s can be viewed either as an extension of supervised learning (classification plus information about <math>p(x)</math>) or as an extension of unsupervised learning (clustering plus some labels).

Generative models assume that the distributions take some particular form <math>p(x|y,\theta)</math> parameterized by the vector <math>\theta</math>. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone.<ref>{{Citation|last1=Fabio|first1=Cozman|title=Risks of Semi-Supervised Learning: How Unlabeled Data Can Degrade Performance of Generative Classifiers|date=2006-09-22|work=Semi-Supervised Learning|pages=56–72|publisher=The MIT Press|isbn=978-0-262-03358-9|last2=Ira|first2=Cohen|doi=10.7551/mitpress/9780262033589.003.0004}} In: {{harvnb|Chapelle|Schölkopf|Zien|2006}}</ref> 
However, if the assumptions are correct, then the unlabeled data necessarily improves performance.<ref name = "Ratsaby"/>

The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.

The parameterized [[joint distribution]] can be written as <math>p(x,y|\theta)=p(y|\theta)p(x|y,\theta)</math> by using the [[Chain rule (probability)|chain rule]]. Each parameter vector <math>\theta</math> is associated with a decision function <math>f_\theta(x) = \underset{y}{\operatorname{argmax}}\ p(y|x,\theta)</math>. 
The parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by <math>\lambda</math>:

:<math>\underset{\Theta}{\operatorname{argmax}}\left( \log p(\{x_i,y_i\}_{i=1}^l | \theta) + \lambda \log p(\{x_i\}_{i=l+1}^{l+u}|\theta)\right) </math><ref name="SSL_EoML">Zhu, Xiaojin. [http://pages.cs.wisc.edu/~jerryzhu/pub/SSL_EoML.pdf   Semi-Supervised Learning] University of Wisconsin-Madison.</ref>

====Low-density separation====
Another major class of methods attempts to place boundaries in regions with few data points (labeled or unlabeled). One of the most commonly used algorithms is the [[Support vector machine#Transductive support vector machines|transductive support vector machine]], or TSVM (which, despite its name, may be used for inductive learning as well). Whereas [[support vector machines]] for supervised learning seek a decision boundary with maximal [[Margin (machine learning)|margin]] over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard [[hinge loss]] <math>(1-yf(x))_+</math> for labeled data, a loss function <math>(1-|f(x)|)_+</math> is introduced over the unlabeled data by letting <math>y=\operatorname{sign}{f(x)}</math>. TSVM then selects <math>f^*(x) = h^*(x) + b</math> from a [[reproducing kernel Hilbert space]] <math>\mathcal{H}</math> by minimizing the [[Regularization (mathematics)|regularized]] [[Empirical risk minimization|empirical risk]]:

:<math>f^* = \underset{f}{\operatorname{argmin}}\left( 
\displaystyle \sum_{i=1}^l(1-y_if(x_i))_+ + \lambda_1 \|h\|_\mathcal{H}^2 + \lambda_2 \sum_{i=l+1}^{l+u} (1-|f(x_i)|)_+
\right) </math>

An exact solution is intractable due to the non-[[convex function|convex]] term <math>(1-|f(x)|)_+</math>, so research focuses on useful approximations.<ref name="SSL_EoML"/>

Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).

==== Laplacian regularization ====

Laplacian regularization has been historically approached through graph-Laplacian.
Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its <math>k</math> nearest neighbors or to examples within some distance <math>\epsilon</math>. The weight <math>W_{ij}</math> of an edge between <math>x_i</math> and <math>x_j</math> is then set to <math>e^{\frac{-\|x_i-x_j\|^2}{\epsilon}}</math>.

Within the framework of [[manifold regularization]],<ref>{{cite journal|author1=M. Belkin |author2=P. Niyogi |title=Semi-supervised Learning on Riemannian Manifolds|journal=Machine Learning|volume=56|issue=Special Issue on Clustering|pages=209–239|year=2004|url=http://booksc.org/dl/11288633/421f61|doi=10.1023/b:mach.0000033120.25363.1e|doi-access=free}}</ref><ref>M. Belkin, P. Niyogi, V. Sindhwani. On Manifold Regularization. AISTATS 2005.</ref> the graph serves as a proxy for the manifold. A term is added to the standard [[Tikhonov regularization]] problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes

:<math>\underset{f\in\mathcal{H}}{\operatorname{argmin}}\left(
\frac{1}{l}\displaystyle\sum_{i=1}^l V(f(x_i),y_i) + 
\lambda_A \|f\|^2_\mathcal{H} + 
\lambda_I \int_\mathcal{M}\|\nabla_\mathcal{M} f(x)\|^2dp(x)
\right) </math><ref name="SSL_EoML"/>

where <math>\mathcal{H}</math> is a reproducing kernel [[Hilbert space]] and <math>\mathcal{M}</math> is the manifold on which the data lie. The regularization parameters <math>\lambda_A</math> and <math>\lambda_I</math> control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the [[Laplacian matrix|graph Laplacian]] <math>L = D - W</math> where <math>D_{ii} = \sum_{j=1}^{l+u} W_{ij}</math> and <math>\mathbf{f}</math> is the vector <math>[f(x_1)\dots f(x_{l+u})]</math>, we have

:<math>\mathbf{f}^T L \mathbf{f} = \displaystyle\sum_{i,j=1}^{l+u}W_{ij}(f_i-f_j)^2 \approx \int_\mathcal{M}\|\nabla_\mathcal{M} f(x)\|^2dp(x)</math>.

The graph-based approach to Laplacian regularization is to put in relation with [[finite difference method]].{{Clarify|date=July 2022}}{{Cn|date=July 2022}}

The Laplacian can also be used to extend the supervised learning algorithms: [[regularized least squares]] and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.

====Heuristic approaches====
Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples <math>x_1,\dots,x_{l+u}</math> may inform a choice of representation, [[distance metric]], or [[Kernel (statistics)|kernel]] for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples. In this vein, some methods learn a low-dimensional representation using the supervised data and then apply either low-density separation or graph-based methods to the learned representation.<ref>{{cite journal |last1=Iscen |first1=Ahmet |last2=Tolias |first2=Giorgos |last3=Avrithis |first3=Yannis |last4=Chum |first4=Ondrej |title=Label Propagation for Deep Semi-Supervised Learning |journal=Conference on Computer Vision and Pattern Recognition (CVPR) |date=2019 |pages=5065–5074 |doi=10.1109/CVPR.2019.00521 |url=https://ieeexplore.ieee.org/document/8954421 |access-date=26 March 2021|arxiv=1904.04717 |isbn=978-1-7281-3293-8 |s2cid=104291869 }}</ref><ref>{{cite journal |last1=Burkhart |first1=Michael C. |last2=Shan |first2=Kyle |title=Deep Low-Density Separation for Semi-supervised Classification |journal=International Conference on Computational Science (ICCS) |series=Lecture Notes in Computer Science |date=2020 |volume=12139 |pages=297–311 |doi=10.1007/978-3-030-50420-5_22 |isbn=978-3-030-50419-9 |doi-access=free }}</ref> Iteratively refining the representation and then performing semi-supervised learning on said representation may further improve performance.

''Self-training'' is a wrapper method for semi-supervised learning.<ref>{{Cite journal|title = Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study|journal = Knowledge and Information Systems|date = 2013-11-26|issn = 0219-1377|pages = 245–284|volume = 42|issue = 2|doi = 10.1007/s10115-013-0706-y|language = en|first1 = Isaac|last1 = Triguero|first2 = Salvador|last2 = García|first3 = Francisco|last3 = Herrera|s2cid = 1955810}}</ref> First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident in are added at each step.<ref>{{Cite journal|title = Self-Trained LMT for Semisupervised Learning|journal = Computational Intelligence and Neuroscience|date = 2015-12-29|pages = 3057481|volume = 2016|doi = 10.1155/2016/3057481|pmid = 26839531|pmc = 4709606|language = en|first1 = Nikos|last1 = Fazakis|first2 = Stamatis|last2 = Karlos|first3 = Sotiris|last3 = Kotsiantis|first4 = Kyriakos|last4 = Sgarbas|doi-access = free}}</ref>

[[Co-training]] is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.<ref>{{Cite book|title = Analysis of Co-training Algorithm with Very Small Training Sets|publisher = Springer Berlin Heidelberg|date = 2012-11-07|isbn = 9783642341656|pages = 719–726|series = Lecture Notes in Computer Science|language = en|first1 = Luca|last1 = Didaci|first2 = Giorgio|last2 = Fumera|first3 = Fabio|last3 = Roli|editor-first = Georgy|editor-last = Gimel’farb|editor-first2 = Edwin|editor-last2 = Hancock|editor-first3 = Atsushi|editor-last3 = Imiya|editor-first4 = Arjan|editor-last4 = Kuijper|editor-first5 = Mineichi|editor-last5 = Kudo|editor-first6 = Shinichiro|editor-last6 = Omachi|editor-first7 = Terry|editor-last7 = Windeatt|editor-first8 = Keiji|editor-last8 = Yamada|doi = 10.1007/978-3-642-34166-3_79| s2cid=46063225 }}</ref>

===In human cognition===
Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data.<ref name="ZhuGoldberg">
{{Cite book|title=Introduction to semi-supervised learning|last=Zhu|first=Xiaojin|date=2009|publisher=Morgan & Claypool Publishers|others=Goldberg, A. B. (Andrew B.)|isbn=978-1-59829-548-1|location=[San Rafael, Calif.]|oclc=428541480}}</ref> More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human [[concept learning]] involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).

Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces.<ref>{{cite journal |author1=Younger B. A. |author2=Fearing D. D. | year = 1999 | title = Parsing Items into Separate Categories: Developmental Change in Infant Categorization | journal = Child Development | volume = 70 | issue = 2| pages = 291–303 | doi=10.1111/1467-8624.00022}}</ref> Infants and children take into account not only unlabeled examples, but the [[sampling (statistics)|sampling]] process from which labeled examples arise.<ref>{{cite journal|author1=Xu, F.  |author2=Tenenbaum, J. B. |name-list-style=amp |year=2007|title=Sensitivity to sampling in Bayesian word learning|volume=10|issue=3 |pages=288–297|doi=10.1111/j.1467-7687.2007.00590.x|pmid=17444970 |journal=Developmental Science|citeseerx=10.1.1.141.7505 }}</ref><ref>{{cite journal|author=Gweon, H., Tenenbaum J.B., and Schulz L.E|year=2010|title=Infants consider both the sample and the sampling process in inductive generalization|journal=Proc Natl Acad Sci U S A|volume=107|issue=20|pages=9066–71|doi=10.1073/pnas.1003095107|pmid=20435914|pmc=2889113|bibcode=2010PNAS..107.9066G|doi-access=free}}</ref>

==See also==
*[[PU learning]]

==References==
<references responsive="0" />

== Sources ==
* {{Cite book  | last1 = Chapelle | first1 = Olivier | last2 = Schölkopf | first2 = Bernhard | last3 = Zien | first3 = Alexander | title = Semi-supervised learning | year = 2006 | publisher = MIT Press | location = Cambridge, Mass. | isbn = 978-0-262-03358-9 }}

==External links==
* [http://manifold.cs.uchicago.edu/manifold_regularization/software.html Manifold Regularization] A freely available [[MATLAB]] implementation of the graph-based semi-supervised algorithms Laplacian support vector machines and Laplacian regularized least squares.
* [http://sci2s.ugr.es/keel/algorithms.php#sub10 KEEL: A software tool to assess evolutionary algorithms for Data Mining problems (regression, classification, clustering, pattern mining and so on)] KEEL module for semi-supervised learning.
* [http://pages.cs.wisc.edu/~jerryzhu/ssl/software.html Semi-Supervised Learning Software] Semi-Supervised Learning Software
* [https://scikit-learn.org/stable/modules/semi_supervised.html Semi-Supervised learning — scikit-learn documentation] Semi-supervised learning in [[scikit-learn]].

{{Differentiable computing}}

[[Category:Machine learning]]