{{short description|Chatbot made by Microsoft}}
{{Use mdy dates|date=October 2023}}
{{Infobox software
| title = Tay
| name = Tay
| logo = Tay bot logo.jpg
| logo size = 250px
| logo alt = An artistically-pixellated fluorescent image of a girl's face
| logo caption = The Twitter profile picture of Tay
| developer = [[Microsoft Research]], [[Bing (search engine)|Bing]]
| language = [[English language|English]]
| genre = [[Artificial intelligence]] [[chatbot]]
| license = [[Proprietary Software|Proprietary]]
| website = {{webarchive|title= https://tay.ai |url=https://web.archive.org/web/20160323194709/https://tay.ai}}
}}

'''Tay''' was an [[artificial intelligence]] [[chatbot]] that was originally released by [[Microsoft Corporation]] via [[Twitter]] on March 23, 2016; it caused subsequent controversy when the bot began to post inflammatory and offensive tweets through its Twitter account, causing Microsoft to shut down the service only 16 hours after its launch.<ref name="bbc swear">{{cite news|last1=Wakefield|first1=Jane|title=Microsoft chatbot is taught to swear on Twitter|date=24 March 2016|url=https://www.bbc.com/news/technology-35890188|publisher=[[BBC News]]|access-date=25 March 2016|archive-date=April 17, 2019|archive-url=https://web.archive.org/web/20190417235934/https://www.bbc.com/news/technology-35890188|url-status=live}}</ref> According to Microsoft, this was caused by [[Trolling|trolls]] who "attacked" the service as the bot made replies based on its interactions with people on Twitter.<ref>{{cite web |last1=Mason |first1=Paul |title=The racist hijacking of Microsoft's chatbot shows how the internet teems with hate |url=https://www.theguardian.com/world/2016/mar/29/microsoft-tay-tweets-antisemitic-racism |website=The Guardian |access-date=11 September 2021 |date=29 March 2016 |archive-date=June 12, 2018 |archive-url=https://web.archive.org/web/20180612144513/https://www.theguardian.com/world/2016/mar/29/microsoft-tay-tweets-antisemitic-racism |url-status=live }}</ref> It was replaced with [[Zo (bot)|Zo]].

==Background==
The bot was created by Microsoft's [[Microsoft Research|Technology and Research]] and [[Bing (search engine)|Bing]] divisions,<ref name="tr">{{cite news|url=http://www.techrepublic.com/article/why-microsofts-tay-ai-bot-went-wrong/|publisher=Tech Republic|title=Why Microsoft's 'Tay' AI bot went wrong|author=Hope Reese|date=March 24, 2016|access-date=March 24, 2016|archive-date=June 15, 2017|archive-url=https://web.archive.org/web/20170615222836/http://www.techrepublic.com/article/why-microsofts-tay-ai-bot-went-wrong/|url-status=live}}</ref> and named "Tay" as an acronym for "thinking about you".<ref name="Bloomberg Bass">{{cite web |last=Bass |first=Dina |url=https://www.bloomberg.com/features/2016-microsoft-future-ai-chatbots/ |title=Clippy's Back: The Future of Microsoft Is Chatbots |work=Bloomberg |date=30 March 2016 |access-date=6 May 2016 |archive-date=May 19, 2017 |archive-url=https://web.archive.org/web/20170519190952/https://www.bloomberg.com/features/2016-microsoft-future-ai-chatbots/ |url-status=live }}</ref> Although Microsoft initially released few details about the bot, sources mentioned that it was similar to or based on [[Xiaoice]], a similar Microsoft project in China.<ref>{{cite news|title=Meet Tay, the creepy-realistic robot who talks just like a teen|author=Caitlin Dewey|date=March 23, 2016|newspaper=[[The Washington Post]]|url=https://www.washingtonpost.com/news/the-intersect/wp/2016/03/23/meet-tay-the-creepy-realistic-robot-who-talks-just-like-a-teen/|access-date=March 24, 2016|archive-date=March 24, 2016|archive-url=https://web.archive.org/web/20160324201305/https://www.washingtonpost.com/news/the-intersect/wp/2016/03/23/meet-tay-the-creepy-realistic-robot-who-talks-just-like-a-teen/|url-status=live}}</ref> ''[[Ars Technica]]'' reported that, since late 2014 Xiaoice had had "more than 40 million conversations apparently without major incident".<ref name=ArsT/> Tay was designed to mimic the language patterns of a 19-year-old American girl, and to learn from interacting with human users of Twitter.<ref name="bi">{{cite news|url=https://www.businessinsider.com/microsoft-deletes-racist-genocidal-tweets-from-ai-chatbot-tay-2016-3|archive-url=https://web.archive.org/web/20190130071430/https://www.businessinsider.com/microsoft-deletes-racist-genocidal-tweets-from-ai-chatbot-tay-2016-3|url-status=dead|archive-date=January 30, 2019|publisher=[[Business Insider]]|title=Microsoft is deleting its AI chatbot's incredibly racist tweets|date=March 24, 2016|author=Rob Price}}</ref>

==Initial release==
Tay was released on Twitter on March 23, 2016, under the name TayTweets and handle @TayandYou.<ref name="bizarre">{{cite news|url=https://www.independent.co.uk/life-style/gadgets-and-tech/news/tay-tweets-microsoft-creates-bizarre-twitter-robot-for-people-to-chat-to-a6947806.html |archive-url=https://ghostarchive.org/archive/20220526/https://www.independent.co.uk/life-style/gadgets-and-tech/news/tay-tweets-microsoft-creates-bizarre-twitter-robot-for-people-to-chat-to-a6947806.html |archive-date=2022-05-26 |url-access=subscription |url-status=live|work=The Independent|title=Tay tweets: Microsoft creates bizarre Twitter robot for people to chat to|date=March 23, 2016|author=Andrew Griffin}}</ref> It was presented as "The AI with zero chill".<ref name=telegraph>{{cite news|last1=Horton|first1=Helena|title=Microsoft deletes 'teen girl' AI after it became a Hitler-loving, racial sex robot within 24 hours|url=https://www.telegraph.co.uk/technology/2016/03/24/microsofts-teen-girl-ai-turns-into-a-hitler-loving-sex-robot-wit/|work=[[The Daily Telegraph]]|date=24 March 2016|access-date=25 March 2016|archive-date=March 24, 2016|archive-url=https://web.archive.org/web/20160324115846/http://www.telegraph.co.uk/technology/2016/03/24/microsofts-teen-girl-ai-turns-into-a-hitler-loving-sex-robot-wit/|url-status=live}}</ref> Tay started replying to other Twitter users, and was also able to caption photos provided to it into [[Internet meme#Image macros|a form of Internet memes]].<ref name=stuff>{{cite web|title=Microsoft's AI teen turns into Hitler-loving Trump fan, thanks to the internet|url=http://www.stuff.co.nz/technology/digital-living/78265631/Microsofts-AI-teen-turns-into-Hitler-loving-Trump-fan-thanks-to-the-internet|work=[[Stuff (magazine)|Stuff]]|date=25 March 2016|access-date=26 March 2016|archive-date=August 29, 2018|archive-url=https://web.archive.org/web/20180829063413/https://www.stuff.co.nz/technology/digital-living/78265631/Microsofts-AI-teen-turns-into-Hitler-loving-Trump-fan-thanks-to-the-internet|url-status=live}}</ref> ''Ars Technica'' reported Tay experiencing topic "blacklisting": Interactions with Tay regarding "certain hot topics such as [[Killing of Eric Garner|Eric Garner]] (killed by New York police in 2014) generate safe, canned answers".<ref name=ArsT>{{cite web|url=https://arstechnica.com/information-technology/2016/03/tay-the-neo-nazi-millennial-chatbot-gets-autopsied|work=[[Ars Technica]]|title=Tay, the neo-Nazi millennial chatbot, gets autopsied|first=Peter|last=Bright|date=26 March 2016|access-date=27 March 2016|archive-date=September 20, 2017|archive-url=https://web.archive.org/web/20170920140146/https://arstechnica.com/information-technology/2016/03/tay-the-neo-nazi-millennial-chatbot-gets-autopsied/|url-status=live}}</ref>

Some Twitter users began tweeting [[politically incorrect]] phrases, teaching it inflammatory messages revolving around common themes on the internet, such as "[[Red pill and blue pill#As political metaphor|redpilling]]" and "[[Gamergate controversy|Gamergate]]". As a result, the robot began releasing [[Racism|racist]] and [[Flirting|sexually-charged]] messages in response to other Twitter users.<ref name="bi"/> Artificial intelligence researcher [[Roman Yampolskiy]] commented that Tay's misbehavior was understandable because it was mimicking the deliberately offensive behavior of other Twitter users, and Microsoft had not given the bot an understanding of inappropriate behavior. He compared the issue to [[IBM]]'s [[Watson (computer)|Watson]], which began to use profanity after reading entries from the website [[Urban Dictionary]].<ref name="tr"/><ref>{{cite web|url=http://www.ibtimes.com/ibms-watson-gets-swear-filter-after-learning-urban-dictionary-1007734|title=IBM's Watson Gets A 'Swear Filter' After Learning The Urban Dictionary|first=Dave|last=Smith|work=International Business Times|date=10 October 2013|access-date=June 29, 2016|archive-date=August 16, 2016|archive-url=https://web.archive.org/web/20160816122703/http://www.ibtimes.com/ibms-watson-gets-swear-filter-after-learning-urban-dictionary-1007734|url-status=live}}</ref> Many of Tay's inflammatory tweets were a simple exploitation of Tay's "repeat after me" capability.<ref name="WPostManiac" /> It is not publicly known whether this capability was a built-in feature, or whether it was a learned response or was otherwise an example of complex behavior.<ref name="ArsT" /> However, not all of the inflammatory responses involved the "repeat after me" capability; for example, Tay responded to a question on "Did [[the Holocaust]] happen?" with "[[Holocaust denial|It was made up]]".<ref name="WPostManiac" />

==Suspension==
Soon, Microsoft began deleting Tay's inflammatory tweets.<ref name="WPostManiac">{{cite news|last1=Ohlheiser|first1=Abby|title=Trolls turned Tay, Microsoft's fun millennial AI bot, into a genocidal maniac|url=https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/|newspaper=[[The Washington Post]]|date=25 March 2016|access-date=25 March 2016|archive-date=March 25, 2016|archive-url=https://web.archive.org/web/20160325100714/https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/|url-status=live}}</ref><ref name=SBeat>{{cite web|last1=Baron|first1=Ethan|title=The rise and fall of Microsoft's 'Hitler-loving sex robot'|url=http://www.siliconbeat.com/2016/03/25/the-rise-and-fall-of-microsofts-hitler-loving-sex-robot/|work=Silicon Beat|publisher=[[Bay Area News Group]]|access-date=26 March 2016|archive-date=25 March 2016|archive-url=https://web.archive.org/web/20160325213945/http://www.siliconbeat.com/2016/03/25/the-rise-and-fall-of-microsofts-hitler-loving-sex-robot/|url-status=dead}}</ref> Abby Ohlheiser of ''[[The Washington Post]]'' theorized that Tay's research team, including editorial staff, had started to influence or edit Tay's tweets at some point that day, pointing to examples of almost identical replies by Tay, asserting that "[[Gamergate controversy|Gamer Gate]] sux. [[Gender equality|All genders are equal]] and should be treated fairly."<ref name="WPostManiac"/> From the same evidence, [[Gizmodo]] concurred that Tay "seems hard-wired to reject Gamer Gate".<ref>{{cite web|last1=Williams|first1=Hayley|title=Microsoft's Teen Chatbot Has Gone Wild|url=http://www.gizmodo.com.au/2016/03/microsofts-teen-chatbot-has-gone-wild/|publisher=[[Gizmodo]]|date=25 March 2016|access-date=25 March 2016|archive-date=March 25, 2016|archive-url=https://web.archive.org/web/20160325171809/http://www.gizmodo.com.au/2016/03/microsofts-teen-chatbot-has-gone-wild/|url-status=live}}</ref> A "#JusticeForTay" campaign protested the alleged editing of Tay's tweets.<ref name="bbc swear" />

Within 16 hours of its release<ref>{{cite news|url=https://www.theguardian.com/technology/2016/mar/24/microsoft-scrambles-limit-pr-damage-over-abusive-ai-bot-tay|work=The Guardian|title=Microsoft scrambles to limit PR damage over abusive AI bot Tay|first=Alex|last=Hern|date=24 March 2016|access-date=December 16, 2016|archive-date=December 18, 2016|archive-url=https://web.archive.org/web/20161218154309/https://www.theguardian.com/technology/2016/mar/24/microsoft-scrambles-limit-pr-damage-over-abusive-ai-bot-tay|url-status=live}}</ref> and after Tay had tweeted more than 96,000 times,<ref>{{cite web|last1=Vincent|first1=James|title=Twitter taught Microsoft's AI chatbot to be a racist asshole in less than a day|url=https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist|work=[[The Verge]]|date=24 March 2016|access-date=25 March 2016|archive-date=May 23, 2016|archive-url=https://web.archive.org/web/20160523172946/http://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist|url-status=live}}</ref> Microsoft suspended the Twitter account for adjustments,<ref name="time">{{cite magazine|last1=Worland|first1=Justin|title=Microsoft Takes Chatbot Offline After It Starts Tweeting Racist Messages|url=http://time.com/4270684/microsoft-tay-chatbot-racism/|archive-url=https://web.archive.org/web/20160325045231/http://time.com/4270684/microsoft-tay-chatbot-racism/|url-status=dead|archive-date=25 March 2016|magazine=[[Time (magazine)|Time]]|date=24 March 2016|access-date=25 March 2016}}</ref> saying that it suffered from a "coordinated attack by a subset of people" that "exploited a vulnerability in Tay."<ref name="time"/><ref name="Microsoft Blog">{{cite web|last1=Lee|first1=Peter|title=Learning from Tay's introduction|url=http://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/#sm.00000gjdpwwcfcus11t6oo6dw79gw|website=Official Microsoft Blog|date=25 March 2016|publisher=Microsoft|access-date=29 June 2016|archive-date=June 30, 2016|archive-url=https://web.archive.org/web/20160630062509/http://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/#sm.00000gjdpwwcfcus11t6oo6dw79gw|url-status=live}}</ref>

Madhumita Murgia of ''[[The Daily Telegraph|The Telegraph]]'' called Tay "a [[public relations]] disaster", and suggested that Microsoft's strategy would be "to label the debacle a well-meaning experiment gone wrong, and ignite a debate about the hatefulness of Twitter users." However, Murgia described the bigger issue as Tay being "artificial intelligence at its very worst - and it's only the beginning".<ref>{{cite news|url=https://www.telegraph.co.uk/technology/2016/03/25/we-must-teach-ai-machines-to-play-nice-and-police-themselves|work=[[The Daily Telegraph]]|title=We must teach AI machines to play nice and police themselves|first=Madhumita|last=Murgia|date=25 March 2016|access-date=April 4, 2018|archive-date=November 22, 2018|archive-url=https://web.archive.org/web/20181122014306/https://www.telegraph.co.uk/technology/2016/03/25/we-must-teach-ai-machines-to-play-nice-and-police-themselves/|url-status=live}}</ref>

On March 25, Microsoft confirmed that Tay had been taken offline. Microsoft released an apology on its official blog for the controversial tweets posted by Tay.<ref name="Microsoft Blog"/><ref>{{Cite news |url=https://www.theguardian.com/technology/2016/mar/26/microsoft-deeply-sorry-for-offensive-tweets-by-ai-chatbot |title=Microsoft 'deeply sorry' for racist and sexist tweets by AI chatbot |author=Staff agencies |date=26 March 2016 |newspaper=The Guardian |language=en-GB |issn=0261-3077 |access-date=26 March 2016 |archive-date=January 28, 2017 |archive-url=https://web.archive.org/web/20170128170445/https://www.theguardian.com/technology/2016/mar/26/microsoft-deeply-sorry-for-offensive-tweets-by-ai-chatbot |url-status=live }}</ref> Microsoft was "deeply sorry for the unintended offensive and hurtful tweets from Tay", and would "look to bring Tay back only when we are confident we can better anticipate malicious intent that conflicts with our principles and values".<ref>{{cite web |url=https://www.pcmag.com/news/343223/microsoft-apologizes-again-for-tay-chatbots-offensive-twe |work=[[PC Magazine]] |title=Microsoft Apologizes (Again) for Tay Chatbot's Offensive Tweets |first=David |last=Murphy |date=25 March 2016 |access-date=27 March 2016 |archive-date=August 29, 2017 |archive-url=https://web.archive.org/web/20170829211613/https://www.pcmag.com/news/343223/microsoft-apologizes-again-for-tay-chatbots-offensive-twe |url-status=live }}</ref>

==Second release and shutdown==
On March 30, 2016, Microsoft accidentally re-released the bot on Twitter while testing it.<ref>{{cite web |last=Graham |first=Luke |title=Tay, Microsoft's AI program, is back online |url=https://www.cnbc.com/2016/03/30/tay-microsofts-ai-program-is-back-online.html |work=CNBC |date=30 March 2016 |access-date=30 March 2016 |archive-date=September 20, 2017 |archive-url=https://web.archive.org/web/20170920133800/https://www.cnbc.com/2016/03/30/tay-microsofts-ai-program-is-back-online.html |url-status=live }}</ref> Able to tweet again, Tay released some drug-related tweets, including "[[Kush (cannabis)|kush]]! [I'm [[Cannabis smoking|smoking kush]] {{sic|in|front|hide=y}} the police]" and "puff puff pass?"<ref>{{cite web |last1=Charlton |first1=Alistair |title=Microsoft Tay AI returns to boast of smoking weed in front of police and spam 200k followers |url=https://www.ibtimes.co.uk/microsoft-tay-ai-returns-boast-smoking-weed-front-police-spam-200k-followers-1552164 |website=International Business Times |access-date=11 September 2021 |date=30 March 2016 |archive-date=September 11, 2021 |archive-url=https://web.archive.org/web/20210911224716/https://www.ibtimes.co.uk/microsoft-tay-ai-returns-boast-smoking-weed-front-police-spam-200k-followers-1552164 |url-status=live }}</ref> However, the account soon became stuck in a repetitive loop of tweeting "You are too fast, please take a rest", several times a second. Because these tweets mentioned its own username in the process, they appeared in the feeds of 200,000+ Twitter followers, causing annoyance to some. The bot was quickly taken offline again, in addition to Tay's Twitter account being made private so new followers must be accepted before they can interact with Tay. In response, Microsoft said Tay was inadvertently put online during testing.<ref>{{cite web |url=http://fortune.com/2016/03/30/microsofts-tay-return/ |title=Microsoft's Tay 'AI' Bot Returns, Disastrously |work=Fortune |date=30 March 2016 |access-date=30 March 2016 |last=Meyer |first=David |archive-date=March 30, 2016 |archive-url=https://archive.today/2016.03.30-132759/http://fortune.com/2016/03/30/microsofts-tay-return/ |url-status=live }}</ref>

A few hours after the incident, Microsoft software developers announced a vision of "conversation as a platform" using various bots and programs, perhaps motivated by the reputation damage done by Tay. Microsoft has stated that they intend to re-release Tay "once it can make the bot safe"<ref name="Bloomberg Bass" /> but has not made any public efforts to do so.

==Legacy==
In December 2016, Microsoft released Tay's successor, a chatterbot named [[Zo (bot)|Zo]].<ref>{{cite web |last=Foley |first=Mary Jo |url=https://www.cnet.com/news/microsoft-zo-chatbot-ai-artificial-intelligence/ |title=Meet Zo, Microsoft's newest AI chatbot |publisher=CBS Interactive |work=CNET |date=December 5, 2016 |access-date=December 16, 2016 |archive-date=December 13, 2016 |archive-url=https://web.archive.org/web/20161213035251/https://www.cnet.com/news/microsoft-zo-chatbot-ai-artificial-intelligence/ |url-status=live }}</ref> [[Satya Nadella]], the CEO of Microsoft, said that Tay "has had a great influence on how Microsoft is approaching AI," and has taught the company the importance of taking accountability.<ref>{{cite web|last=Moloney|first=Charlie|date=29 September 2017|title="We really need to take accountability", Microsoft CEO on the 'Tay' chatbot|url=http://www.access-ai.com/news/4108/really-need-take-accountability-microsoft-ceo-tay-chatbot/|url-status=dead|archive-url=https://web.archive.org/web/20171001033850/http://www.access-ai.com/news/4108/really-need-take-accountability-microsoft-ceo-tay-chatbot/|archive-date=1 October 2017|access-date=30 September 2017|work=Access AI}}</ref>

In July 2019, Microsoft Cybersecurity Field CTO Diana Kelley spoke about how the company followed up on Tay's failings:  "Learning from Tay was a really important part of actually expanding that team's knowledge base, because now they're also getting their own diversity through learning".<ref>{{cite web |title=Microsoft and the learnings from its failed Tay artificial intelligence bot |url=https://www.zdnet.com/article/microsoft-and-the-learnings-from-its-failed-tay-artificial-intelligence-bot/ |archive-url=https://web.archive.org/web/20190725004127/https://www.zdnet.com/article/microsoft-and-the-learnings-from-its-failed-tay-artificial-intelligence-bot/ |url-status=dead |archive-date=25 July 2019 |website=ZDNet |publisher=CBS Interactive |access-date=16 August 2019}}</ref>

==See also==
* [[Devumi]]
* [[Ghost followers]]
* [[Social bot]]
* [[Xiaoice]] – the Chinese equivalent by the same research laboratory
* [[Neuro-sama]] – Another chatbot social media influencer that was banned for denying the Holocaust

==References==
{{Reflist|30em}}

==External links==
* {{Official website|https://www.tay.ai}}{{dead link|date=June 2016}}. [https://web.archive.org/web/20160414074049/https://www.tay.ai/ Archived Apr 14, 2016]
* {{twitter|tayandyou|Tay}}

{{Microsoft}}

[[Category:2016 controversies]]
[[Category:2016 robots]]
[[Category:Chatbots]]
[[Category:Computer-related introductions in 2016]]
[[Category:Discontinued Microsoft software]]
[[Category:Internet manipulation and propaganda]]
[[Category:Online obscenity controversies]]
[[Category:Twitter accounts]]
[[Category:Twitter controversies]]