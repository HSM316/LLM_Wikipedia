{{short description|Field associated with machine learning and transfer learning}}
{{technical|date=February 2015}}

[[File:Transfer learning and domain adaptation.png|thumb|Distinction between usual machine learning setting and [[transfer learning]], and positioning of domain adaptation]]

'''Domain adaptation'''<ref>{{cite book |last1=Redko |first1=Ievgen |last2=Morvant |first2=Emilie |last3=Habrard |first3=Amaury |last4=Sebban |first4=Marc |last5=Bennani |first5=Younès |title=Advances in Domain Adaptation Theory |date=2019 |publisher=ISTE Press - Elsevier |isbn=9781785482366 |pages=187 |url=https://www.elsevier.com/books/advances-in-domain-adaptation-theory/redko/978-1-78548-236-6}}</ref><ref>{{cite book|last1=Bridle|first1=John S.|last2=Cox|first2=Stephen J|chapter=RecNorm: Simultaneous normalisation and classification applied to speech recognition|title=Conference on Neural Information Processing Systems (NIPS)|date=1990|pages=234–240|chapter-url=http://papers.nips.cc/paper/328-recnorm-simultaneous-normalisation-and-classification-applied-to-speech-recognition.pdf}}</ref><ref>{{cite journal|last1=Ben-David|first1=Shai|last2=Blitzer|first2=John|last3=Crammer|first3=Koby|last4=Kulesza|first4=Alex|last5=Pereira|first5=Fernando|last6=Wortman Vaughan|first6=Jennifer|title=A theory of learning from different domains|journal=Machine Learning|date=2010|volume=79|issue=1–2|pages=151–175|url=https://link.springer.com/content/pdf/10.1007/s10994-009-5152-4.pdf|doi=10.1007/s10994-009-5152-4|doi-access=free}}</ref> is a field associated with [[machine learning]] and [[inductive transfer|transfer learning]]. This scenario arises when we aim at learning a model from a source data distribution and applying that model on a different (but related) target data distribution. For instance, one of the tasks of the common [[Anti-spam techniques|spam filtering problem]] consists in adapting a model from one user (the source distribution) to a new user who receives significantly different emails (the target distribution). Domain adaptation has also been shown to be beneficial to learning unrelated sources.<ref name=":bmdl">{{Cite arXiv |eprint = 1810.09433|last1 = Hajiramezanali|first1 = Ehsan|title = Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data|author2 = Siamak Zamani Dadaneh|last3 = Karbalayghareh|first3 = Alireza|last4 = Zhou|first4 = Mingyuan|last5 = Qian|first5 = Xiaoning|class = stat.ML|year = 2018}}</ref>
When more than one source distribution is available, the problem is referred to as multi-source domain adaptation.<ref>{{cite journal|last1=Crammer|first1=Koby|last2=Kearns|first2=Michael|last3=Wortman|first3=Jeniifer|title=Learning from Multiple Sources|journal=Journal of Machine Learning Research|date=2008|volume=9|pages=1757–1774|url=http://www.jmlr.org/papers/volume9/crammer08a/crammer08a.pdf}}</ref>

== Overview ==
Domain adaptation is the ability to apply an algorithm trained in one or more "source domains" to a different (but related) "target domain". Domain adaptation is a subcategory of transfer learning. In domain adaptation, the source and target domains all have the same [[feature space]] (but different distributions); in contrast, transfer learning includes cases where the target domain's feature space is different from the source feature space or spaces.<ref name="survey multi-source">{{cite journal |last1=Sun |first1=Shiliang |last2=Shi |first2=Honglei |last3=Wu |first3=Yuanbin |title=A survey of multi-source domain adaptation |journal=Information Fusion |date=July 2015 |volume=24 |pages=84–92 |doi=10.1016/j.inffus.2014.12.003|s2cid=18385140 }}</ref>

=== Domain shift ===
A '''domain shift''',<ref name=frustratingly>Sun, Baochen, Jiashi Feng, and Kate Saenko. "Return of frustratingly easy domain adaptation." In Thirtieth AAAI Conference on Artificial Intelligence. 2016.</ref> or '''distributional shift''',<ref>Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. "Concrete problems in AI safety." arXiv preprint arXiv:1606.06565 (2016).</ref> is a change in the data distribution between an algorithm's training dataset, and a dataset it encounters when deployed. These domain shifts are common in practical applications of artificial intelligence. Conventional machine-learning algorithms often adapt poorly to domain shifts. The modern machine-learning community has many different strategies to attempt to gain better domain adaptation.<ref name=frustratingly/>

=== Examples ===
* An algorithm trained on newswires might have to adapt to a new dataset of biomedical documents.<ref>Daumé III, Hal. "Frustratingly easy domain adaptation." arXiv preprint arXiv:0907.1815 (2009).</ref>
* A spam filter, trained on a certain group of email users during training, must adapt to a new target user when deployed.<ref>Ben-David, Shai, John Blitzer, Koby Crammer, and Fernando Pereira. "Analysis of representations for domain adaptation." In Advances in neural information processing systems, pp. 137-144. 2007.</ref>
* Applying AI diagnostic algorithms, trained on labeled data associated with previous diseases, to new unlabeled data associated with the [[COVID-19 pandemic]].<ref>{{cite journal |last1=Hu |first1=Yipeng |last2=Jacob |first2=Joseph |last3=Parker |first3=Geoffrey J. M. |last4=Hawkes |first4=David J. |last5=Hurst |first5=John R. |last6=Stoyanov |first6=Danail |title=The challenges of deploying artificial intelligence models in a rapidly evolving pandemic |journal=Nature Machine Intelligence |date=June 2020 |volume=2 |issue=6 |pages=298–300 |doi=10.1038/s42256-020-0185-2|arxiv=2005.12137  |language=en |issn=2522-5839|doi-access=free }}</ref>
* A sudden societal change, such as a pandemic outbreak, can constitute domain shift and cause machine learning algorithms trained on now-obsolete consumer data to fail and require intervention.<ref>{{cite news |last1=Matthews |first1=Dylan |title=AI disaster won't look like the Terminator. It'll be creepier. |url=https://www.vox.com/future-perfect/2019/3/26/18281297/ai-artificial-intelligence-safety-disaster-scenarios |access-date=21 June 2020 |work=Vox |date=26 March 2019 |language=en}}</ref><ref>{{cite news |title=Our weird behavior during the pandemic is messing with AI models |url=https://www.technologyreview.com/2020/05/11/1001563/covid-pandemic-broken-ai-machine-learning-amazon-retail-fraud-humans-in-the-loop/ |access-date=21 June 2020 |work=MIT Technology Review |date=11 May 2020 |language=en}}</ref>

Other applications include Wi-Fi localization detection and many aspects of [[computer vision]].<ref name="survey multi-source"/>

== Formalization ==
Let <math>X</math> be the input space (or description space) and let <math>Y</math> be the output space (or label space). The objective of a machine learning algorithm is to learn a mathematical model (a hypothesis) <math>h:X\to Y</math> able to attach a label from <math>Y</math> to an example from <math>X</math>. This model is learned from a learning sample <math>S=\{(x_i,y_i) \in (X \times Y)\}_{i=1}^m</math>.

Usually in [[supervised learning]] (without domain adaptation), we suppose that the examples <math>(x_i,y_i)\in S</math> are drawn i.i.d. from a distribution <math>D_S</math> of support <math>X\times Y</math> (unknown and fixed). The objective is then to learn <math>h</math> (from <math>S</math>) such that it commits the least error possible for labelling new examples coming from the distribution <math>D_S</math>.

The main difference between supervised learning and domain adaptation is that in the latter situation we study two different (but related) distributions <math>D_S</math> and <math>D_T</math> on <math>X\times Y</math>{{cn|date=February 2020}}. The domain adaptation task then consists of the transfer of knowledge from the source domain <math>D_S</math> to the target one <math>D_T</math>. The goal is then to learn <math>h</math> (from labeled or unlabelled samples coming from the two domains) such that it commits as little error as possible on the target domain <math>D_T</math>{{cn|date=February 2020}}.

The major issue is the following: if a model is learned from a source domain, what is its capacity to correctly label data coming from the target domain?

== The different types of domain adaptation == 
There are several contexts of domain adaptation. They differ in the information considered for the target task.
# The '''unsupervised domain adaptation''': the learning sample contains a set of labeled source examples, a set of unlabeled source examples and a set of unlabeled target examples.
# The '''semi-supervised domain adaptation''':  in this situation, we also consider a "small" set of labeled target examples. 
# The '''supervised domain adaptation''': all the examples considered are supposed to be labeled.

== Four algorithmic principles ==

=== Reweighting algorithms ===
The objective is to reweight the source labeled sample such that it "looks like" the target sample (in terms of the error measure considered).<ref>{{cite book|last1=Huang|first1=Jiayuan|last2=Smola|first2=Alexander J.|last3=Gretton|first3=Arthur|last4=Borgwardt|first4=Karster M.|last5=Schölkopf|first5=Bernhard|chapter=Correcting Sample Selection Bias by Unlabeled Data|title=Conference on Neural Information Processing Systems (NIPS)|date=2006|pages=601–608|chapter-url=http://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data.pdf}}</ref><ref>{{cite journal|last1=Shimodaira|first1=Hidetoshi|title=Improving predictive inference under covariate shift by weighting the log-likelihood function|journal=Journal of Statistical Planning and Inference|date=2000|volume=90|issue=2|pages=227–244|doi=10.1016/S0378-3758(00)00115-4|s2cid=9238949 |url=https://www.researchgate.net/publication/230710850}}</ref>

=== Iterative algorithms ===
A method for adapting consists in iteratively "auto-labeling" the target examples.<ref>{{cite journal |last1=Gallego |first1=A.J. |last2=Calvo-Zaragoza |first2=J. |last3=Fisher |first3=R.B. |date=2020 |title=Incremental Unsupervised Domain-Adversarial Training of Neural Networks |journal=IEEE Transactions on Neural Networks and Learning Systems |volume=PP |issue=11 |pages=4864–4878 |doi=10.1109/TNNLS.2020.3025954 |pmid=33027004 |url=https://www.pure.ed.ac.uk/ws/files/172035660/Incremental_Unsupervised_GALLEGO_DOA18092020_AFV.pdf |hdl=20.500.11820/72ba0443-8a7d-4cdd-8212-38682d4f0730 |s2cid=210164756 |hdl-access=free }}</ref> The principle is simple:
# a model <math>h</math> is learned from the labeled examples;
# <math>h</math> automatically labels some target examples;
# a new model is learned from the new labeled examples. 
Note that there exist other iterative approaches, but they usually need target labeled examples.<ref>{{Cite conference|last1=Arief-Ang|first1=I.B.|last2=Salim|first2=F.D.|last3=Hamilton|first3=M. |date=2017-11-08|title=DA-HOC: semi-supervised domain adaptation for room occupancy prediction using CO2 sensor data|url=https://dl.acm.org/citation.cfm?id=3137146|conference=4th ACM International Conference on Systems for Energy-Efficient Built Environments (BuildSys)|pages=1–10|doi=10.1145/3137133.3137146|location=Delft, Netherlands|isbn=978-1-4503-5544-5}}</ref><ref>{{cite journal |last1=Arief-Ang |first1=I.B. |last2=Hamilton |first2=M. |last3=Salim |first3=F.D. |date=2018-12-01 |title=A Scalable Room Occupancy Prediction with Transferable Time Series Decomposition of CO2 Sensor Data |journal=ACM Transactions on Sensor Networks |volume=14 |issue=3–4 |pages=21:1–21:28 |doi=10.1145/3217214 |s2cid=54066723 }}</ref>

=== Search of a common representation space ===
The goal is to find or construct a common representation space for the two domains. The objective is to obtain a space in which the domains are close to each other while keeping good performances on the source labeling task.
This can be achieved through the use of [[Adversarial machine learning]] techniques where feature representations from samples in different domains are encouraged to be indistinguishable.<ref name="Domain-Adversarial Training">{{cite journal | last1 = Ganin | first1 = Yaroslav | last2 = Ustinova | first2 = Evgeniya | last3 = Ajakan | first3 = Hana | last4 = Germain | first4 = Pascal | last5 = Larochelle | first5 = Hugo | last6 = Laviolette | first6 = François | last7 = Marchand | first7 = Mario | last8 = Lempitsky | first8 = Victor | year = 2016 | title = Domain-Adversarial Training of Neural Networks | url = http://jmlr.org/papers/volume17/15-239/15-239.pdf | journal = Journal of Machine Learning Research | volume = 17 | pages = 1–35 }}</ref><ref name="ADA">{{Cite arXiv |eprint = 1703.01461|last1 = Hajiramezanali|first1 = Ehsan|title = Addressing Appearance Change in Outdoor Robotics with Adversarial Domain Adaptation|author2 = Siamak Zamani Dadaneh|last3 = Karbalayghareh|first3 = Alireza|last4 = Zhou|first4 = Mingyuan|last5 = Qian|first5 = Xiaoning|class = cs.RO|year = 2017}}</ref>

=== Hierarchical Bayesian Model ===
The goal is to construct a [[Bayesian hierarchical model]] <math>p(n)</math>, which is essentially a factorization model for counts <math>n</math>, to derive domain-dependent latent representations allowing both domain-specific and globally shared latent factors.<ref name=":bmdl"/>

== Softwares ==

Several compilations of domain adaptation and transfer learning algorithms have been implemented over the past decades:
* ADAPT<ref>de Mathelin, Antoine and Deheeger, François and Richard, Guillaume and Mougeot, Mathilde and Vayatis, Nicolas (2020) [https://github.com/adapt-python/adapt "ADAPT: Awesome Domain Adaptation Python Toolbox"]</ref> (Python)
* TLlib <ref>Mingsheng Long Junguang Jiang, Bo Fu. (2020) [https://github.com/thuml/Transfer-Learning-Library "Transfer-learning-library"]</ref> (Python)
* Domain-Adaptation-Toolbox <ref>Ke Yan. (2016) [https://github.com/viggin/domain-adaptation-toolbox "Domain adaptation toolbox"]</ref> (MATLAB)

== References ==
{{Reflist}}

[[Category:Machine learning]]