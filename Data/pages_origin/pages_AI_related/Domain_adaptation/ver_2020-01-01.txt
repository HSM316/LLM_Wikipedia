{{short description|Field associated with machine learning and transfer learning}}
{{technical|date=February 2015}}

[[File:Transfer learning and domain adaptation.png|thumb|Distinction between usual machine learning setting and transfer learning, and positioning of domain adaptation.]]

'''Domain adaptation'''<ref>{{cite book |last1=Redko |first1=Ievgen |last2=Morvant |first2=Emilie |last3=Habrard |first3=Amaury |last4=Sebban |first4=Marc |last5=Bennani |first5=Younès |title=Advances in Domain Adaptation Theory |date=2019 |publisher=ISTE Press - Elsevier |isbn=9781785482366 |pages=187 |url=https://www.elsevier.com/books/advances-in-domain-adaptation-theory/redko/978-1-78548-236-6}}</ref><ref>{{cite book|last1=Bridle|first1=John S.|last2=Cox|first2=Stephen J|chapter=RecNorm: Simultaneous normalisation and classification applied to speech recognition|title=Conference on Neural Information Processing Systems (NIPS)|date=1990|pages=234–240|chapter-url=http://papers.nips.cc/paper/328-recnorm-simultaneous-normalisation-and-classification-applied-to-speech-recognition.pdf}}</ref><ref>{{cite journal|last1=Ben-David|first1=Shai|last2=Blitzer|first2=John|last3=Crammer|first3=Koby|last4=Kulesza|first4=Alex|last5=Pereira|first5=Fernando|last6=Wortman Vaughan|first6=Jennifer|title=A theory of learning from different domains|journal=Machine Learning|date=2010|volume=79|issue=1–2|pages=151–175|url=https://link.springer.com/content/pdf/10.1007/s10994-009-5152-4.pdf|doi=10.1007/s10994-009-5152-4}}</ref> is a field associated with [[machine learning]] and [[inductive transfer|transfer learning]]. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common [[Anti-spam techniques|spam filtering problem]] consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution). Domain adaptation has also been shown to be beneficial for learning unrelated sources.<ref name=":bmdl">{{Cite arXiv |eprint = 1810.09433|last1 = Hajiramezanali|first1 = Ehsan|title = Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data|author2 = Siamak Zamani Dadaneh|last3 = Karbalayghareh|first3 = Alireza|last4 = Zhou|first4 = Mingyuan|last5 = Qian|first5 = Xiaoning|class = stat.ML|year = 2018}}</ref>
Note that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation.<ref>{{cite journal|last1=Crammer|first1=Koby|last2=Kearns|first2=Michael|last3=Wortman|first3=Jeniifer|title=Learning from Multiple Sources|journal=Journal of Machine Learning Research|date=2008|volume=9|pages=1757–1774|url=http://www.jmlr.org/papers/volume9/crammer08a/crammer08a.pdf}}</ref>

== Formalization ==
Let <math>X</math> be the input space (or description space) and let <math>Y</math> be the output space (or label space). The objective of a machine learning algorithm is to learn a mathematical model (a hypothesis) <math>h:X\to Y</math> able to attach a label from <math>Y</math> to an example from <math>X</math>. This model is learned from a learning sample <math>S=\{(x_i,y_i) \in (X \times Y)\}_{i=1}^m</math>.

Usually in [[supervised learning]] (without domain adaptation), we suppose that the examples <math>(x_i,y_i)\in S</math> are drawn i.i.d. from a distribution <math>D_S</math> of support <math>X\times Y</math> (unknown and fixed). The objective is then to learn <math>h</math> (from <math>S</math>) such that it commits the least error possible for labelling new examples coming from the distribution <math>D_S</math>.

The main difference between supervised learning and domain adaptation is that in the latter situation we study two different (but related) distributions <math>D_S</math> and <math>D_T</math> on <math>X\times Y</math>. The domain adaptation task then consists of the transfer of knowledge from the source domain <math>D_S</math> to the target one <math>D_T</math>. The goal is then to learn <math>h</math> (from labeled or unlabelled samples coming from the two domains) such that it commits as little error as possible on the target domain <math>D_T</math>.

The major issue is the following: if a model is learned from a source domain, what is its capacity to correctly label data coming from the target domain?

== The different types of domain adaptation == 
There are several contexts of domain adaptation. They differ in the information considered for the target task.
# The '''unsupervised domain adaptation''': the learning sample contains a set of labeled source examples, a set of unlabeled source examples and a set of unlabeled target examples.
# The '''semi-supervised domain adaptation''':  in this situation, we also consider a "small" set of labeled target examples. 
# The '''supervised domain adaptation''': all the examples considered are supposed to be labeled.

== Four algorithmic principles ==

=== Reweighting algorithms ===
The objective is to reweight the source labeled sample such that it "looks like" the target sample (in term of the error measure considered).<ref>{{cite book|last1=Huang|first1=Jiayuan|last2=Smola|first2=Alexander J.|last3=Gretton|first3=Arthur|last4=Borgwardt|first4=Karster M.|last5=Schölkopf|first5=Bernhard|chapter=Correcting Sample Selection Bias by Unlabeled Data|title=Conference on Neural Information Processing Systems (NIPS)|date=2006|pages=601–608|chapter-url=http://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data.pdf}}</ref><ref>{{cite journal|last1=Shimodaira|first1=Hidetoshi|title=Improving predictive inference under covariate shift by weighting the log-likelihood function|journal=Journal of Statistical Planning and Inference|date=2000|pages=227–244|url=https://www.researchgate.net/publication/230710850}}</ref>

=== Iterative algorithms ===
A method for adapting consists in iteratively "auto-labeling" the target examples. The principle is simple:
# a model <math>h</math> is learned from the labeled examples;
# <math>h</math> automatically labels some target examples;
# a new model is learned from the new labeled examples. 
Note that there exist other iterative approaches, but they usually need target labeled examples.<ref>{{Cite conference|last=Arief-Ang|first=I.B.|last2=Salim|first2=F.D.|last3=Hamilton|first3=M. |date=2017-11-08|title=DA-HOC: semi-supervised domain adaptation for room occupancy prediction using CO2 sensor data|url=https://dl.acm.org/citation.cfm?id=3137146|conference=4th ACM International Conference on Systems for Energy-Efficient Built Environments (BuildSys)|pages=1–10|doi=10.1145/3137133.3137146|location=Delft, Netherlands|isbn=978-1-4503-5544-5}}</ref><ref>{{cite journal |last1=Arief-Ang |first1=I.B. |last2=Hamilton |first2=M. |last3=Salim |first3=F.D. |date=2018-12-01 |title=A Scalable Room Occupancy Prediction with Transferable Time Series Decomposition of CO2 Sensor Data |journal=ACM Transactions on Sensor Networks |volume=14 |issue=3–4 |pages=21:1–21:28 |doi=10.1145/3217214 }}</ref>

=== Search of a common representation space ===
The goal is to find or construct a common representation space for the two domains. The objective is to obtain a space in which the domains are close to each other while keeping good performances on the source labeling task.
This can be achieved through the use of [[Adversarial machine learning]] techniques where feature representations from samples in different domains are encouraged to be indistinguishable.<ref name="Domain-Adversarial Training">Ganin, Yaroslav; Ustinova, Evgeniya; Ajakan, Hana; Germain, Pascal; Larochelle, Hugo; Laviolette, François; Marchand, Mario; Lempitsky, Victor (2016). [http://jmlr.org/papers/volume17/15-239/15-239.pdf "Domain-Adversarial Training of Neural Networks"]. Journal of Machine Learning Research, 17:1–35.</ref><ref name="ADA">{{Cite arXiv |eprint = 1703.01461|last1 = Hajiramezanali|first1 = Ehsan|title = Addressing Appearance Change in Outdoor Robotics with Adversarial Domain Adaptation|author2 = Siamak Zamani Dadaneh|last3 = Karbalayghareh|first3 = Alireza|last4 = Zhou|first4 = Mingyuan|last5 = Qian|first5 = Xiaoning|class = cs.RO|year = 2017}}</ref>

=== Hierarchical Bayesian Model ===
The goal is to construct a Bayesian hierarchical model <math>p(n)</math>, which is essentially a factorization model for counts <math>n</math>, to derive domain-dependent latent representations allowing both domain-specific and globally shared latent factors.<ref name=":bmdl"/>

== References ==
{{Reflist}}

[[Category:Machine learning]]