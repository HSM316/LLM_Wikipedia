'''Federated learning''' (also known as '''collaborative learning''') is a [[machine learning]] technique that trains an algorithm across multiple decentralized edge devices or [[Server (computing)|servers]] holding local [[Data|data samples]], without exchanging them. This approach stands in contrast to traditional centralized machine learning techniques where all the local datasets are uploaded to one [[Server (computing)|server]], as well as to more classical decentralized approaches which often assume that local data samples are [[Independent and identically distributed random variables|identically distributed]].

Federated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus allowing to address critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, [[Internet of things|IoT]], and pharmaceutics.

== Definition ==
Federated learning aims at training a machine learning algorithm, for instance deep [[neural network]]s, on multiple local datasets contained in local nodes without explicitly exchanging data samples. The general principle consists in training local models on local data samples and exchanging [[Parameters (computer science)|parameters]] (e.g. the weights and biases of a deep neural network) between these local nodes at some frequency to generate a global model shared by all nodes. 

The main difference between federated learning and distributed learning lies in the assumptions made on the properties of the local datasets,<ref name=":1">{{Cite arXiv|eprint = 1511.03575|last1 = Konečný|first1 = Jakub|last2 = McMahan|first2 = Brendan|last3 = Ramage|first3 = Daniel|title = Federated Optimization: Distributed Optimization Beyond the Datacenter|year = 2015|class = cs.LG}}</ref> as [[distributed learning]] originally aims at [[Computing power|parallelizing computing power]] where federated learning originally aims at training on [[Homogeneity and heterogeneity|heterogeneous datasets]]. While distributed learning also aims at training a single model on multiple servers, a common underlying assumption is that the local datasets are identically distributed (i.i.d.) and roughly have the same size. None of these hypotheses are made for federated learning; instead, the datasets are typically heterogeneous and their sizes may span several orders of magnitude. Moreover, the clients involved in federated learning may be unreliable as they are subject to more failures or drop out since they commonly rely on less powerful communication media (i.e. [[Wi-fi]]) and battery-powered systems (i.e. [[smartphone|smartphones]] and IoT devices) compared to distributed learning where nodes are typically [[data center|datacenters]] that have powerful computational capabilities and are connected to one another with fast networks.<ref name="Survey-2019">{{cite arXiv |title=Advances and Open Problems in Federated Learning |date=10 December 2019 |eprint=1912.04977 |last1=Kairouz |first1=Peter |last2=Brendan McMahan |first2=H. |last3=Avent |first3=Brendan |last4=Bellet |first4=Aurélien |last5=Bennis |first5=Mehdi |author6=Arjun Nitin Bhagoji |last7=Bonawitz |first7=Keith |last8=Charles |first8=Zachary |last9=Cormode |first9=Graham |last10=Cummings |first10=Rachel |last11=D'Oliveira |first11=Rafael G. L. |author12=Salim El Rouayheb |last13=Evans |first13=David |last14=Gardner |first14=Josh |last15=Garrett |first15=Zachary |last16=Gascón |first16=Adrià |last17=Ghazi |first17=Badih |last18=Gibbons |first18=Phillip B. |last19=Gruteser |first19=Marco |last20=Harchaoui |first20=Zaid |last21=He |first21=Chaoyang |last22=He |first22=Lie |last23=Huo |first23=Zhouyuan |last24=Hutchinson |first24=Ben |last25=Hsu |first25=Justin |last26=Jaggi |first26=Martin |last27=Javidi |first27=Tara |last28=Joshi |first28=Gauri |last29=Khodak |first29=Mikhail |last30=Konečný |first30=Jakub |class=cs.LG |display-authors=29 }}</ref> 

=== Centralized federated learning ===
In the centralized federated learning setting, a central server is used to orchestrate the different steps of the algorithms and coordinate all the participating nodes during the learning process. The server is responsible for the nodes selection at the beginning of the training process and for the aggregation of the received model updates. Since all the selected nodes have to send updates to a single entity, the server may become a bottleneck of the system.<ref name="Survey-2019"></ref>

=== Decentralized federated learning ===
In the decentralized federated learning setting, the nodes are able to coordinate themselves to obtain the global model. This setup prevents single point failures as the model updates are exchanged only between interconnected nodes without the orchestration of the central server. Nevertheless, the specific [[network topology]] may affect the performances of the learning process.<ref name="Survey-2019"></ref> See blockchain-based federated learning<ref> {{cite journal|title=Federated Learning with Blockchain for Autonomous Vehicles: Analysis and Design Challenges |journal=IEEE Transactions on Communications |year=2020|doi=10.1109/TCOMM.2020.2990686|last1=Pokhrel |first1=Shiva Raj |last2=Choi |first2=Jinho |volume=68 |issue=8 |pages=4734–4746 |s2cid=219006840 }}</ref> and the references therein.

=== Heterogeneous federated learning ===
An increasing number of application domains involve a large set of heterogeneous clients, e.g., mobile phones and IoT devices. Most of the existing Federated learning strategies assume that local models share the same global model architecture. Recently, a new federated learning framework named HeteroFL was developed to address heterogeneous clients equipped with very different computation and communication capabilities.<ref name=":2">{{Cite journal|last=Diao|first=Enmao|last2=Ding|first2=Jie|last3=Tarokh|first3=Vahid|date=2020-10-02|title=HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients|url=http://arxiv.org/abs/2010.01264|journal=arXiv:2010.01264 [cs, stat]}}</ref> The HeteroFL technique can enable the training of heterogeneous local models with dynamically-varying computation complexities while still producing a single global inference model.<ref name=":2" /> 

[[File:Federated learning process central case.png|alt=Federated learning general process in central orchestrator setup|thumb|upright=2|Federated learning general process in central orchestrator setup]]

== Main features ==

=== Iterative learning ===
To ensure good task performance of a final, central machine learning model, federated learning relies on an iterative process broken up into an atomic set of client-server interactions known as a federated learning round. Each round of this process consists in transmitting the current global model state to participating nodes, training local models on these local nodes to produce a set of potential model updates at each node, and then aggregating and processing these local updates into a single global update and applying it to the global model.<ref name="Survey-2019"></ref>

In the methodology below, a central server is used for aggregation, while local nodes perform local training depending on the central server's orders. However, other strategies lead to the same results without central servers, in a [[peer-to-peer]] approach, using [[Gossip protocol|gossip]]<ref>Decentralized Collaborative Learning of Personalized Models over Networks Paul Vanhaesebrouck, Aurélien Bellet, Marc Tommasi, 2017</ref> or [[Consensus (computer science)|consensus]] methodologies.<ref>{{cite journal |last1=Savazzi |first1=Stefano |last2=Nicoli |first2=Monica |last3=Rampa |first3=Vittorio |title=Federated Learning With Cooperating Devices: A Consensus Approach for Massive IoT Networks |journal=IEEE Internet of Things Journal |date=May 2020 |volume=7 |issue=5 |pages=4641–4654 |doi=10.1109/JIOT.2020.2964162|arxiv=1912.13163 |s2cid=209515403 }}</ref>

Assuming a federated round composed by one iteration of the learning process, the learning procedure can be summarized as follows:<ref name=":0">Towards federated learning at scale: system design, Keith Bonawitz Hubert Eichner and al., 2019</ref>
# '''Initialization''': according to the server inputs, a machine learning model (e.g., [[linear regression]], neural network, [[Boosting (machine learning)|boosting]]) is chosen to be trained on local nodes and initialized. Then, nodes are activated and wait for the central server to give the calculation tasks.
# '''Client selection''': a fraction of local nodes is selected to start training on local data. The selected nodes acquire the current statistical model while the others wait for the next federated round.
# '''Configuration''': the central server orders selected nodes to undergo training of the model on their local data in a pre-specified fashion (e.g., for some mini-batch updates of [[gradient descent]]).
# '''Reporting''': each selected node sends its local model to the server for aggregation. The central server aggregates the received models and sends back the model updates to the nodes. It also handles failures for disconnected nodes or lost model updates. The next federated round is started returning to the client selection phase.
# '''Termination''': once a pre-defined termination criterion is met (e.g., a maximum number of iterations is reached or the model accuracy is greater than a threshold) the central server aggregates the updates and finalizes the global model.

The procedure considered before assumes synchronized model updates. Recent federated learning developments introduced novel techniques to tackle asynchronicity during the training process, or training with dynamically varying models.<ref name=":2" /> Compared to synchronous approaches where local models are exchanged once the computations have been performed for all layers of the neural network, asynchronous ones leverage the properties of neural networks to exchange model updates as soon as the computations of a certain layer are available. These techniques are also commonly referred to as split learning<ref name="Gupta-2018">{{cite arXiv |last1=Gupta |first1=Otkrist |last2=Raskar |first2=Ramesh |title=Distributed learning of deep neural network over multiple agents  |date=14 October 2018 |class=cs.LG |eprint=1810.06060 }}</ref><ref name="Vepakomma-2018">{{cite arXiv |last1=Vepakomma |first1=Praneeth |last2=Gupta |first2=Otkrist |last3=Swedish |first3=Tristan |last4=Raskar |first4=Ramesh |title=Split learning for health: Distributed deep learning without sharing raw patient data |date=3 December 2018 |class=cs.LG |eprint=1812.00564 }}</ref> and they can be applied both at training and inference time regardless of centralized or decentralized federated learning settings.<ref name="Survey-2019"></ref><ref name=":2" />

=== Non-iid data ===
In most cases, the assumption of independent and identically distributed samples across local nodes does not hold for federated learning setups. Under this setting, the performances of the training process may vary significantly according to the unbalancedness of local data samples as well as the particular probability distribution of the training examples (i.e., [[Feature (machine learning)|features]] and [[Labeled data|labels]]) stored at the local nodes. To further investigate the effects of non-iid data, the following description considers the main categories presented in the by Peter Kiarouz and al. in 2019.<ref name="Survey-2019"></ref>

The description of non-iid data relies on the analysis of the [[Joint probability distribution|joint probability]] between features and labels for each node.
This allows to decouple each contribution according to the specific distribution available at the local nodes.
The main categories for non-iid data can be summarized as follows:<ref name ="Survey-2019"></ref>

* '''Covariate shift''': local nodes may store examples that have different statistical distributions compared to other nodes. An example occurs in [[natural language processing]] datasets where people typically write the same digits/letters with different stroke widths or slants.<ref name ="Survey-2019"></ref>
* '''Prior probability shift''': local nodes may store labels that have different statistical distributions compared to other nodes. This can happen if datasets are regional and/or demographically partitioned. For example, datasets containing images of animals vary significantly from country to country.<ref name ="Survey-2019"></ref>
* '''Concept shift''' (''same label, different features''): local nodes may share the same labels but some of them correspond to different features at different local nodes. For example, images that depict a particular object can vary according to the weather condition in which they were captured.<ref name ="Survey-2019"></ref>
* '''Concept shift''' (''same features, different labels''): local nodes may share the same features but some of them correspond to different labels at different local nodes. For example, in natural language processing, the sentiment analysis may yield different sentiments even if the same text is observed.<ref name ="Survey-2019"></ref>
* '''Unbalancedness''': the data available at the local nodes may vary significantly in size.<ref name ="Survey-2019"></ref><ref name=":2" />

Other non-iid data descriptors take into account the dynamic variation of the network topology,<ref name="SGD-2019">{{cite arXiv |last1=Eichner |first1=Hubert |last2=Koren |first2=Tomer |last3=McMahan |first3=H. Brendan |last4=Srebro |first4=Nathan |last5=Talwar |first5=Kunal |title=Semi-Cyclic Stochastic Gradient Descent |date=22 April 2019 |class=cs.LG |eprint=1904.10120 }}</ref> due to failures or ineligibility of local nodes during the federated learning process, or dataset shifts, where the nodes participating in the training phase for learning the global model may not be eligible during inference due to insufficient computational capabilities. This results in a difference between the statistics of training and testing data samples.<ref name="Survey-2019"></ref>

== Algorithmic hyper-parameters ==

=== Network topology ===
The way the statistical local outputs are pooled and the way the nodes communicate with each other can change from the centralized model explained in the previous section. This leads to a variety of federated learning approaches: for instance no central orchestrating server, or stochastic communication.<ref>''Collaborative Deep Learning in Fixed Topology Networks,'' Zhanhong Jiang, Aditya Balu, Chinmay Hegde, Soumik Sarkar, 2017</ref>

In particular, orchestrator-less distributed networks are one important variation. In this case, there is no central server dispatching queries to local nodes and aggregating local models. Each local node sends its outputs to several randomly-selected others, which aggregate their results locally. This restrains the number of transactions, thereby sometimes reducing training time and computing cost.<ref name="Gossip-2018">GossipGraD: Scalable Deep Learning using Gossip Communication based Asynchronous Gradient Descent, Jeff Daily, Abhinav Vishnu, Charles Siegel, Thomas Warfel, Vinay Amatya, 2018</ref>

=== Federated learning parameters ===
Once the topology of the node network is chosen, one can control different parameters of the federated learning process (in opposition to the machine learning model's own hyperparameters) to optimize learning:

* Number of federated learning rounds: <math>T</math>
* Total number of nodes used in the process: <math>K</math>
* Fraction of nodes used at each iteration for each node: <math>C</math>
* Local [[Batch normalization|batch size]] used at each learning iteration: <math>B</math>

Other model-dependent parameters can also be tinkered with, such as:

* Number of iterations for local training before pooling: <math>N</math>
* Local learning rate: <math>\eta</math>

Those parameters have to be optimized depending on the constraints of the machine learning application (e.g., available computing power, available memory, [[Bandwidth (computing)|bandwidth]]). For instance, stochastically choosing a limited fraction <math>C</math> of nodes for each iteration diminishes computing cost and may prevent [[overfitting]], in the same way that stochastic gradient descent can reduce overfitting.

== Federated learning variations ==
In this section, the exposition of the paper published by H. Brendan McMahan and al. in 2017 is followed.<ref name="ReferenceA">Communication-Efficient Learning of Deep Networks from Decentralized Data, H. Brendan McMahan and al. 2017</ref> 

To describe the federated strategies, let us introduce some notations:

* <math>K</math> : total number of clients;
* <math>k</math> : index of clients;
* <math>n_k</math>: number of data samples available during training for client <math>k</math>;
* <math>k_t</math>: model's weight vector on client <math>k</math>, at the federated round <math>t</math>;
* <math>l(w, b)</math> : loss function for weights <math>w</math> and batch <math>b</math>;
* <math>E</math> : number of local epochs;

=== Federated Stochastic Gradient Descent (FedSGD) ===
[[Deep learning]] training mainly relies on variants of [[stochastic gradient descent]], where gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent.

Federated stochastic gradient descent<ref name="ReferencePPDL">Privacy Preserving Deep Learning, R. Shokri and V. Shmatikov, 2015</ref> is the direct transposition of this algorithm to the federated setting, but by using a random fraction <math>C</math> of the nodes and using all the data on this node. The gradients are averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.

=== Federated averaging ===
Federated averaging (FedAvg) is a generalization of FedSGD, which allows local nodes to perform more than one batch update on local data and exchanges the updated weights rather than the gradients. The rationale behind this generalization is that in FedSGD, if all local nodes start from the same initialization, averaging the gradients is strictly equivalent to averaging the weights themselves. Further, averaging tuned weights coming from the same initialization does not necessarily hurt the resulting averaged model's performance.<ref name="ReferenceA"></ref>

== Technical limitations ==
Federated learning requires frequent communication between nodes during the learning process. Thus, it requires not only enough local computing power and memory, but also high bandwidth connections to be able to exchange parameters of the machine learning model. However, the technology also avoid data communication, which can require significant resources before starting centralized machine learning. Nevertheless, the devices typically employed in federated learning are communication-constrained, for example IoT devices or smartphones are generally connected to Wi-fi networks, thus, even if the models are commonly less expensive to be transmitted compared to raw data, federated learning mechanisms may not be suitable in their general form.<ref name="Survey-2019"></ref>   

Federated learning raises several statistical challenges:

* Heterogeneity between the different local datasets: each node may have some bias with respect to the general population, and the size of the datasets may vary significantly;<ref name=":3">{{Cite journal|last=Diao|first=Enmao|last2=Ding|first2=Jie|last3=Tarokh|first3=Vahid|date=2020-10-02|title=HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients|url=http://arxiv.org/abs/2010.01264|journal=arXiv:2010.01264 [cs, stat]}}</ref>
* Temporal heterogeneity: each local dataset's distribution may vary with time;
* [[Interoperability]] of each node's dataset is a prerequisite;
* Each node's dataset may require regular curations;
* Hiding training data might allow attackers to inject [[Backdoor (computing)|backdoors]] into the global model;<ref>How To Backdoor Federated Learning, Eugene Bagdasaryan, 2018</ref>
* Lack of access to global training data makes it harder to identify unwanted biases entering the training e.g. age, gender, sexual orientation;
* Partial or total loss of model updates due to node failures affecting the global model.<ref name = "Survey-2019"></ref>

== Properties of federated learning ==

=== Privacy by design ===
The main advantage of using federated approaches to machine learning is to ensure data [[privacy]] or data secrecy. Indeed, no local data is uploaded externally, concatenated or exchanged. Since the entire database is segmented into local bits, this makes it more difficult to hack into it.

With federated learning, only machine learning parameters are exchanged. In addition, such parameters can be [[Encryption|encrypted]] before sharing between learning rounds to extend privacy and [[homomorphic encryption]] schemes can be used to directly make computations on the encrypted data without decrypting them beforehand. Despite such protective measures, these parameters may still leak information about the underlying data samples, for instance, by making multiple specific queries on specific datasets. Querying capability of nodes thus is a major attention point, which can be addressed using differential privacy or secure aggregation.<ref>Practical Secure Aggregation for Privacy Preserving Machine Learning, Keith Bonawitz, 2018</ref>

It was found that the privacy issues of federated learning is often due to running estimates, which hinders the usage of advanced deep learning models. A Static Batch Normalization (sBN) for optimizing privacy constrained deep neural networks was developed.<ref name=":2" /> During the training phase, sBN does not track running estimates but simply normalize batch data. Only the statistics of hidden representations from local data after the model converges are calculated. This method is suitable for the FL framework as local models do not need to upload running estimates during training. Local models only upload their statistics for once after optimization, which significantly reduces data leakage risk.

=== Personalization ===
The generated model delivers insights based on the global patterns of nodes. However, if a participating node wishes to learn from global patterns but also adapt outcomes to its peculiar status, the federated learning methodology can be adapted to generate two models at once in a [[multi-task learning]] framework. In addition, [[Cluster analysis|clustering]] techniques may be applied to aggregate nodes that share some similarities after the learning process is completed. This allows the generalization of the models learned by the nodes according also to their local data.<ref name=":2" /><ref>{{cite arXiv |last1=Sattler |first1=Felix |last2=Müller |first2=Klaus-Robert |last3=Samek |first3=Wojciech |title=Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints |date=4 October 2019 |class=cs.LG |eprint=1910.01991 }}</ref>  

In the case of deep neural networks, it is possible to share some layers across the different nodes and keep some of them on each local node. Typically, first layers performing general [[pattern recognition]] are shared and trained all datasets. The last layers will remain on each local node and only be trained on the local node's dataset.<ref>{{cite arXiv |last1=Arivazhagan |first1=Manoj Ghuhan |last2=Aggarwal |first2=Vinay |last3=Singh |first3=Aaditya Kumar |last4=Choudhary |first4=Sunav |title=Federated Learning with Personalization Layers |date=2 December 2019 |class=cs.LG |eprint=1912.00818 }}</ref>

Early personalization methods often introduce additional computation and communication overhead that may not be necessary. To significantly reduce computation and communication costs in FL, a “Masking Trick” approach was developed.<ref name=":3" /> The “Masking Trick” allows local clients to adaptively contribute to the training of global models much more flexibly and efficiently compared with classical federated learning. 

=== Legal upsides of federated learning ===
Western legal frameworks emphasize more and more on data protection and data traceability. White House 2012 Report<ref name="White House-2013">{{cite journal |title=Consumer Data Privacy in a Networked World: A Framework for Protecting Privacy and Promoting Innovation in the Global Digital Economy |journal=Journal of Privacy and Confidentiality |date=1 March 2013 |doi=10.29012/jpc.v4i2.623 |last1=Anonymous |doi-access=free }}</ref> recommended the application of a data minimization principle, which is mentioned in European [[General Data Protection Regulation|GDPR]].<ref>Recital 39 of the Regulation (EU) 2016/679 (General Data Protection Regulation)</ref> In some cases, it is illegal to transfer data from a country to another (e.g., genomic data), however international consortia are sometimes necessary for scientific advances. In such cases federated learning brings solutions to train a global model while respecting security constraints.

== Current research topics ==
Federated learning has started to emerge as an important research topic in 2015<ref name=":1" /> and 2016,<ref name="Opt-2016">''Federated Optimization: Distributed Machine Learning for On-Device Intelligence,'' Jakub Konečný, H. Brendan McMahan, Daniel Ramage and Peter Richtárik, 2016</ref> with the first publications on federated averaging in telecommunication settings. Another important aspect of active research is the reduction of the communication burden during the federated learning process. In 2017 and 2018, publications have emphasized the development of resource allocation strategies, especially to reduce communication<ref name="ReferenceA"></ref> requirements<ref name="Comm-2017">{{cite arXiv|last1=Konečný |first1=Jakub |last2=McMahan |first2=H. Brendan |last3=Yu |first3=Felix X. |last4=Richtárik |first4=Peter |last5=Suresh |first5=Ananda Theertha |last6=Bacon |first6=Dave |title=Federated Learning: Strategies for Improving Communication Efficiency |date=30 October 2017 |class=cs.LG |eprint=1610.05492 }}</ref> between nodes with gossip algorithms<ref>''Gossip training for deep learning, Michael Blot and al., 2017''</ref> as well as on the characterization of the robustness to differential privacy attacks.<ref>''Differentially Private Federated Learning: A Client Level Perspective'' Robin C. Geyer and al., 2018</ref> Other research activities focus on the reduction of the bandwidth during training through sparsification and quantization methods,<ref name="Comm-2017"/> where the machine learning models are sparsified and/or compressed before they are shared with other nodes. 

Recent research advancements are starting to consider real-word propagating [[Communication channel|channels]]<ref>{{cite arXiv |last1=Amiri |first1=Mohammad Mohammadi |last2=Gunduz |first2=Deniz |title=Federated Learning over Wireless Fading Channels |date=10 February 2020 |class=cs.IT |eprint=1907.09769 }}</ref> as in previous implementations ideal channels were assumed. Another active direction of research is to develop Federated learning for training heterogeneous local models with varying computation complexities and producing a single powerful global inference model.<ref>{{Cite journal|last=Diao|first=Enmao|last2=Ding|first2=Jie|last3=Tarokh|first3=Vahid|date=2020-10-02|title=HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients|url=http://arxiv.org/abs/2010.01264|journal=arXiv:2010.01264 [cs, stat]}}</ref> 

A learning framework named Assisted learning was recently developed to improve each agent's learning capabilities without transmitting private data, models, and even learning objectives.<ref>{{Cite journal|last=Xian|first=Xun|last2=Wang|first2=Xinran|last3=Ding|first3=Jie|last4=Ghanadan|first4=Reza|date=2020|title=Assisted Learning: A Framework for Multi-Organization Learning|url=https://proceedings.neurips.cc//paper/2020/hash/a7b23e6eefbe6cf04b8e62a6f0915550-Abstract.html|journal=Advances in Neural Information Processing Systems|language=en|volume=33}}</ref> Compared with Federated learning that often requires a [[Master/slave (technology)|central controller]] to orchestrate the learning and optimization, [http://assisted-learning.org/ Assisted learning] aims to provide protocols for the agents to optimize and learn among themselves without a global model.

== Use cases ==
Federated learning typically applies when individual actors need to train models on larger datasets than their own, but cannot afford to share the data in itself with other (e.g., for legal, strategic or economic reasons). The technology yet requires good connections between local servers and minimum computational power for each node.<ref name="Survey-2019"></ref>

=== Transportation: Self-driving cars ===
[[Self-driving car]] encapsulate many machine learning technologies to function: [[computer vision]] for analyzing obstacles, [[machine learning]] for adapting their pace to the environment (e.g., bumpiness of the road). Due to the potential high number of self-driving cars and the need for them to quickly respond to real world situations, traditional cloud approach may generate safety risks. Federated learning can represent a solution for limiting volume of data transfer and accelerating learning processes.<ref>{{cite journal |title=Federated learning meets blockchain at 6G edge: a drone-assisted networking for disaster response | year= 2020 |doi=10.1145/3414045.3415949 |last1=Pokhrel |first1=Shiva Raj |page=49-54}}</ref><ref name="Elbir-2020">{{cite arXiv |last1=Elbir |first1=Ahmet M. |last2=Coleri |first2=S. |title=Federated Learning for Vehicular Networks |date=2 June 2020 |class=eess.SP |eprint=2006.01412 }}</ref>

=== Industry 4.0: smart manufacturing ===
In [[Industry 4.0]], there is a widespread adoption of machine learning techniques<ref>{{cite journal |last1=Cioffi |first1=Raffaele |last2=Travaglioni |first2=Marta |last3=Piscitelli |first3=Giuseppina |last4=Petrillo |first4=Antonella |last5=De Felice |first5=Fabio |title=Artificial Intelligence and Machine Learning Applications in Smart Production: Progress, Trends, and Directions |journal=Sustainability |date=2019 |volume=12 |issue=2 |pages=492 |doi=10.3390/su12020492 |language=en|doi-access=free }}</ref> to improve the efficiency and effectiveness of industrial process while guaranteeing a high level of safety. Nevertheless, privacy of sensible data for industries and manufacturing companies is of paramount importance. Federated learning algorithms can be applied to these problems as they do not disclose any sensitive data.<ref name="Opt-2016"></ref>

=== Medicine: Digital Health ===
Federated learning seeks to address the problem of data governance and privacy by training algorithms collaboratively without exchanging the data itself. Today’s standard approach of centralizing data from multiple centers comes at the cost of critical concerns regarding patient privacy and data protection. To solve this problem, the ability to train machine learning models at scale across multiple medical institutions without moving the data is a critical technology. Nature Digital Medicine published in September 2020 a paper The Future of Digital Health with Federated Learning<ref>{{cite journal |last1=Rieke |first1=Nicola |last2=Hancox |first2=Jonny |last3=Li |first3=Wenqi |last4=Milletarì |first4=Fausto |last5=Roth |first5=Holger R. |last6=Albarqouni |first6=Shadi |last7=Bakas |first7=Spyridon |last8=Galtier |first8=Mathieu N. |last9=Landman |first9=Bennett A. |last10=Maier-Hein |first10=Klaus |last11=Ourselin |first11=Sébastien |last12=Sheller |first12=Micah |last13=Summers |first13=Ronald M. |last14=Trask |first14=Andrew |last15=Xu |first15=Daguang |last16=Baust |first16=Maximilian |last17=Cardoso |first17=M. Jorge |title=The future of digital health with federated learning |journal=NPJ Digital Medicine |date=14 September 2020 |volume=3 |issue=1 |page=119 |doi=10.1038/s41746-020-00323-1 |pmid=33015372 |pmc=7490367 |arxiv=2003.08119 |s2cid=212747909 }}</ref> where the authors explore how federated learning may provide a solution for the future of digital health and highlight the challenges and considerations that need to be addressed.

== References ==
{{reflist}}

== External links ==
*[https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679 "Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016"] at eur-lex.europa.eu. Retrieved October 18, 2019.
*[https://ico.org.uk/about-the-ico/news-and-events/ai-blog-data-minimisation-and-privacy-preserving-techniques-in-ai-systems/ "Data minimisation and privacy-preserving techniques in AI systems"] at UK Information Commissioners Office. Retrieved July 22, 2020

[[Category:Machine learning]]
[[Category:Distributed algorithms]]
[[Category:Multi-agent systems]]