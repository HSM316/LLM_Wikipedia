In the fields of [[machine learning]], the [[theory of computation]], and [[random matrix theory]], a probability distribution over vectors is said to be in '''isotropic position''' if its [[covariance matrix]] is equal to the [[identity matrix]]. 

== Formal definitions ==
Let <math display="inline">D</math> be a distribution over vectors in the vector space <math display="inline">\mathbb{R}^n</math>.
Then <math display="inline">D</math> is in isotropic position if, for vector <math display="inline">v</math> sampled from the distribution,
:<math>\mathbb{E}\, vv^T = \mathrm{Id}.</math>

A ''set'' of vectors is said to be in isotropic position if the [[uniform distribution (continuous)|uniform distribution]] over that set is in isotropic position. In particular, every [[orthonormal]] set of vectors is isotropic.

As a related definition, a [[convex body]] <math display="inline">K</math> in <math display="inline">\mathbb{R}^n</math> is called isotropic if it has volume <math display="inline">|K|=1</math>, center of mass at the origin, and there is a constant <math display="inline">\alpha>0</math> such that

:<math>\int_K \langle x, y \rangle^2 dx = \alpha^2 |y|^2,</math>

for all vectors <math display="inline">y</math> in <math display="inline">\mathbb{R}^n</math>; here <math display="inline">|\cdot|</math> stands 
for the standard Euclidean norm.


== See also ==

* [[Whitening transformation]]

== References ==
* {{cite journal |first=M. |last=Rudelson |title=Random Vectors in the Isotropic Position |journal=[[Journal of Functional Analysis]] |volume=164 |year=1999 |issue=1 |pages=60â€“72 |eprint=math/9608208 }}

[[Category:Machine learning]]
[[Category:Random matrices]]