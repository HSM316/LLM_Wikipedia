{{Short description|Machine learning-powered structure design}}
{{Machine learning bar}}
{{otheruses|Nas (disambiguation)}}
'''Neural architecture search''' (NAS)<ref name="survey">{{Cite journal|url=http://jmlr.org/papers/v20/18-598.html|title=Neural Architecture Search: A Survey|first1=Thomas|last1=Elsken|first2=Jan Hendrik|last2=Metzen|first3=Frank|last3=Hutter|date=August 8, 2019|journal=Journal of Machine Learning Research|volume=20|issue=55|pages=1–21|via=jmlr.org|bibcode=2018arXiv180805377E|arxiv=1808.05377}}</ref><ref name="survey2">{{cite arxiv|last=Wistuba|first=Martin|last2=Rawat|first2=Ambrish|last3=Pedapati|first3=Tejaswini|date=2019-05-04|title=A Survey on Neural Architecture Search|eprint=1905.01392|class=cs.LG}}</ref> is a technique for automating the design of [[artificial neural network]]s (ANN), a widely used model in the field of [[machine learning]]. NAS has been used to design networks that are on par or outperform hand-designed architectures.<ref name="Zoph 2016" /><ref name="Zoph 2017">{{cite arxiv|last=Zoph|first=Barret|last2=Vasudevan|first2=Vijay|last3=Shlens|first3=Jonathon|last4=Le|first4=Quoc V.|date=2017-07-21|title=Learning Transferable Architectures for Scalable Image Recognition|eprint=1707.07012|class=cs.CV}}</ref> Methods for NAS can be categorized according to the search space, search strategy and performance estimation strategy used:<ref name="survey" />

* The ''search space'' defines the type(s) of ANN that can be designed and optimized.
* The ''search strategy'' defines the approach used to explore the search space.
* The ''performance estimation strategy'' evaluates the performance of a possible ANN from its design (without constructing and training it).

NAS is closely related to [[hyperparameter optimization]]<ref>Matthias Feurer and Frank Hutter. [https://link.springer.com/content/pdf/10.1007%2F978-3-030-05318-5_1.pdf Hyperparameter optimization]. In: ''AutoML: Methods, Systems, Challenges'', pages 3–38.</ref> and [[Meta learning (computer science)|meta-learning]]<ref>Vanschoren J. (2019) [https://link.springer.com/chapter/10.1007/978-3-030-05318-5_2 Meta-Learning]. In: Hutter F., Kotthoff L., Vanschoren J. (eds) Automated Machine Learning, pages 35-61.</ref> and is a subfield of [[automated machine learning]] (AutoML).<ref>{{Cite journal|last=He, X., Zhao, K., & Chu, X|date=2021-01-05|title=AutoML: A survey of the state-of-the-art|url=https://www.sciencedirect.com/science/article/abs/pii/S0950705120307516|journal=Knowledge-Based Systems|language=en|volume=212|pages=106622|doi=10.1016/j.knosys.2020.106622|issn=0950-7051|via=|arxiv=1908.00709}}</ref>

==Reinforcement learning==
[[Reinforcement learning]] (RL) can underpin a NAS search strategy. Zoph et al.<ref name="Zoph 2016" /> applied NAS with RL targeting the [[CIFAR-10]] dataset and achieved a network architecture that rivals the best manually-designed architecture for accuracy, with an error rate of 3.65, 0.09 percent better and 1.05x faster than a related hand-designed model. On the [[Treebank|Penn Treebank]] dataset, that model composed a recurrent cell that outperforms [[Long short-term memory|LSTM]], reaching a test set perplexity of 62.4, or 3.6 perplexity better than the prior leading system. On the PTB character language modeling task it achieved bits per character of 1.214.<ref name="Zoph 2016">{{cite arxiv|last=Zoph|first=Barret|last2=Le|first2=Quoc V.|date=2016-11-04|title=Neural Architecture Search with Reinforcement Learning|eprint=1611.01578 |class=cs.LG}}</ref>

Learning a model architecture directly on a large dataset can be a lengthy process. NASNet<ref name="Zoph 2017" /><ref>{{Cite news|url=https://research.googleblog.com/2017/11/automl-for-large-scale-image.html|title=AutoML for large scale image classification and object detection|last=Zoph|first=Barret|date=November 2, 2017|work=Research Blog|access-date=2018-02-20|last2=Vasudevan|first2=Vijay|language=en-US|last3=Shlens|first3=Jonathon|last4=Le|first4=Quoc V.}}</ref> addressed this issue by transferring a building block designed for a small dataset to a larger dataset. The design was constrained to use two types of [[Convolutional neural network|convolutional]] cells to return feature maps that serve two main functions when convoluting an input feature map: ''normal cells'' that return maps of the same extent (height and width) and ''reduction cells'' in which the returned feature map height and width is reduced by a factor of two. For the reduction cell, the initial operation applied to the cell’s inputs uses a stride of two (to reduce the height and width).<ref name="Zoph 2017" /> The learned aspect of the design included elements such as which lower layer(s) each higher layer took as input, the transformations applied at that layer and to merge multiple outputs at each layer. In the studied example, the best convolutional layer (or "cell") was designed for the CIFAR-10 dataset and then applied to the [[ImageNet]] dataset by stacking copies of this cell, each with its own parameters. The approach yielded accuracy of 82.7% top-1 and 96.2% top-5. This exceeded the best human-invented architectures at a cost of 9 billion fewer [[FLOPS]]—a reduction of 28%. The system continued to exceed the manually-designed alternative at varying computation levels. The image features learned from image classification can be transferred to other computer vision problems. E.g., for object detection, the learned cells integrated with the Faster-RCNN framework improved performance by 4.0% on the [[COCO (dataset)|COCO]] dataset.<ref name="Zoph 2017" />

In the so-called Efficient Neural Architecture Search (ENAS), a controller discovers architectures by learning to search for an optimal subgraph within a large graph. The controller is trained with [[Reinforcement learning|policy gradient]] to select a subgraph that maximizes the validation set's expected reward. The model corresponding to the subgraph is trained to minimize a canonical [[cross entropy]] loss. Multiple child models share parameters, ENAS requires fewer GPU-hours than other approaches and 1000-fold less than "standard" NAS. On CIFAR-10, the ENAS design achieved a test error of 2.89%, comparable to NASNet. On Penn Treebank, the ENAS design reached test perplexity of 55.8.<ref>{{cite arxiv|last=Hieu|first=Pham|last2=Y.|first2=Guan, Melody|last3=Barret|first3=Zoph|last4=V.|first4=Le, Quoc|last5=Jeff|first5=Dean|date=2018-02-09|title=Efficient Neural Architecture Search via Parameter Sharing|eprint=1802.03268|class=cs.LG}}</ref>

== Evolution ==
Several groups employed [[evolutionary algorithm]]s for NAS.<ref>{{cite arxiv|last=Real|first=Esteban|last2=Moore|first2=Sherry|last3=Selle|first3=Andrew|last4=Saxena|first4=Saurabh|last5=Suematsu|first5=Yutaka Leon|last6=Tan|first6=Jie|last7=Le|first7=Quoc|last8=Kurakin|first8=Alex|date=2017-03-03|title=Large-Scale Evolution of Image Classifiers|eprint=1703.01041|class=cs.NE}}</ref><ref name="Real 2018">{{cite arxiv|last=Real|first=Esteban|last2=Aggarwal|first2=Alok|last3=Huang|first3=Yanping|last4=Le|first4=Quoc V.|date=2018-02-05|title=Regularized Evolution for Image Classifier Architecture Search|eprint=1802.01548|class=cs.NE}}</ref><ref>Stanley, Kenneth; Miikkulainen, Risto, "[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.5457&rep=rep1&type=pdf Evolving Neural Networks through Augmenting Topologies]", in: Evolutionary Computation, 2002</ref>  Mutations in the context of evolving ANNs are operations such as adding a layer, removing a layer or changing the type of a layer (e.g., from convolution to pooling). On [[CIFAR-10]], evolution and RL performed comparably, while both outperformed [[random search]].<ref name="Real 2018" />

==Hill-climbing==
Another group used a [[hill climbing]] procedure that applies network morphisms, followed by short cosine-annealing optimization runs. The approach yielded competitive results, requiring resources on the same order of magnitude as training a single network. E.g., on CIFAR-10, the method designed and trained a network with an error rate below 5% in 12 hours on a single GPU.<ref>{{cite arxiv|last=Thomas|first=Elsken|last2=Jan Hendrik|first2=Metzen|last3=Frank|first3=Hutter|date=2017-11-13|title=Simple And Efficient Architecture Search for Convolutional Neural Networks|eprint=1711.04528|class=stat.ML}}</ref>

== Multi-objective search ==
While most approaches solely focus on finding architecture with maximal predictive performance, for most practical applications other objectives are relevant, such as memory consumption, model size or inference time (i.e., the time required to obtain a prediction). Because of that, researchers created a [[Multi-objective optimization|multi-objective]] search.<ref name="Elsken 2018">{{cite arxiv|last=Elsken|first=Thomas|last2=Metzen|first2=Jan Hendrik|last3=Hutter|first3=Frank|date=2018-04-24|title=Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution|eprint=1804.09081|class=stat.ML}}</ref><ref name="Zhou 2018">{{cite web|url=https://www.sysml.cc/doc/2018/94.pdf|title=Neural Architect: A Multi-objective Neural Architecture Search with Performance Prediction|last1=Zhou|first1=Yanqi|last2=Diamos|first2=Gregory|date=|website=|publisher=Baidu|access-date=2019-09-27}}</ref>

LEMONADE<ref name="Elsken 2018" /> is an evolutionary algorithm that adopted [[Lamarckism]] to efficiently optimize multiple objectives. In every generation, child networks are generated to improve the [[Pareto efficiency#Pareto frontier|Pareto frontier]] with respect to the current population of ANNs.

Neural Architect<ref name="Zhou 2018" /> is claimed to be a resource-aware multi-objective RL-based NAS with network embedding and performance prediction. Network embedding encodes an existing network to a trainable embedding vector. Based on the embedding, a controller network generates transformations of the target network. A multi-objective reward function considers network accuracy, computational resource and training time. The reward is predicted by multiple performance simulation networks that are pre-trained or co-trained with the controller network. The controller network is trained via policy gradient. Following a modification, the resulting candidate network is evaluated by both an accuracy network and a training time network. The results are combined by a reward engine that passes its output back to the controller network.

== One-shot models ==
RL or evolution-based NAS require thousands of GPU-days of searching/training to achieve state-of-the-art computer vision results as described in the NASNet, mNASNet and MobileNetV3 papers.<ref name="Zoph 2017" /><ref name="mNASNet2">{{cite arxiv|eprint=1807.11626|last1=Tan|first1=Mingxing|title=MnasNet: Platform-Aware Neural Architecture Search for Mobile|last2=Chen|first2=Bo|last3=Pang|first3=Ruoming|last4=Vasudevan|first4=Vijay|last5=Sandler|first5=Mark|last6=Howard|first6=Andrew|last7=Le|first7=Quoc V.|class=cs.CV|year=2018}}</ref><ref name="MobileNetV3">{{cite arxiv|date=2019-05-06|title=Searching for MobileNetV3|eprint=1905.02244|class=cs.CV|last1=Howard|first1=Andrew|last2=Sandler|first2=Mark|last3=Chu|first3=Grace|last4=Chen|first4=Liang-Chieh|last5=Chen|first5=Bo|last6=Tan|first6=Mingxing|last7=Wang|first7=Weijun|last8=Zhu|first8=Yukun|last9=Pang|first9=Ruoming|last10=Vasudevan|first10=Vijay|last11=Le|first11=Quoc V.|last12=Adam|first12=Hartwig}}</ref>

To reduce computational cost, many recent NAS methods rely on the weight-sharing idea.<ref>Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: [[arxiv:1802.03268|Efficient neural architecture search via parameter sharing]]. In: Proceedings of the 35th International Conference on Machine Learning (2018).</ref><ref>Li, L., Talwalkar, A.: [[arxiv:1902.07638|Random search and reproducibility for neural architecture search]]. In: Proceedings of the Conference on Uncertainty in Artificial Intelligence (2019).</ref> In this approach a single supernetwork (also known as the one-shot model) is used as the search space where weights are shared among a large number of different sub-architectures that have edges in common, each of which is considered as a path within the supernet. The essential idea is to train one supernetwork that spans many options for the final design rather than generating and training thousands of networks independently. In addition to the learned parameters, a set of architecture parameters learn to prefer one module over another. Such methods reduce the required computational resources to only a few GPU days.

More recent works further combine this weight-sharing paradigm, with a continuous relaxation of the search space,<ref>H. Cai, L. Zhu, and S. Han. [[arxiv:1812.00332|Proxylessnas: Direct neural architecture search on target task and hardware]]. ICLR, 2019.</ref><ref>X. Dong and Y. Yang. [[arxiv:1910.04465|Searching for a robust neural architecture in four gpu hours]]. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2019.</ref><ref>H. Liu, K. Simonyan, and Y. Yang. [[arxiv:1806.09055|Darts: Differentiable architecture search]]. In ICLR, 2019</ref><ref>S. Xie, H. Zheng, C. Liu, and L. Lin. [[arxiv:1812.09926|Snas: stochastic neural architecture search]]. ICLR, 2019.</ref> which enables the use of gradient-based optimization methods. These approaches are generally referred to as differentiable NAS and have proven very efficient in exploring the search space of neural architectures.

Differentiable NAS has shown to produce competitive results using a fraction of the search-time required by RL-based search methods. For example, FBNet (which is short for Facebook Berkeley Network) demonstrated that supernetwork-based search produces networks that outperform the speed-accuracy tradeoff curve of mNASNet and MobileNetV2 on the ImageNet image-classification dataset. FBNet accomplishes this using over 400x ''less'' search time than was used for mNASNet.<ref name="FBNet">{{cite arxiv|eprint=1812.03443|last1=Wu|first1=Bichen|title=FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search|last2=Dai|first2=Xiaoliang|last3=Zhang|first3=Peizhao|last4=Wang|first4=Yanghan|last5=Sun|first5=Fei|last6=Wu|first6=Yiming|last7=Tian|first7=Yuandong|last8=Vajda|first8=Peter|last9=Jia|first9=Yangqing|last10=Keutzer|first10=Kurt|class=cs.CV|date=24 May 2019}}</ref><ref name="MobileNetV2">{{cite arxiv|eprint=1801.04381|last1=Sandler|first1=Mark|title=MobileNetV2: Inverted Residuals and Linear Bottlenecks|last2=Howard|first2=Andrew|last3=Zhu|first3=Menglong|last4=Zhmoginov|first4=Andrey|last5=Chen|first5=Liang-Chieh|class=cs.CV|year=2018}}</ref><ref>{{Cite web|url=http://sites.ieee.org/scv-cas/files/2019/05/2019-05-22-ieee-co-design-trim.pdf|title=Co-Design of DNNs and NN Accelerators|last=Keutzer|first=Kurt|date=2019-05-22|website=IEEE|url-status=live|archive-url=|archive-date=|access-date=2019-09-26}}</ref> Further, SqueezeNAS demonstrated that supernetwork-based NAS produces neural networks that outperform the speed-accuracy tradeoff curve of MobileNetV3 on the Cityscapes semantic segmentation dataset, and SqueezeNAS uses over 100x less search time than was used in the MobileNetV3 authors' RL-based search.<ref name="SqueezeNAS">{{cite arxiv|eprint=1908.01748|last1=Shaw|first1=Albert|title=SqueezeNAS: Fast neural architecture search for faster semantic segmentation|last2=Hunter|first2=Daniel|last3=Iandola|first3=Forrest|last4=Sidhu|first4=Sammy|class=cs.CV|year=2019}}</ref><ref>{{Cite news|url=https://www.eetimes.com/document.asp?doc_id=1335063|title=Does Your AI Chip Have Its Own DNN?|last=Yoshida|first=Junko|date=2019-08-25|work=EE Times|access-date=2019-09-26}}</ref>

==See also==
*[[Neural Network Intelligence]]

==References==
{{Reflist|30em}}

{{Differentiable computing}}

[[Category:Artificial intelligence]]