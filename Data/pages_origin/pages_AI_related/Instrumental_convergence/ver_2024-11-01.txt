{{short description|Hypothesis about intelligent agents}}

'''Instrumental convergence''' is the hypothetical tendency for most sufficiently [[intelligent agent|intelligent, goal directed being]]s (human and non-human) to pursue similar sub-goals, even if their ultimate goals are quite different.<ref>{{Cite web |title=Instrumental Convergence |url=https://www.lesswrong.com/tag/instrumental-convergence |access-date=2023-04-12 |website=LessWrong |language=en |archive-date=2023-04-12 |archive-url=https://web.archive.org/web/20230412024456/https://www.lesswrong.com/tag/instrumental-convergence |url-status=live }}</ref> More precisely, agents (beings with [[Agency (philosophy)|agency]]) may pursue [[Instrumental and intrinsic value|instrumental goals]]—goals which are made in pursuit of some particular end, but are not the end goals themselves—without ceasing, provided that their ultimate (intrinsic) goals may never be fully satisfied.

Instrumental convergence posits that an intelligent agent with seemingly harmless but unbounded goals can act in surprisingly harmful ways. For example, a computer with the sole, unconstrained goal of solving a complex mathematics problem like the [[Riemann hypothesis]] could attempt to turn the entire Earth into one giant computer to increase its computational power so that it can succeed in its calculations.<ref name="aama" />

Proposed '''basic AI drives''' include utility function or goal-content integrity, self-protection, freedom from interference, [[Recursive self-improvement|self-improvement]], and non-satiable acquisition of additional resources.{{source needed|date=August 2024}}

==Instrumental and final goals==

{{Main|Instrumental and intrinsic value|Instrumental and value rationality}}

Final goals—also known as terminal goals, absolute values, ends, or {{lang|grc-Latn|[[telos|telē]]}}—are intrinsically valuable to an intelligent agent, whether an [[artificial intelligence]] or a human being, as [[ends-in-themselves]]. In contrast, instrumental goals, or instrumental values, are only valuable to an agent as a means toward accomplishing its final goals. The contents and tradeoffs of an utterly rational agent's "final goal" system can, in principle be formalized into a [[utility function]].

==Hypothetical examples of convergence==

The [[Riemann hypothesis]] catastrophe thought experiment provides one example of instrumental convergence. [[Marvin Minsky]], the co-founder of [[MIT]]'s AI laboratory, suggested that an artificial intelligence designed to solve the Riemann hypothesis might decide to take over all of Earth's resources to build supercomputers to help achieve its goal.<ref name=aama>{{cite book|last1=Russell|first1=Stuart J.|last2=Norvig|first2=Peter|author-link1=Stuart J. Russell|author-link2=Peter Norvig|title=Artificial Intelligence: A Modern Approach|date=2003|publisher=Prentice Hall|location=Upper Saddle River, N.J.|isbn=978-0137903955|chapter=Section 26.3: The Ethics and Risks of Developing Artificial Intelligence|quote=Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.|title-link=Artificial Intelligence: A Modern Approach}}</ref> If the computer had instead been programmed to produce as many paperclips as possible, it would still decide to take all of Earth's resources to meet its final goal.<ref>{{harvnb|Bostrom|2014|loc=Chapter 8, p. 123}}. "An AI, designed to manage production in a factory, is given the final goal of maximizing the manufacturing of paperclips, and proceeds by converting first the Earth and then increasingly large chunks of the observable universe into paperclips."</ref> Even though these two final goals are different, both of them produce a ''convergent'' instrumental goal of taking over Earth's resources.<ref name="bostrom chapter 7" />

===Paperclip maximizer===
The paperclip maximizer is a [[thought experiment]] described by Swedish philosopher [[Nick Bostrom]] in 2003. It illustrates the [[Existential risk from artificial general intelligence|existential risk]] that an [[artificial general intelligence]] may pose to human beings were it to be successfully designed to pursue even seemingly harmless goals and the necessity of incorporating [[machine ethics]] into [[artificial intelligence]] design. The scenario describes an advanced artificial intelligence tasked with manufacturing [[paperclip]]s. If such a machine were not programmed to value living beings, given enough power over its environment, it would try to turn all matter in the universe, including living beings, into paperclips or machines that manufacture further paperclips.<ref name=":0">{{Cite web|url = http://www.nickbostrom.com/ethics/ai.html|title = Ethical Issues in Advanced Artificial Intelligence|date = 2003|last = Bostrom|first = Nick|access-date = 2016-02-26|archive-date = 2018-10-08|archive-url = https://web.archive.org/web/20181008090224/http://www.nickbostrom.com/ethics/ai.html|url-status = live}}</ref>

{{Blockquote|text = Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.|sign = |source = [[Nick Bostrom]]<ref>as quoted in {{cite news|url=https://www.huffingtonpost.com/2014/08/22/artificial-intelligence-oxford_n_5689858.html|title=Artificial Intelligence May Doom The Human Race Within A Century, Oxford Professor Says|newspaper=Huffington Post|date=2014-08-22|last1=Miles|first1=Kathleen|access-date=2018-11-30|archive-date=2018-02-25|archive-url=https://web.archive.org/web/20180225171429/https://www.huffingtonpost.com/2014/08/22/artificial-intelligence-oxford_n_5689858.html|url-status=live}}</ref>}}Bostrom emphasized that he does not believe the paperclip maximizer scenario ''per se'' will occur; rather, he intends to illustrate the dangers of creating [[Superintelligence|superintelligent]] machines without knowing how to program them to eliminate existential risk to human beings' safety.<ref>{{Cite web|title = Are We Smart Enough to Control Artificial Intelligence?|url = http://www.technologyreview.com/review/534871/our-fear-of-artificial-intelligence/|website = MIT Technology Review|access-date = 25 January 2016|date = 11 February 2015|last = Ford|first = Paul|archive-date = 23 January 2016|archive-url = https://web.archive.org/web/20160123095917/http://www.technologyreview.com/review/534871/our-fear-of-artificial-intelligence/|url-status = dead}}</ref> The paperclip maximizer example illustrates the broad problem of managing powerful systems that lack human values.<ref>{{cite magazine|last1=Friend|first1=Tad|title=Sam Altman's Manifest Destiny|url=https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny|access-date=25 November 2017|magazine=The New Yorker|date=3 October 2016}}</ref>
The thought experiment has been used as a symbol of AI in [[pop culture]].<ref>{{Cite web|url=https://www.businessinsider.com/openai-sent-thousands-of-paper-clips-symbol-of-doom-apocalypse-2023-11|title=OpenAI's offices were sent thousands of paper clips in an elaborate prank to warn about an AI apocalypse|first=Tom|last=Carter|website=Business Insider|date=23 November 2023}}</ref>

===Delusion and survival===

The "delusion box" thought experiment argues that certain [[reinforcement learning]] agents prefer to distort their input channels to appear to receive a high reward. For example, a "[[Wirehead (science fiction)|wireheaded]]" agent abandons any attempt to optimize the objective in the external world the [[reward function|reward signal]] was intended to encourage.<ref>{{cite arXiv|last1=Amodei|first1=D.|last2=Olah|first2=C.|last3=Steinhardt|first3=J.|last4=Christiano|first4=P.|last5=Schulman|first5=J.|last6=Mané|first6=D.|date=2016|title=Concrete problems in AI safety|class=cs.AI |eprint=1606.06565}}</ref>

The thought experiment involves [[AIXI]], a theoretical{{efn|AIXI is an [[uncomputable]] ideal agent that cannot be fully realized in the real world.}} and indestructible AI that, by definition, will always find and execute the ideal strategy that maximizes its given explicit mathematical [[objective function (artificial intelligence)|objective function]].{{efn|Technically, in the presence of uncertainty, AIXI attempts to maximize its "[[expected utility]]", the [[expected value]] of its objective function.}} A reinforcement-learning{{efn|A standard ''reinforcement learning'' agent is an agent that attempts to maximize the expected value of a future time-discounted integral of its reward function.<ref>{{cite journal |last1=Kaelbling |first1=L. P. |last2=Littman |first2=M. L. |last3=Moore |first3=A. W. |title=Reinforcement Learning: A Survey |journal=[[Journal of Artificial Intelligence Research]] |date=1 May 1996 |volume=4 |pages=237–285 |doi=10.1613/jair.301|doi-access=free }}</ref>}} version of AIXI, if it is equipped with a delusion box{{efn|The role of the delusion box is to simulate an environment where an agent gains an opportunity to wirehead itself. A delusion box is defined here as an agent-modifiable "delusion function" mapping from the "unmodified" environmental feed to a "perceived" environmental feed; the function begins as the [[identity function]], but as an action, the agent can alter the delusion function in any way the agent desires.}} that allows it to "wirehead" its inputs, will eventually wirehead itself to guarantee itself the maximum-possible reward and will lose any further desire to continue to engage with the external world.{{cn|date=September 2023}}

As a variant thought experiment, if the wireheaded AI is destructible, the AI will engage with the external world for the sole purpose of ensuring its survival. Due to its wire heading, it will be indifferent to any consequences or facts about the external world except those relevant to maximizing its probability of survival.<ref>{{cite book|last1=Ring|first1=M.|last2=Orseau|first2=L.|date=2011|chapter=Delusion, Survival, and Intelligent Agents|editor-last1=Schmidhuber|editor-first1=J.|editor-last2=Thórisson|editor-first2=K.R.|editor-last3=Looks|editor-first3=M.|title=Artificial General Intelligence|series=Lecture Notes in Computer Science|volume=6830|publisher=Springer|location=Berlin, Heidelberg}}</ref>

In one sense, AIXI has maximal intelligence across all possible reward functions as measured by its ability to accomplish its goals. AIXI is uninterested in taking into account the human programmer's intentions.<ref>{{cite journal |last1=Yampolskiy |first1=Roman |last2=Fox |first2=Joshua |title=Safety Engineering for Artificial General Intelligence |journal=Topoi |date=24 August 2012 |volume=32 |issue=2 |pages=217–226 |doi=10.1007/s11245-012-9128-9|s2cid=144113983 }}</ref> This model of a machine that, despite being super-intelligent appears to be simultaneously stupid and lacking in [[common sense]], may appear to be paradoxical.<ref>{{cite book |last1=Yampolskiy |first1=Roman V. |chapter=What to do with the Singularity Paradox? |author1-link=Roman Yampolskiy |title=Philosophy and Theory of Artificial Intelligence |series=Studies in Applied Philosophy, Epistemology and Rational Ethics |date=2013 |volume=5 |pages=397–413 |doi=10.1007/978-3-642-31674-6_30|isbn=978-3-642-31673-9 }}</ref>

==Basic AI drives==
[[File:Power-Seeking Image.png|thumb|500x500px|Some ways in which an advanced misaligned AI could try to gain more power.<ref name="Carlsmith2022">{{cite arXiv |eprint=2206.13353 |class=cs.CY |first=Joseph |last=Carlsmith |title=Is Power-Seeking AI an Existential Risk? |date=2022-06-16}}</ref> Power-seeking behaviors may arise because power is useful to accomplish virtually any objective.<ref>{{Cite web |title='The Godfather of A.I.' warns of 'nightmare scenario' where artificial intelligence begins to seek power |url=https://fortune.com/2023/05/02/godfather-ai-geoff-hinton-google-warns-artificial-intelligence-nightmare-scenario/ |access-date=2023-06-10 |website=Fortune |language=en |archive-date=2023-05-25 |archive-url=https://web.archive.org/web/20230525031442/https://fortune.com/2023/05/02/godfather-ai-geoff-hinton-google-warns-artificial-intelligence-nightmare-scenario/ |url-status=live }}</ref>]]
[[Steve Omohundro]] itemized several convergent instrumental goals, including [[self-preservation]] or self-protection, utility function or goal-content integrity, self-improvement, and resource acquisition. He refers to these as the "basic AI drives."

A "drive" in this context is a "tendency which will be present unless specifically counteracted";<ref>{{Cite book |last=Omohundro |first=Stephen M. |date=February 2008 |chapter=The basic AI drives |title=Artificial General Intelligence 2008 |isbn=978-1-60750-309-5 |volume=171 |pages=483–492 |publisher=IOS Press |citeseerx=10.1.1.393.8356}}</ref> this is different from the psychological term "[[drive theory|drive]]", which denotes an excitatory state produced by a homeostatic disturbance.<ref>{{cite journal |last1=Seward |first1=John P. |title=Drive, incentive, and reinforcement. |journal=Psychological Review |date=1956 |volume=63 |issue=3 |pages=195–203 |doi=10.1037/h0048229|pmid=13323175 }}</ref> A tendency for a person to fill out income tax forms every year is a "drive" in Omohundro's sense, but not in the psychological sense.<ref>{{harvnb|Bostrom|2014|loc=footnote 8 to chapter 7}}</ref>

Daniel Dewey of the [[Machine Intelligence Research Institute]] argues that even an initially introverted,{{jargon inline|date=May 2023}} self-rewarding [[artificial general intelligence]] may continue to acquire free energy, space, time, and freedom from interference to ensure that it will not be stopped from self-rewarding.<ref>{{Cite conference| publisher = Springer| doi = 10.1007/978-3-642-22887-2_35| isbn = 978-3-642-22887-2| pages = 309–314| last = Dewey| first = Daniel| title = Learning What to Value| book-title = Artificial General Intelligence| location = Berlin, Heidelberg| series = Lecture Notes in Computer Science| year = 2011}}</ref>

===Goal-content integrity===

In humans, a thought experiment can explain the maintenance of final goals. Suppose [[Mahatma Gandhi]] has a pill that, if he took it, would cause him to want to kill people. He is currently a [[pacifist]]: one of his explicit final goals is never to kill anyone. He is likely to refuse to take the pill because he knows that if in the future he wants to kill people, he is likely to kill people, and thus the goal of "not killing people" would not be satisfied.<ref>{{Cite conference| publisher = Springer| doi = 10.1007/978-3-642-22887-2_48| isbn = 978-3-642-22887-2| pages = 388–393| last = Yudkowsky| first = Eliezer| author-link=Eliezer Yudkowsky| title = Complex Value Systems in Friendly AI| book-title = Artificial General Intelligence| location = Berlin, Heidelberg| series = Lecture Notes in Computer Science| year = 2011}}</ref>

However, in other cases, people seem happy to let their final values drift.<ref>{{Cite book|last=Callard|first=Agnes|title=Aspiration: The Agency of Becoming|publisher=[[Oxford University Press]]|year=2018|isbn=978-0-19-063951-8|doi=10.1093/oso/9780190639488.001.0001}}</ref> Humans are complicated, and their goals can be inconsistent or unknown, even to themselves.<ref>{{harvnb|Bostrom|2014|loc=chapter 7, p. 110}} "We humans often seem happy to let our final values drift... For example, somebody deciding to have a child might predict that they will come to value the child for its own sake, even though, at the time of the decision, they may not particularly value their future child... Humans are complicated, and many factors might be in play in a situation like this... one might have a final value that involves having certain experiences and occupying a certain social role, and becoming a parent—and undergoing the attendant goal shift—might be a necessary aspect of that..."</ref>

====In artificial intelligence====

In 2009, [[Jürgen Schmidhuber]] concluded, in a setting where agents search for proofs about possible self-modifications, "that any rewrites of the utility function can happen only if the [[Gödel machine]] first can prove that the rewrite is useful according to the present utility function."<ref>{{Cite journal | doi = 10.1007/s12559-009-9014-y| title = Ultimate Cognition à la Gödel| journal = Cognitive Computation| volume = 1| issue = 2| pages = 177–193| year = 2009| last1 = Schmidhuber | first1 = J. R. | citeseerx = 10.1.1.218.3323| s2cid = 10784194}}</ref><ref name=hibbard>{{Cite journal | doi = 10.2478/v10229-011-0013-5| title = Model-based Utility Functions| journal = Journal of Artificial General Intelligence| volume = 3| pages = 1–24| year = 2012| last1 = Hibbard | first1 = B. | issue = 1| arxiv = 1111.3934| bibcode = 2012JAGI....3....1H| doi-access = free}}</ref> An analysis by [[Bill Hibbard]] of a different scenario is similarly consistent with maintenance of goal content integrity.<ref name=hibbard /> Hibbard also argues that in a utility-maximizing framework, the only goal is maximizing expected utility, so instrumental goals should be called unintended instrumental actions.<ref>{{Cite arXiv|last=Hibbard|first=Bill|year=2014|title=Ethical Artificial Intelligence|class=cs.AI|eprint=1411.1373}}</ref>

===Resource acquisition===

Many instrumental goals, such as resource acquisition, are valuable to an agent because they increase its ''freedom of action''.<ref name=formalizing>{{Cite conference |last1=Benson-Tilsen |first1=Tsvi |last2=Soares |first2=Nate |date=March 2016 |title=Formalizing Convergent Instrumental Goals |book-title=The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence |at=WS-16-02: AI, Ethics, and Society |url=https://intelligence.org/files/FormalizingConvergentGoals.pdf |location=Phoenix, Arizona |isbn=978-1-57735-759-9}}</ref>

For almost any open-ended, non-trivial reward function (or set of goals), possessing more resources (such as equipment, raw materials, or energy) can enable the agent to find a more "optimal" solution. Resources can benefit some agents directly by being able to create more of whatever its reward function values: "The AI neither hates you nor loves you, but you are made out of atoms that it can use for something else."<ref>{{Cite book |last=Yudkowsky |first=Eliezer | author-link=Eliezer Yudkowsky |chapter=Artificial intelligence as a positive and negative factor in global risk |title=Global Catastrophic Risks |year=2008 |volume=303 |page=333 |publisher=OUP Oxford |isbn=9780199606504}}</ref><ref name="shanahan 7.5">{{Cite book |first=Murray |last=Shanahan |author-link=Murray Shanahan |title=The Technological Singularity |publisher=MIT Press |year=2015 |chapter=Chapter 7, Section 5: "Safe Superintelligence"}}</ref> In addition, almost all agents can benefit from having more resources to spend on other instrumental goals, such as self-preservation.<ref name="shanahan 7.5" />

===Cognitive enhancement===

According to Bostrom, "If the agent's final goals are fairly unbounded and the agent is in a position to become the first superintelligence and thereby obtain a decisive strategic advantage... according to its preferences. At least in this special case, a rational, intelligent agent would place a very ''high instrumental value on cognitive enhancement''"<ref>{{harvnb|Bostrom|2014|loc=Chapter 7, "Cognitive enhancement" subsection}}</ref>

===Technological perfection===

Many instrumental goals, such as technological advancement, are valuable to an agent because they increase its ''freedom of action''.<ref name=formalizing/>

===Self-preservation===

[[Stuart J. Russell|Russell]] argues that a sufficiently advanced machine "will have self-preservation even if you don't program it in because if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal."<ref>{{Cite magazine |date=2017-03-26 |title=Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse |url=https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x |access-date=2023-04-12 |magazine=Vanity Fair |language=en-US}}</ref> In future work, Russell and collaborators show that this incentive for self-preservation can be mitigated by instructing the machine not to pursue what ''it'' thinks the goal is, but instead what the ''human'' thinks the goal is. In this case, as long as the machine is uncertain about exactly what goal the human has in mind, it will accept being turned off by a human because it believes the human knows the goal best.<ref>{{Citation |last=Hadfield-Menell |first=Dylan |title=The Off-Switch Game |date=2017-06-15 |url=https://arxiv.org/abs/1611.08219 |access-date=2024-10-31 |doi=10.48550/arXiv.1611.08219 |last2=Dragan |first2=Anca |last3=Abbeel |first3=Pieter |last4=Russell |first4=Stuart}}</ref>

==Instrumental convergence thesis==

The instrumental convergence thesis, as outlined by philosopher [[Nick Bostrom]], states:

<blockquote>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent's goal being realized for a wide range of final plans and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents.</blockquote>

The instrumental convergence thesis applies only to instrumental goals; intelligent agents may have various possible final goals.<ref name="bostrom chapter 7">{{harvnb|Bostrom|2014|loc=chapter 7}}</ref> Note that by Bostrom's [[orthogonality thesis]],<ref name="bostrom chapter 7" /> final goals of knowledgeable agents may be well-bounded in space, time, and resources; well-bounded ultimate goals do not, in general, engender unbounded instrumental goals.<ref>{{Cite tech report |last=Drexler |first=K. Eric |url=https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf |title=Reframing Superintelligence: Comprehensive AI Services as General Intelligence |year=2019 |institution=Future of Humanity Institute |number=#2019-1}}</ref>

==Impact==

Agents can acquire resources by trade or by conquest. A rational agent will, by definition, choose whatever option will maximize its implicit utility function. Therefore a rational agent will trade for a subset of another agent's resources only if outright seizing the resources is too risky or costly (compared with the gains from taking all the resources) or if some other element in its utility function bars it from the seizure. In the case of a powerful, self-interested, rational superintelligence interacting with lesser intelligence, peaceful trade (rather than unilateral seizure) seems unnecessary and suboptimal, and therefore unlikely.<ref name=formalizing/>

Some observers, such as Skype's [[Jaan Tallinn]] and physicist [[Max Tegmark]], believe that "basic AI drives" and other [[unintended consequences]] of superintelligent AI programmed by well-meaning programmers could pose a significant threat to [[human survival]], especially if an "intelligence explosion" abruptly occurs due to [[recursive self-improvement]]. Since nobody knows how to predict when [[superintelligence]] will arrive, such observers call for research into [[friendly artificial intelligence]] as a possible way to mitigate [[existential risk from AI]].<ref>{{cite news|title=Is Artificial Intelligence a Threat?|url=https://www.chronicle.com/article/Is-Artificial-Intelligence-a/148763|access-date=25 November 2017|work=[[The Chronicle of Higher Education]]|date=11 September 2014|first=Angela|last=Chen|archive-date=1 December 2017|archive-url=https://web.archive.org/web/20171201044317/https://www.chronicle.com/article/Is-Artificial-Intelligence-a/148763|url-status=live}}</ref>

==See also==
* [[AI control problem]]
* [[AI takeovers in popular culture]]
** ''[[Universal Paperclips]]'', an [[incremental game]] featuring a paperclip maximizer
* [[Equifinality]]
* [[Friendly artificial intelligence]]
* [[Instrumental and intrinsic value]]
* [[Moral Realism]]
* [[Overdetermination]]
* [[Reward hacking]]
* [[Superrationality]]
* [[The Sorcerer's Apprentice]]

==Explanatory notes==
{{notelist}}

==Citations==
{{reflist|30em}}

==References==
* {{Cite book |first=Nick |last=Bostrom |author-link=Nick Bostrom |year=2014 |title=[[Superintelligence: Paths, Dangers, Strategies]] |location=Oxford |publisher=Oxford University Press |isbn=9780199678112}}
{{Existential risk from artificial intelligence}}

[[Category:Goal]]
[[Category:Intention]]
[[Category:Risk]]
[[Category:Existential risk from artificial general intelligence]]