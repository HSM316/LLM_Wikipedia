{{short description|2014 book by Nick Bostrom}}
{{Use dmy dates|date=April 2022}}
{{infobox book | <!-- See Wikipedia:WikiProject_Novels or Wikipedia:WikiProject_Books -->
| name         = Superintelligence:<br />Paths, Dangers, Strategies
| image        = File:Superintelligence.jpg
| caption      = First edition
| border       = yes
| author       = [[Nick Bostrom]]
| country      = [[United Kingdom]]
| genre        = [[Philosophy]], [[popular science]]
| subject      = [[Artificial intelligence]]
| language     = [[English language|English]]
| publisher    = [[Oxford University Press]]<ref name="Superintelligent Swede snapped up by OUP">{{cite web|url=http://www.thebookseller.com/news/superintelligent-swede-snapped-oup|title=Superintelligent Swede snapped up by OUP|website=The Bookseller|date=21 November 2013}}</ref>
| release_date = July 3, 2014 (UK)<br />September 1, 2014 (US)
| media_type   = Print, e-book, audiobook 
| pages        = 352 pp.
| isbn         = 978-0199678112
| preceded_by = [[Global Catastrophic Risks (book)|Global Catastrophic Risks]]
}}

'''''Superintelligence: Paths, Dangers, Strategies''''' is a 2014 book by the philosopher [[Nick Bostrom]]. It explores how [[superintelligence]] could be created and what its features and motivations might be.<ref name=":0" /> It argues that superintelligence, if created, would be difficult to control, and that it could take over the world in order to accomplish its goals. The book also presents strategies to help make superintelligences whose goals benefit humanity.<ref name="Henderson">{{cite news |last1=Henderson |first1=Caspar |date=17 July 2014 |title=Superintelligence by Nick Bostrom and A Rough Ride to the Future by James Lovelock – review |work=The Guardian |url=https://www.theguardian.com/books/2014/jul/17/superintelligence-nick-brostrom-rough-ride-future-james-lovelock-review |access-date=30 July 2014}}</ref> It was particularly influential for raising concerns about [[Existential risk from artificial general intelligence|existential risk from artificial intelligence]].<ref name="new yorker doomsday" />

==Synopsis==
It is unknown whether human-level [[artificial intelligence]] will arrive in a matter of years, later this century, or not until future centuries. Regardless of the initial timescale, once human-level machine intelligence is developed, a "superintelligent" system that "greatly exceeds the cognitive performance of humans in virtually all domains of interest" would most likely follow surprisingly quickly. Such a superintelligence would be very difficult to control.

While the ultimate goals of superintelligences could vary greatly, a functional superintelligence will spontaneously generate, as natural subgoals, "[[instrumental convergence|instrumental goals]]" such as self-preservation and goal-content integrity, cognitive enhancement, and resource acquisition. For example, an agent whose sole final goal is to solve the [[Riemann hypothesis]] (a famous unsolved mathematical [[conjecture]]) could create and act upon a subgoal of transforming the entire Earth into some form of [[computronium]] (hypothetical material optimized for computation) to assist in the calculation. The superintelligence would proactively resist any outside attempts to turn the superintelligence off or otherwise prevent its subgoal completion. In order to prevent such an [[existential risk|existential catastrophe]], it is necessary to successfully solve the "[[AI control problem]]" for the first superintelligence. The solution might involve instilling the superintelligence with goals that are compatible with human survival and well-being. Solving the control problem is surprisingly difficult because most goals, when translated into machine-implementable code, lead to unforeseen and undesirable consequences.

The owl on the book cover alludes to an analogy which Bostrom calls the "Unfinished Fable of the Sparrows".<ref name="observer adams"/> A group of sparrows decide to find an owl chick and raise it as their servant.<ref name=telegraph/> They eagerly imagine "how easy life would be" if they had an owl to help build their nests, to defend the sparrows and to free them for a life of leisure. The sparrows start the difficult search for an owl egg; only "Scronkfinkle", a "one-eyed sparrow with a fretful temperament", suggests thinking about the complicated question of how to tame the owl before bringing it "into our midst". The other sparrows demur; the search for an owl egg will already be hard enough on its own: "Why not get the owl first and work out the fine details later?" Bostrom states that "It is not known how the story ends", but he dedicates his book to Scronkfinkle.<ref name="observer adams">{{cite news |last1=Adams |first1=Tim |title=Nick Bostrom: 'We are like small children playing with a bomb' |url=https://www.theguardian.com/technology/2016/jun/12/nick-bostrom-artificial-intelligence-machine |access-date=29 March 2020 |work=[[The Observer]] |date=12 June 2016}}</ref><ref name="new yorker doomsday">{{cite magazine |last1=Khatchadourian |first1=Raffi |title=The Doomsday Invention |url=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom |access-date=29 March 2020 |magazine=The New Yorker |date=2015 |language=en}}</ref>

==Reception==
The book ranked #17 on ''The New York Times'' list of best selling science books for August 2014.<ref>{{cite web |url=https://www.nytimes.com/2014/09/09/science/best-selling-science-books.html?module=Search&mabReward=relbias%3As&_r=0 |title=Best Selling Science Books |author=<!--Staff writer(s); no by-line.--> |date=September 8, 2014 |work=The New York Times |access-date=9 November 2014}}</ref> In the same month, [[business magnate]] [[Elon Musk]] made headlines by agreeing with the book that artificial intelligence is potentially more dangerous than [[nuclear weapons]].<ref>{{cite web|url=http://www.thetimes.co.uk/tto/technology/article4166795.ece|title=Artificial intelligence 'may wipe out the human race'|work=The Times|first=James|last=Dean|date=5 August 2014|access-date=5 August 2014}}</ref><ref>{{cite web|url=http://www.cbsnews.com/news/elon-musk-artificial-intelligence-may-be-more-dangerous-than-nukes/|title=Elon Musk tweets Artificial Intelligence may be "more dangerous than nukes"|date=4 August 2014|work=CBC News|first=Eliene|last=Augenbraun|access-date=5 August 2014}}</ref><ref>{{cite web|url=http://opinionator.blogs.nytimes.com/2015/02/23/outing-a-i-beyond-the-turing-test/?_r=0|title=Outing A.I.: Beyond the Turing Test|work= The New York Times|first=Benjamin H.|last=Bratton|date=23 February 2015|access-date=4 March 2015}}</ref>
Bostrom's work on [[superintelligence]] has also influenced [[Bill Gates]]’s concern for the existential risks facing humanity over the coming century.<ref>{{cite web|url=https://www.forbes.com/sites/ericmack/2015/01/28/bill-gates-also-worries-artificial-intelligence-is-a-threat/|title=Bill Gates Says You Should Worry About Artificial Intelligence|work=Forbes|first=Eric|last=Mack|date=28 January 2015|access-date=19 February 2015}}</ref><ref>{{cite web|url=http://www.thefiscaltimes.com/2015/01/28/Bill-Gates-Worried-About-Rise-Machines|title=Bill Gates Is Worried About the Rise of the Machines|publisher=The Fiscal Times|first=Andrew|last=Lumby|date=28 January 2015|access-date=19 February 2015}}</ref> In a March 2015 interview by [[Baidu]]'s CEO, [[Robin Li]], Gates said that he would "highly recommend" ''Superintelligence''.<ref>{{cite web|url=https://www.youtube.com/watch?v=NG0ZjUfOBUs&t=17m35s|title=Baidu CEO Robin Li interviews Bill Gates and Elon Musk at the Boao Forum, March 29 2015|author=Kaiser Kuo|publisher=YouTube|date=31 March 2015|access-date=8 April 2015}}</ref> According to the ''[[The New Yorker|New Yorker]]'', philosophers [[Peter Singer]] and [[Derek Parfit]] have "received it as a work of importance".<ref name="new yorker doomsday"/> [[Sam Altman]] wrote in 2015 that the book is the best thing he has ever read on AI risks.<ref>{{Cite web |last=Black |first=Melia Russell, Julia |title=He's played chess with Peter Thiel, sparred with Elon Musk and once, supposedly, stopped a plane crash: Inside Sam Altman's world, where truth is stranger than fiction |url=https://www.businessinsider.com/sam-altman-openai-chatgpt-worldcoin-helion-future-tech-2023-4 |access-date=2023-08-15 |website=Business Insider |language=en-US}}</ref>

The science editor of the ''[[Financial Times]]'' found that Bostrom's writing "sometimes veers into opaque language that betrays his background as a philosophy professor" but convincingly demonstrates that the risk from superintelligence is large enough that society should start thinking now about ways to endow future machine intelligence with positive values.<ref name="ft">{{cite web |last=Cookson |first=Clive |date=13 July 2014 |title=Superintelligence: Paths, Dangers, Strategies, by Nick Bostrom |url=http://www.ft.com/intl/cms/s/2/021d3484-fd1d-11e3-8ca9-00144feab7de.html |url-access=subscription |url-status=live |archive-url=https://archive.today/20140806205203/http://www.ft.com/cms/s/2/021d3484-fd1d-11e3-8ca9-00144feab7de.html%23axzz39eC4pmuS |archive-date=2014-08-06 |access-date=30 July 2014 |publisher=The Financial Times}}</ref> A review in ''[[The Guardian]]'' pointed out that "even the most sophisticated machines created so far are intelligent in only a limited sense" and that "expectations that AI would soon overtake human intelligence were first dashed in the 1960s", but the review finds common ground with Bostrom in advising that "one would be ill-advised to dismiss the possibility altogether".<ref name=Henderson/>

Some of Bostrom's colleagues suggest that [[nuclear war]] presents a greater threat to humanity than superintelligence, as does the future prospect of the weaponisation of [[nanotechnology]] and [[biotechnology]].<ref name=Henderson/> ''[[The Economist]]'' stated that "Bostrom is forced to spend much of the book discussing speculations built upon plausible conjecture... but the book is nonetheless valuable. The implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect of actually doing so seems remote."<ref name=":0">{{cite news|title=Clever cogs|url=https://www.economist.com/news/books-and-arts/21611037-potential-impacts-intelligent-machines-human-life-clever-cogs|access-date=9 August 2014|publisher=[[The Economist]]|date=9 August 2014}}</ref> [[Ronald Bailey]] wrote in the libertarian ''[[Reason (magazine)|Reason]]'' that Bostrom makes a strong case that solving the AI control problem is the "essential task of our age".<ref>{{cite news|last1=Bailey|first1=Ronald|title=Will Superintelligent Machines Destroy Humanity?|url=http://reason.com/archives/2014/09/12/will-superintelligent-machines-destroy-h|access-date=16 September 2014|work=[[Reason (magazine)|Reason]]|date=12 September 2014}}</ref> According to Tom Chivers of ''[[The Daily Telegraph]]'', the book is difficult to read but nonetheless rewarding.<ref name=telegraph>{{cite news|last1=Chivers|first1=Tom|title=Superintelligence by Nick Bostrom, review: 'a hard read'|url=https://www.telegraph.co.uk/culture/books/bookreviews/11021594/Superintelligence-by-Nick-Bostrom-review-a-hard-read.html|work=The Telegraph|access-date=16 August 2014|date=10 August 2014}}</ref> A reviewer in the ''[[Journal of Experimental & Theoretical Artificial Intelligence]]'' broke with others by stating the book's "writing style is clear" and praised the book for avoiding "overly technical jargon".<ref>{{cite journal |last1=Thomas |first1=Joel |title=In defense of philosophy: a review of Nick Bostrom |journal=[[Journal of Experimental & Theoretical Artificial Intelligence]] |date=July 2015 |volume=28 |issue=6 |pages=1089–1094 |doi=10.1080/0952813X.2015.1055829|doi-access=free }}</ref> A reviewer in ''[[Philosophy (journal)|Philosophy]]'' judged ''Superintelligence'' to be "more realistic" than Ray Kurzweil's ''[[The Singularity Is Near]]''.<ref>{{cite journal |last1=Richmond |first1=Sheldon |title=Superintelligence: Paths, Dangers, Strategies. |journal=[[Philosophy (journal)|Philosophy]] |date=8 July 2015 |volume=91 |issue=1 |pages=125–130 |doi=10.1017/S0031819115000340|s2cid=171005535 }}</ref>

==See also==
* [[AI alignment]]
* [[AI safety]]
* [[Future of Humanity Institute]]
* ''[[Human Compatible]]''
* ''[[Life 3.0]]''
* [[Philosophy of artificial intelligence]]
* ''[[The Precipice: Existential Risk and the Future of Humanity]]''

==References==
{{Reflist|30em}}

{{Existential risk from artificial intelligence|state=expanded}}{{Effective altruism}}{{Future of Humanity Institute}}

[[Category:2014 non-fiction books]]
[[Category:Existential risk from artificial general intelligence]]
[[Category:Futurology books]]
[[Category:Works by Nick Bostrom]]
[[Category:English non-fiction books]]
[[Category:English-language books]]
[[Category:Oxford University Press books]]
[[Category:Books in philosophy of technology]]