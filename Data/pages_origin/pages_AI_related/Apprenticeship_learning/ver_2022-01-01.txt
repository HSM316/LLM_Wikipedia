{{about|machine learning via observation of human experts|human learning|Apprenticeship}}

In [[artificial intelligence]], '''apprenticeship learning''' (or '''learning from demonstration''') is the process of learning by observing an expert.<ref name=AIRP>[http://dl.acm.org/citation.cfm?id=1015430 "Apprenticeship learning via inverse reinforcement learning"]. [[Pieter Abbeel]], Andrew Ng, In 21st International Conference on Machine Learning (ICML). 2004.</ref><ref name=survey>{{cite journal|last1=Argall|first1=Brenna D.|last2=Chernova|first2=Sonia|last3=Veloso|first3=Manuela|last4=Browning|first4=Brett|title=A survey of robot learning from demonstration|journal=Robotics and Autonomous Systems|date=May 2009|volume=57|issue=5|pages=469–483|doi=10.1016/j.robot.2008.10.024|citeseerx=10.1.1.145.345}}</ref>  It can be viewed as a form of [[supervised learning]], where the training dataset consists of task executions by a demonstration teacher.<ref name=survey/>

==Mapping function approach==
Mapping methods try to mimic the expert by forming a direct mapping either from states to actions,<ref name=survey/> or from states to reward values.<ref name=AIRP/> For example, in 2002 researchers used such an approach to teach an AIBO robot basic soccer skills.<ref name=survey/>

===Inverse reinforcement learning approach===
'''Inverse reinforcement learning''' (IRL) is the process of deriving a reward function from observed behavior. While ordinary "reinforcement learning" involves using rewards and punishments to learn behavior, in IRL the direction is reversed, and a robot observes a person's behavior to figure out what goal that behavior seems to be trying to achieve.<ref>{{cite news|last1=Wolchover|first1=Natalie|title=This Artificial Intelligence Pioneer Has a Few Concerns|url=https://www.wired.com/2015/05/artificial-intelligence-pioneer-concerns/|access-date=22 January 2018|work=WIRED}}</ref> The IRL problem can be defined as:<ref name="russell1998learning" />

<blockquote>Given 1) measurements of an agent's behaviour over time, in a variety of circumstances; 2) measurements of the sensory inputs to that agent; 3) a model of the physical environment (including the agent's body): Determine the reward function that the agent is optimizing.</blockquote>

IRL researcher [[Stuart J. Russell]] proposes that IRL might be used to observe humans and attempt to codify their complex "ethical values", in an effort to create "ethical robots" that might someday know "not to cook your cat" without needing to be explicitly told.<ref>{{cite news|last1=Havens|first1=John C.|title=The ethics of AI: how to stop your robot cooking your cat|url=https://www.theguardian.com/sustainable-business/2015/jun/23/the-ethics-of-ai-how-to-stop-your-robot-cooking-your-cat|access-date=22 January 2018|work=the Guardian|date=23 June 2015}}</ref> The scenario can be modeled as a "cooperative inverse reinforcement learning game", where a "person" player and a "robot" player cooperate to secure the person's implicit goals, despite these goals not being explicitly known by either the person nor the robot.<ref>{{cite news|title=Artificial Intelligence And The King Midas Problem|url=https://www.huffingtonpost.com/entry/artificial-intelligence-and-the-king-midas-problem_us_5847198ae4b05236f110601b|access-date=22 January 2018|work=Huffington Post|date=12 December 2016}}</ref><ref>Hadfield-Menell, D., Russell, S. J., Abbeel, Pieter & Dragan, A. (2016). Cooperative inverse reinforcement learning. In Advances in neural information processing systems (pp. 3909-3917).</ref>

In 2017, [[OpenAI]] and [[DeepMind]] applied [[deep learning]] to the cooperative inverse reinforcement learning in simple domains such as Atari games and straightforward robot tasks such as backflips. The human role was limited to answering queries from the robot as to which of two different actions were preferred. The researchers found evidence that the techniques may be economically scalable to modern systems.<ref>{{cite news|title=Two Giants of AI Team Up to Head Off the Robot Apocalypse|url=https://www.wired.com/story/two-giants-of-ai-team-up-to-head-off-the-robot-apocalypse/|access-date=29 January 2018|work=WIRED|date=7 July 2017}}</ref><ref>Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems (pp. 4302-4310).</ref>

'''Apprenticeship via inverse reinforcement learning''' (AIRP) was developed by in 2004 [[Pieter Abbeel]], Professor in [[University of California, Berkeley|Berkeley]]'s [[Electrical engineering|EE]][[Computer science|CS]] department, and [[Andrew Ng]], Associate Professor in [[Stanford University]]'s Computer Science Department. AIRP deals with "[[Markov decision process]] where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform".<ref name=AIRP/> AIRP has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted.

One domain where AIRP has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like [[aerobatics]] for shows has been successful. These include [[aerobatic maneuver]]s like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and Andrew Ng - "Autonomous Helicopter Aerobatics through Apprenticeship Learning"<ref>[http://dl.acm.org/citation.cfm?id=1894944 Pieter Abbeel, Adam Coates, Andrew Ng, “Autonomous Helicopter Aerobatics through Apprenticeship Learning.” In Vol. 29, Issue 13 International Journal of Robotics Research. 2010.]</ref>

===System model approach===
System models try to mimic the expert by modeling world dynamics.<ref name=survey/>

==Plan approach==
The system learns rules to associate preconditions and postconditions with each action. In one 1994 demonstration, a humanoid learns a generalized plan from only two demonstrations of a repetitive ball
collection task.<ref name=survey/>

== Example ==
Learning from demonstration is often explained from a perspective that the working [[Robot software|Robot-control-system]] is available and the human-demonstrator is using it. And indeed, if the software works, the [[Human–machine system|Human operator]] takes the robot-arm, makes a move with it, and the robot will reproduce the action later. For example, he teaches the robot-arm how to put a cup under a coffeemaker and press the start-button. In the replay phase, the robot is imitating this behavior 1:1. But that is not how the system works internally; it is only what the audience can observe. In reality, Learning from demonstration is much more complex.

In 1997, robotics expert [[Stefan Schaal]] was working on the [[Sarcos]] robot-arm. The goal was simple: solve the [[Pendulum (mathematics)|pendulum swingup task]]. The robot itself can execute a movement, and as a result, the pendulum is moving. The problem is, that it is unclear what actions will result into which movement. It is an [[Optimal control]]-problem which can be described with mathematical formulas but is hard to solve. The idea from Schaal was, not to use a [[Brute-force search|Brute-force solver]] but record the movements of a human-demonstration. The angle of the pendulum is logged over the timeperiod of 3 seconds at the y-axis. This results into a diagram which produces a pattern.<ref name="atkeson1997learning" />

{| class="wikitable"
|+ Trajectory over time
|-
! time (seconds)
! angle (radians)
|-
| 0
| -3.0
|-
| 0.5
| -2.8
|-
| 1.0
| -4.5
|-
| 1.5
| -1.0
|}

In computer animation, the principle is called [[Interpolation (computer graphics)|spline animation]].<ref name="akgun2012keyframe" /> That means, on the x-axis the time is given, for example 0.5 seconds, 1.0 seconds, 1.5 seconds, while on the y-axis is the variable given. In most cases it's the position of an object. In the inverted pendulum it is the angle.

The overall task consists of two parts: recording the angle over time and reproducing the recorded motion. The reproducing step is surprisingly simple. As an input we know, in which time step which angle the pendulum must have. Bringing the system to a state is called “Tracking control” or [[PID controller#PID controller theory|PID control]]. That means, we have a trajectory over time, and must find control actions to map the system to this trajectory. Other authors call the principle “steering behavior”,<ref name="reynolds1999steering" /> because the aim is to bring a robot to a given line.

==See also==
* [[Inverse reinforcement learning]]

==References==
<references>
<ref name="atkeson1997learning">
{{cite book
| title      = Learning tasks from a single demonstration
| author     = Atkeson, Christopher G., and Stefan Schaal
| year       = 1997
| publisher  = IEEE
| journal    = Proceedings of International Conference on Robotics and Automation
| url        = https://www.cs.cmu.edu/~cga/papers/cga-icra97alt.pdf
| doi        = 10.1109/robot.1997.614389
| volume     = 2
| pages      = 1706–1712
| isbn     = 978-0-7803-3612-4
| citeseerx     = 10.1.1.385.3520
}}
</ref>

<ref name="russell1998learning">
{{cite book
| contribution = Learning agents for uncertain environments
| author     = Russell, Stuart
| year       = 1998
| title  = Proceedings of the eleventh annual conference on Computational learning theory
| doi        = 10.1145/279943.279964
| pages      = 101–103
}}
</ref>

<ref name="reynolds1999steering">
{{cite conference
| title      = Steering behaviors for autonomous characters
| author     = Reynolds, Craig W
| year       = 1999
| pages      = 763–782
| conference = Game developers conference
| url        = http://www.red3d.com/cwr/steer/gdc99/
}}
</ref>

<ref name="akgun2012keyframe">
{{cite journal
| title      = Keyframe-based Learning from Demonstration
| author     = Baris Akgun and Maya Cakmak and Karl Jiang and Andrea L. Thomaz
| year       = 2012
| journal    = International Journal of Social Robotics
| url        = https://smartech.gatech.edu/bitstream/handle/1853/44594/akgun12-soro.pdf
| doi        = 10.1007/s12369-012-0160-0
| volume     = 4
| number     = 4
| pages      = 343–355
}}
</ref>
</references>
{{Use dmy dates|date=September 2018}}

{{DEFAULTSORT:Apprenticeship Learning}}
[[Category:Machine learning]]