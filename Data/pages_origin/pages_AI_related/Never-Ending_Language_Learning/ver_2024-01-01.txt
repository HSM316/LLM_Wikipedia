'''Never-Ending Language Learning''' system ('''NELL''') is a [[Semantics|semantic]] [[machine learning]] [[Computer system|system]] that as of 2010 was being developed by a research team at [[Carnegie Mellon University]], and supported by grants from [[DARPA]], [[Google]], [[National Science Foundation|NSF]], and [[CNPq]] with portions of the system running on a [[Supercomputer|supercomputing]] [[Computer cluster|cluster]] provided by [[Yahoo!]].<ref name=NYT2010>{{cite news |title=Aiming to Learn as We Do, a Machine Teaches Itself |url=https://www.nytimes.com/2010/10/05/science/05compute.html?hpw=&pagewanted=all |quote=Since the start of the year, a team of researchers at Carnegie Mellon University — supported by grants from the Defense Advanced Research Projects Agency and Google, and tapping into a research supercomputing cluster provided by Yahoo — has been fine-tuning a computer system that is trying to master semantics by learning more like a human.  |work=[[New York Times]] |date=October 4, 2010 |accessdate=2010-10-05 }}</ref>

==Process and goals==
{{update section|date=October 2023}}
NELL was programmed by its developers to be able to identify a basic set of fundamental semantic relationships between a few hundred predefined categories of data, such as cities, companies, emotions and sports teams. Since the beginning of 2010, the Carnegie Mellon research team has been running NELL around the clock, sifting through hundreds of millions of web pages looking for connections between the information it already knows and what it finds through its search process &ndash; to make new connections in a manner that is intended to mimic the way humans learn new information.<ref>[http://rtw.ml.cmu.edu/rtw/overview Project Overview], [[Carnegie Mellon University]]. Accessed October 5, 2010.</ref> For example, in encountering the word pair "Pikes Peak", NELL would notice that both words are capitalized and deduce from the second word that it was the name of a mountain, and then build on the relationship of words surrounding those two words to deduce other connections.<ref name=NYT2010/>

The goal of NELL and other semantic learning systems, such as [[IBM]]'s [[Watson (artificial intelligence software)|Watson]] system, is to be able to develop means of [[question answering|answering questions]] posed by users in natural language with no human intervention in the process.<ref>Trader, Tiffany. [http://www.hpcwire.com/news/Machine-Learns-Language-Starting-with-the-Facts-104384244.html "Machine Learns Language Starting with the Facts"], HPCwire, October 5, 2010. Accessed October 5, 2010.</ref> [[Oren Etzioni]] of the [[University of Washington]] lauded the system's "continuous learning, as if NELL is exercising curiosity on its own, with little human help".<ref name=NYT2010/>

By October 2010, NELL has doubled the number of relationships it has available in its knowledge base and has learned 440,000 new facts, with an accuracy of 87%.<ref>[http://rtw.ml.cmu.edu/rtw/ "NELL: Never-Ending Language Learning"], [[Carnegie Mellon University]]. Accessed October 5, 2010.</ref><ref name=NYT2010/> Team leader [[Tom M. Mitchell]], chairman of the machine learning department at Carnegie Mellon described how NELL "self-corrects when it has more information, as it learns more", though it does sometimes arrive at incorrect conclusions. Accumulated errors, such as the deduction that [[HTTP cookie|Internet cookies]] were a kind of baked good, led NELL to deduce from the phrases "I deleted my Internet cookies" and "I deleted my files" that "[[computer file]]s" also belonged in the baked goods category.<ref>VanHemert, Kyle. [http://www.gizmodo.com.au/2010/10/right-now-a-computer-is-reading-online-teaching-itself-language/ "Right Now A Computer Is Reading Online, Teaching Itself Language"], [[Gizmodo]], October 6, 2010. Accessed October 5, 2010.</ref> Clear errors like these are{{When|date=September 2023}} corrected every few weeks by the members of the research team and the system is allowed to continue its learning process.<ref name=NYT2010/> By 2018, NELL had "acquired a knowledge base with 120mn diverse, confidence-weighted beliefs (e.g., ''servedWith(tea,biscuits)''), while learning thousands of interrelated functions that continually improve its reading competence over time."<ref>{{Cite journal |last=Mitchell |first=T. |last2=Cohen |first2=W. |last3=Hruschka |first3=E. |last4=Talukdar |first4=P. |last5=Yang |first5=B. |last6=Betteridge |first6=J. |last7=Carlson |first7=A. |last8=Dalvi |first8=B. |last9=Gardner |first9=M. |last10=Kisiel |first10=B. |last11=Krishnamurthy |first11=J. |last12=Lao |first12=N. |last13=Mazaitis |first13=K. |last14=Mohamed |first14=T. |last15=Nakashole |first15=N. |date=2018-04-24 |title=Never-ending learning |url=https://dl.acm.org/doi/10.1145/3191513 |journal=Communications of the ACM |language=en |volume=61 |issue=5 |pages=103–115 |doi=10.1145/3191513 |issn=0001-0782|doi-access=free }}</ref>

As of September 2023, the project's most recently gathered facts dated from February 2019 (according to its Twitter feed)<ref>{{Cite web |title=NELL (@cmunell) {{!}} Twitter |url=https://twitter.com/cmunell |access-date=2023-09-04 |website=twitter.com |language=en}}</ref> or September 2018 (according to its home page).<ref>{{Cite web |title=Read the Web :: Carnegie Mellon University |url=http://rtw.ml.cmu.edu/rtw/ |access-date=2023-09-04 |website=rtw.ml.cmu.edu}}</ref>

== Reception ==
In his 2019 book "[[Human Compatible]]", [[Stuart J. Russell|Stuart Russell]] commented that 'Unfortunately NELL has confidence in only 3 percent of its beliefs and relies on human experts to clean out false or meaningless beliefs on a regular basis—such as its beliefs that “Nepal is a country also known as United States” and "value is an agricultural product that is usually cut into basis."'<ref>{{cite book |last1=Russell |first1=Stuart |title=Human Compatible: AI and the Problem of Control |date=2019 |publisher=Allen Lane |chapter= 3 }}</ref> A 2023 paper commented that "While the ''never-ending'' part seems like the right approach, NELL still had the drawback that its focus remained much too grounded on object-language descriptions, and relied on web pages as its only source, which significantly influenced the type of grammar, symbolism, slang, etc. analysed."<ref>{{Cite journal |last=de Jager |first=S. |date=2023-04-11 |title=Semantic noise in the Winograd Schema Challenge of pronoun disambiguation |url=https://www.nature.com/articles/s41599-023-01643-9 |journal=Humanities and Social Sciences Communications |language=en |volume=10 |issue=1 |pages=1–10 |doi=10.1057/s41599-023-01643-9 |issn=2662-9992|doi-access=free }}</ref>

==See also ==
* [[Cognitive architecture]]
* [[Computational models of language acquisition]]
* [[Cyc]]
* [[Darwin among the Machines]]
* [[The Adolescence of P-1]]

==References==
{{reflist}}

== External links ==
*[http://rtw.ml.cmu.edu/rtw/ Project homepage]

[[Category:Natural language processing software]]
[[Category:Data mining and machine learning software]]