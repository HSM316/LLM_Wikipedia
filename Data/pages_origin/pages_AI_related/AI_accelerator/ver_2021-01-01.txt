{{Use American English|date = January 2019}}
{{Short description|Hardware acceleration unit for artificial intelligence tasks}}
{{Use mdy dates|date = January 2019}}

An '''AI accelerator''' is a class of specialized [[hardware acceleration|hardware accelerator]]<ref>{{cite web|url=https://www.v3.co.uk/v3-uk/news/3014293/intel-unveils-movidius-compute-stick-usb-ai-accelerator|title=Intel unveils Movidius Compute Stick USB AI Accelerator|date=2017-07-21|access-date=August 11, 2017|archive-url=https://web.archive.org/web/20170811193632/https://www.v3.co.uk/v3-uk/news/3014293/intel-unveils-movidius-compute-stick-usb-ai-accelerator|archive-date=August 11, 2017|url-status=dead|df=mdy-all}}</ref> or computer system<ref>{{cite web|url=https://insidehpc.com/2017/06/inspurs-unveils-gx4-ai-accelerator/|title=Inspurs unveils GX4 AI Accelerator|date=2017-06-21}}</ref><ref>{{Citation|title=Neural Magic raises $15 million to boost AI inferencing speed on off-the-shelf processors|last=Wiggers|first=Kyle|date=November 6, 2019|url=https://venturebeat.com/2019/11/06/neural-magic-raises-15-million-to-boost-ai-training-speed-on-off-the-shelf-processors/|publication-date=November 6, 2019|orig-year=2019|archive-url=https://web.archive.org/web/20200306120524/https://venturebeat.com/2019/11/06/neural-magic-raises-15-million-to-boost-ai-training-speed-on-off-the-shelf-processors/|archive-date=2020-03-06|access-date=2020-03-14}}</ref> designed to accelerate [[artificial intelligence]] applications, especially [[artificial neural network]]s, [[machine vision]] and [[machine learning]]. Typical applications include algorithms for [[robotics]], [[internet of things]] and other [[data (computing)|data]]-intensive or sensor-driven tasks.<ref>
{{cite web|url=http://www.eetimes.com/document.asp?doc_id=1329715|title=Google Developing AI Processors}}Google using its own AI accelerators.
</ref> They are often [[manycore]] designs and generally focus on [[Precision (computer science)#LOW|low-precision]] arithmetic, novel [[dataflow architecture]]s or [[in-memory computing]] capability.<ref name="MEMRISTOR_PIM">"[https://www.academia.edu/36504841/A_Survey_of_ReRAM-based_Architectures_for_Processing-in-memory_and_Neural_Networks A Survey of ReRAM-based Architectures for Processing-in-memory and Neural Networks]", S. Mittal, Machine Learning and Knowledge Extraction, 2018</ref> {{As of|2018}}, a typical AI [[integrated circuit]] chip [[transistor count|contains billions]] of [[MOSFET]] transistors.<ref name="computerhistory2018">{{cite web |title=13 Sextillion & Counting: The Long & Winding Road to the Most Frequently Manufactured Human Artifact in History |url=https://www.computerhistory.org/atchm/13-sextillion-counting-the-long-winding-road-to-the-most-frequently-manufactured-human-artifact-in-history/ |date=April 2, 2018 |website=[[Computer History Museum]] |access-date=28 July 2019}}</ref>
A number of vendor-specific terms exist for devices in this category, and it is an [[Emerging technologies|emerging technology]] without a [[dominant design]].

== History ==
[[Computer system]]s have frequently complemented the [[Central processing unit|CPU]] with special purpose accelerators for specialized tasks, known as [[coprocessor]]s. Notable [[Application-specific integrated circuit|application-specific]] [[Expansion card|hardware units]] include [[video card]]s for [[computer graphics|graphic]]s, [[sound card]]s, [[graphics processing unit]]s and [[digital signal processors]]. As [[deep learning]] and [[artificial intelligence]] workloads rose in prominence in the 2010s, specialized hardware units were developed or adapted from existing products to [[Hardware acceleration|accelerate]] these tasks.

=== Early attempts ===
As early as 1993, [[digital signal processor]]s were used as neural network accelerators e.g. to accelerate [[optical character recognition]] software.<ref>{{Cite web|url=https://www.youtube.com/watch?v=FwFduRA_L6Q|title=convolutional neural network demo from 1993 featuring DSP32 accelerator}}</ref> In the 1990s, there were also attempts to create parallel high-throughput systems for workstations aimed at various applications, including neural network simulations.<ref name="krste">{{Cite web|url=http://people.eecs.berkeley.edu/~krste/papers/cns-injs1993.ps|title=design of a connectionist network supercomputer}}</ref><ref name="krste general purpose">{{cite web|title=The end of general purpose computers (not)|url=https://www.youtube.com/watch?v=VtJthbiiTBQ}}This presentation covers a past attempt at neural net accelerators, notes the similarity to the modern SLI GPGPU processor setup, and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN's are just dense and sparse matrices, one of several recurring algorithms)</ref><ref>{{cite book|doi=10.1109/IPPS.1995.395862|title = Proceedings of 9th International Parallel Processing Symposium|pages=774–781|year = 1995|last1 = Ramacher|first1 = U.|last2=Raab|first2=W.|last3=Hachmann|first3=J.A.U.|last4=Beichter|first4=J.|last5=Bruls|first5=N.|last6=Wesseling|first6=M.|last7=Sicheneder|first7=E.|last8=Glass|first8=J.|last9=Wurz|first9=A.|last10=Manner|first10=R.|isbn=978-0-8186-7074-9|citeseerx = 10.1.1.27.6410}}</ref> [[Field-programmable gate array|FPGA]]-based accelerators were also first explored in the 1990s for both inference<ref name="fpga-inference">{{Cite web|url=https://www.researchgate.net/publication/2318589|title=Space Efficient Neural Net Implementation}}</ref> and training.<ref name="fpga-training">{{cite book|chapter=A Generic Building Block for Hopfield Neural Networks with On-Chip Learning|year=1996|doi=10.1109/ISCAS.1996.598474|s2cid=17630664|title=1996 IEEE International Symposium on Circuits and Systems. Circuits and Systems Connecting the World. ISCAS 96|last1=Gschwind|first1=M.|last2=Salapura|first2=V.|last3=Maischberger|first3=O.|pages=49–52|isbn=0-7803-3073-0}}</ref>  ANNA was a neural net [[CMOS]] accelerator developed by [[Yann LeCun]].<ref>{{Cite web|url=http://yann.lecun.com/exdb/publis/pdf/saeckinger-92.pdf|title=Application of the ANNA Neural Network Chip to High-Speed Character Recognition}}</ref>

=== Heterogeneous computing ===
[[Heterogeneous computing]] refers to incorporating a number of specialized processors in a single system, or even a single chip, each optimized for a specific type of task. Architectures such as the [[Cell (microprocessor)|Cell microprocessor]]<ref name="cell">{{cite journal|title=Synergistic Processing in Cell's Multicore Architecture|year=2006|doi=10.1109/MM.2006.41|s2cid=17834015|last1=Gschwind|first1=Michael|last2=Hofstee|first2=H. Peter|last3=Flachs|first3=Brian|last4=Hopkins|first4=Martin|last5=Watanabe|first5=Yukio|last6=Yamazaki|first6=Takeshi|journal=IEEE Micro|volume=26|issue=2|pages=10–24}}</ref> have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic, [[dataflow]] architecture, and prioritizing 'throughput' over latency. The Cell microprocessor was subsequently applied to a number of tasks<ref>{{cite journal|title=Performance of Cell processor for biomolecular simulations|journal=Computer Physics Communications|volume=176|issue=11–12|pages=660–664|arxiv=physics/0611201|doi=10.1016/j.cpc.2007.02.107|year=2007|last1=De Fabritiis|first1=G.}}</ref><ref>{{cite journal|title=Video Processing and Retrieval on Cell architecture|citeseerx=10.1.1.138.5133}}</ref><ref>{{cite book|doi=10.1109/RT.2006.280210|title = 2006 IEEE Symposium on Interactive Ray Tracing|pages=15–23|year = 2006|last1 = Benthin|first1 = Carsten|last2=Wald|first2=Ingo|last3=Scherbaum|first3=Michael|last4=Friedrich|first4=Heiko|isbn=978-1-4244-0693-7|citeseerx = 10.1.1.67.8982}}</ref> including AI.<ref>{{Cite web|url=https://www.teco.edu/~scholz/papers/ScholzDiploma.pdf|title=Development of an artificial neural network on a heterogeneous multicore architecture to predict a successful weight loss in obese individuals}}</ref><ref>{{cite book|doi=10.1109/ccnc08.2007.235|title = 2008 5th IEEE Consumer Communications and Networking Conference|pages=1030–1034|year = 2008|last1 = Kwon|first1 = Bomjun|last2=Choi|first2=Taiho|last3=Chung|first3=Heejin|last4=Kim|first4=Geonho|isbn=978-1-4244-1457-4}}</ref><ref>{{cite book|doi=10.1007/978-3-540-85451-7_71|title = Euro-Par 2008 – Parallel Processing|volume = 5168|pages = 665–675|series = Lecture Notes in Computer Science|year = 2008|last1 = Duan|first1 = Rubing|last2 = Strey|first2 = Alfred|isbn = 978-3-540-85450-0}}</ref>

In the [[2000s (decade)|2000s]], [[Central processing unit|CPU]]s also gained increasingly wide [[SIMD]] units, driven by video and gaming workloads; as well as support for [[packed]] low precision [[data type]]s.<ref>
{{cite web|title=Improving the performance of video with AVX|url=https://software.intel.com/en-us/articles/improving-the-compute-performance-of-video-processing-software-using-avx-advanced-vector-extensions-instructions|date=2012-02-08}}</ref>

=== Use of GPU ===
[[Graphics processing unit]]s or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and [[Graphics pipeline|image manipulation]] are similar, [[embarrassingly parallel]] tasks involving matrices, leading GPUs to become increasingly used for machine learning tasks.<ref>{{Cite web|url=https://hal.inria.fr/inria-00112631/document|title=microsoft research/pixel shaders/MNIST}}</ref><ref>{{Cite web|url=http://igoro.com/archive/how-gpu-came-to-be-used-for-general-computation/|title=How GPU came to be used for general computation}}</ref><ref>{{Cite web|url=https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf|title=imagenet classification with deep convolutional neural networks}}</ref> {{As of|2016}}, GPUs are popular for AI work, and they continue to evolve in a direction to facilitate deep learning, both for training<ref>{{cite web|title=nvidia driving the development of deep learning|url=http://insidehpc.com/2016/05/nvidia-driving-the-development-of-deep-learning/|date=2016-05-17}}</ref> and inference in devices such as [[self-driving car]]s.<ref>{{cite web|title=nvidia introduces supercomputer for self driving cars|url=http://gas2.org/2016/01/06/nvidia-introduces-supercomputer-for-self-driving-cars/|date=2016-01-06}}</ref> GPU developers such as Nvidia [[NVLink]] are developing additional connective capability for the kind of dataflow workloads AI benefits from.<ref>{{cite web|title=how nvlink will enable faster easier multi GPU computing|url=https://devblogs.nvidia.com/parallelforall/how-nvlink-will-enable-faster-easier-multi-gpu-computing/|date=2014-11-14}}</ref> As GPUs have been increasingly applied to AI acceleration, GPU manufacturers have incorporated [[neural network]] [[Application-specific integrated circuit|specific]] hardware to further accelerate these tasks.<ref>"[https://www.researchgate.net/publication/329802520_A_Survey_on_Optimized_Implementation_of_Deep_Learning_Models_on_the_NVIDIA_Jetson_Platform A Survey on Optimized Implementation of Deep Learning Models on the NVIDIA Jetson Platform]", 2019</ref><ref name="CUDA9">{{Cite web| first = Mark | last = Harris | url = https://devblogs.nvidia.com/parallelforall/cuda-9-features-revealed/ | title = CUDA 9 Features Revealed: Volta, Cooperative Groups and More | date = May 11, 2017 | access-date = August 12, 2017}}</ref> Tensor [[Processor core|cores]] are intended to speed up the training of neural networks.<ref name="CUDA9"/>

=== Use of FPGAs ===
Deep learning frameworks are still evolving, making it hard to design custom hardware. [[Reconfigurable computing|Reconfigurable]] devices such as [[field-programmable gate array]]s (FPGA) make it easier to evolve hardware, frameworks and software [[Co-design|alongside each other]].<ref>{{Cite journal|last=Sefat|first=Md Syadus|last2=Aslan|first2=Semih|last3=Kellington|first3=Jeffrey W|last4=Qasem|first4=Apan|date=August 2019|title=Accelerating HotSpots in Deep Neural Networks on a CAPI-Based FPGA|url=https://ieeexplore.ieee.org/document/8855410|journal=2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)|pages=248–256|doi=10.1109/HPCC/SmartCity/DSS.2019.00048}}</ref><ref name="fpga-inference" /><ref name="fpga-training" /><ref>{{Cite web|url=http://www.nextplatform.com/2016/08/23/fpga-based-deep-learning-accelerators-take-asics/|title=FPGA Based Deep Learning Accelerators Take on ASICs|date=2016-08-23|website=The Next Platform|access-date=2016-09-07}}</ref>

Microsoft has used FPGA chips to accelerate [[inference]].<ref>{{Cite web|title=Project Brainwave|url=https://www.microsoft.com/en-us/research/project/project-brainwave/|access-date=2020-06-16|website=Microsoft Research|language=en-US}}</ref> The application of FPGAs to AI acceleration motivated [[Intel]] to acquire [[Altera]] with the aim of integrating FPGAs in server CPUs, which would be capable of accelerating AI as well as [[General purpose computer|general purpose]] tasks.<ref name="CNNFPGAsurvey">"[https://www.academia.edu/37491583/A_Survey_of_FPGA-based_Accelerators_for_Convolutional_Neural_Networks A Survey of FPGA-based Accelerators for Convolutional Neural Networks]", Mittal et al., NCAA, 2018</ref>

=== Emergence of dedicated AI accelerator ASICs ===
While GPUs and FPGAs perform far better{{Quantify|date=October 2018}} than CPUs for AI related tasks, a factor of up to 10 in efficiency<ref>{{Cite web|url=http://techreport.com/news/30155/google-boosts-machine-learning-with-its-tensor-processing-unit|title=Google boosts machine learning with its Tensor Processing Unit|date=2016-05-19|access-date=2016-09-13}}</ref><ref>{{Cite web|url=https://www.sciencedaily.com/releases/2016/02/160203134840.htm|title=Chip could bring deep learning to mobile devices|date=2016-02-03|website=www.sciencedaily.com|access-date=2016-09-13}}</ref> may be gained with a more specific design, via an [[application-specific integrated circuit]] (ASIC).{{citation needed|date=November 2017}} These accelerators employ strategies such as optimized [[Cache-aware model|memory use]]{{citation needed|date=November 2017}} and the use of [[minifloat|lower precision arithmetic]] to accelerate calculation and increase [[throughput]] of computation.<ref name="lowprecision">{{Cite web|url=http://jmlr.org/proceedings/papers/v37/gupta15.pdf|title=Deep Learning with Limited Numerical Precision}}</ref><ref>{{cite arXiv|title=XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks|eprint=1603.05279|last1=Rastegari|first1=Mohammad|last2=Ordonez|first2=Vicente|last3=Redmon|first3=Joseph|last4=Farhadi|first4=Ali|class=cs.CV|year=2016}}</ref> Some adopted low-precision [[Floating point format|floating-point formats]] used AI acceleration are [[half-precision floating-point format|half-precision]] and the [[bfloat16 floating-point format]].<ref>{{Cite web | title = Intel unveils Nervana Neural Net L-1000 for accelerated AI training | author = Khari Johnson | work = VentureBeat | date = 2018-05-23 | access-date = 2018-05-23 | url = https://venturebeat.com/2018/05/23/intel-unveils-nervana-neural-net-l-1000-for-accelerated-ai-training/ |quote = ...Intel will be extending bfloat16 support across our AI product lines, including Intel Xeon processors and Intel FPGAs. }}</ref><ref name="top5_Inte">{{Cite web | title = Intel Lays Out New Roadmap for AI Portfolio | author = Michael Feldman | work = TOP500 Supercomputer Sites | date = 2018-05-23 | access-date = 2018-05-23 | url = https://www.top500.org/news/intel-lays-out-new-roadmap-for-ai-portfolio/ | quote = Intel plans to support this format across all their AI products, including the Xeon and FPGA lines }}</ref><ref name="toms_Inte">{{Cite web | title = Intel To Launch Spring Crest, Its First Neural Network Processor, In 2019 | author = Lucian Armasu | work = Tom's Hardware | date = 2018-05-23 | access-date = 2018-05-23 | url = https://www.tomshardware.com/news/intel-neural-network-processor-lake-crest,37105.html | quote = Intel said that the NNP-L1000 would also support bfloat16, a numerical format that’s being adopted by all the ML industry players for neural networks. The company will also support bfloat16 in its FPGAs, Xeons, and other ML products. The Nervana NNP-L1000 is scheduled for release in 2019. }}</ref><ref name="clou_Avai">{{Cite web | title = Available TensorFlow Ops {{!}} Cloud TPU {{!}} Google Cloud | work = Google Cloud | access-date = 2018-05-23 | url = https://cloud.google.com/tpu/docs/tensorflow-ops | quote = This page lists the TensorFlow Python APIs and graph operators available on Cloud TPU. }}</ref><ref name="blog_Comp">{{Cite web | title = Comparing Google's TPUv2 against Nvidia's V100 on ResNet-50 | author = Elmar Haußmann | work = RiseML Blog | date = 2018-04-26 | access-date = 2018-05-23 | url = https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e | quote = For the Cloud TPU, Google recommended we use the bfloat16 implementation from the official TPU repository with TensorFlow 1.7.0. Both the TPU and GPU implementations make use of mixed-precision computation on the respective architecture and store most tensors with half-precision. | archive-url = https://web.archive.org/web/20180426200043/https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e | archive-date = April 26, 2018 | url-status = dead | df = mdy-all }}</ref><ref name="gith_tens">{{Cite web | title = ResNet-50 using BFloat16 on TPU | author = Tensorflow Authors | work = Google | date = 2018-02-28 | access-date = 2018-05-23 | url = https://github.com/tensorflow/tpu/tree/master/models/experimental/resnet_bfloat16 }}{{Dead link|date=April 2019 |bot=InternetArchiveBot |fix-attempted=yes }}</ref><ref name="arxiv_1711.10604">{{cite report |title= TensorFlow Distributions |author=Joshua V. Dillon |author2=Ian Langmore |author3=Dustin Tran |author4=Eugene Brevdo |author5=Srinivas Vasudevan |author6=Dave Moore |author7=Brian Patton |author8=Alex Alemi |author9=Matt Hoffman |author10=Rif A. Saurous |date= 2017-11-28 |id= Accessed 2018-05-23 |arxiv= 1711.10604 |quote= All operations in TensorFlow Distributions are numerically stable across half, single, and double floating-point precisions (as TensorFlow dtypes: tf.bfloat16 (truncated floating point), tf.float16, tf.float32, tf.float64). Class constructors have a validate_args flag for numerical asserts |bibcode= 2017arXiv171110604D }}</ref> Companies such as Facebook, Amazon and Google are all designing their own AI ASICs.<ref>{{Cite web|url=https://social.techcrunch.com/2018/04/18/facebook-has-a-new-job-posting-calling-for-chip-designers/|title=Facebook has a new job posting calling for chip designers}}</ref><ref>{{Cite web|url=https://www.ft.com/content/1c2aab18-3337-11e9-bd3a-8b2a211d90d5|title=Subscribe to read &#124; Financial Times|website=www.ft.com}}</ref>

=== In-memory computing architectures ===
{{Expand section|date=October 2018}}
In June 2017, [[IBM]] researchers announced an architecture in contrast to the [[Von Neumann architecture]] based on [[In-memory processing|in-memory computing]] and [[phase-change memory]] arrays applied to temporal [[Correlation (statistics)|correlation]] detection, intending to generalize the approach to [[heterogeneous computing]] and [[massively parallel]] systems.<ref>{{Cite journal|arxiv=1706.00511|author=Abu Sebastian |author2=Tomas Tuma |author3=Nikolaos Papandreou |author4=Manuel Le Gallo |author5=Lukas Kull |author6=Thomas Parnell |author7=Evangelos Eleftheriou|title=Temporal correlation detection using computational phase-change memory|journal=Nature Communications|volume=8 |doi=10.1038/s41467-017-01481-9 |year=2017|pmid=29062022 }}</ref> In October 2018, IBM researchers announced an architecture based on [[in-memory processing]] and [[Neuromorphic engineering|modeled on the human brain's synaptic network]] to accelerate [[Deep learning|deep neural networks]].<ref>{{Cite news|url=https://phys.org/news/2018-10-brain-inspired-architecture-advance-ai.html|title=A new brain-inspired architecture could improve how computers handle data and advance AI|date=2018-10-03|work=American Institute of Physics|access-date=2018-10-05}}</ref> The system is based on [[phase-change memory]] arrays.<ref>{{Cite arXiv|eprint=1801.06228|class=cs.ET |author=Carlos Ríos |author2=Nathan Youngblood |author3=Zengguang Cheng |author4=Manuel Le Gallo |author5=Wolfram H.P. Pernice |author6=C David Wright |author7=Abu Sebastian |author8=Harish Bhaskaran|title=In-memory computing on a photonic platform|year=2018 }}</ref>

=== In-memory computing with analog resistive memories === 
In 2019 researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on [[in-memory computing]] with analog resistive memories which performs with high efficiencies of time and energy, via conducting matrix-vector multiplication in one step with Ohm’s law and Kirchhoff’s law. The researchers showed that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations, matrix eigenvectors, and differential equations in just one step. Such an approach improves computational times drastically in comparison with conventional algorithms.<ref>{{cite journal|title=Solving matrix equations in one step with cross-point resistive arrays |year=2019 |author=Zhong Sun |author2=Giacomo Pedretti |author3=Elia Ambrosi |author4=Alessandro Bricalli |author5=Wei Wang |author6=Daniele Ielmini |journal=Proceedings of the National Academy of Sciences |volume=116 |issue=10 |pages=4123-4128 }}</ref>

=== Atomically thin semiconductors ===

In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on [[floating-gate]] [[field-effect transistor]]s (FGFETs).<ref name="atomthin">{{cite journal|title=Logic-in-memory based on an atomically thin semiconductor|year=2020|doi=10.1038/s41586-020-2861-0|last1=Marega|first1=Guilherme Migliato|last2=Zhao|first2=Yanfei|last3=Avsar|first3=Ahmet|last4=Wang|first4=Zhenyu|last5=Tripati|first5=Mukesh|last6=Radenovic|first6=Aleksandra|last7=Kis|first7=Anras|journal=Nature|volume=587|issue=2|pages=72-77}}</ref> Such atomically thin [[semiconductors]] are considered promising for energy-efficient [[machine learning]] applications, where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting [[molybdenum disulphide]].<ref name="atomthin"/>

== Nomenclature ==

As of 2016, the field is still in flux and vendors are pushing their own marketing term for what amounts to an "AI accelerator", in the hope that their designs and [[Application programming interface|API]]s will become the [[dominant design]]. There is no consensus on the boundary between these devices, nor the exact form they will take; however several examples clearly aim to fill this new space, with a fair amount of overlap in capabilities.

In the past when consumer [[graphics accelerator]]s emerged, the industry eventually adopted [[Nvidia|Nvidia']]s self-assigned term, "the GPU",<ref>{{Cite web|url=http://www.nvidia.com/object/IO_20020111_5424.html|title=NVIDIA launches the World's First Graphics Processing Unit, the GeForce 256}}</ref> as the collective noun for "graphics accelerators", which had taken many forms before settling on an overall pipeline implementing a model presented by [[Direct3D]].

== Potential applications ==
*[[Vehicular automation|Autonomous vehicles]]: Nvidia has targeted their [[Drive PX-series]] boards at this space.<ref>{{Cite web|url=https://www.nvidia.com/en-us/self-driving-cars/|title=Self-Driving Cars Technology & Solutions from NVIDIA Automotive|website=NVIDIA}}</ref>
*[[Military robot]]s
* [[Agricultural robot]]s, for example pesticide-free weed control.<ref>{{cite web|title=design of a machine vision system for weed control|url=http://abe.ufl.edu/wlee/publications/icame96.pdf|access-date=June 17, 2016|archive-url=https://web.archive.org/web/20100623062608/http://www.abe.ufl.edu/wlee/Publications/ICAME96.pdf|archive-date=June 23, 2010|url-status=dead|df=mdy-all}}</ref>
* [[Voice control]], e.g. in mobile phones, a target for [[Qualcomm Zeroth]].<ref>{{cite web|title=qualcomm research brings server class machine learning to every data devices|url=https://www.qualcomm.com/news/onq/2015/10/01/qualcomm-research-brings-server-class-machine-learning-everyday-devices-making|date=October 2015}}</ref>
* [[Machine translation]]
*[[Unmanned aerial vehicle]]s, e.g. navigation systems, e.g. the [[Movidius Myriad 2]] has been demonstrated successfully guiding autonomous drones.<ref>{{cite web|title=movidius powers worlds most intelligent drone|url=https://www.siliconrepublic.com/machines/movidius-dji-drone|date=2016-03-16}}</ref>
*[[Industrial robot]]s, increasing the range of tasks that can be automated, by adding adaptability to variable situations.
*[[Health care]], to assist with diagnoses
*[[Web search engine|Search engines]], increasing the [[Energy efficiency in computing|energy efficiency]] of [[data center]]s and ability to use increasingly advanced [[Information retrieval|queries]].
* [[Natural language processing]]

== See also ==
*[[Cognitive computer]]
*[[Neuromorphic computing]]
*[[Physical neural network]]
*[[Optical neural network]]
*[[Deep learning accelerator]]

== References ==
{{reflist|32em}}

== External links ==
* http://www.nextplatform.com/2016/04/05/nvidia-puts-accelerator-metal-pascal/
* [http://eyeriss.mit.edu Eyeriss Project]
*http://www.alphaics.ai/

{{Hardware acceleration}}

[[Category:Application-specific integrated circuits]]
[[Category:AI accelerators| ]]
[[Category:Coprocessors]]
[[Category:Computer optimization]]
[[Category:Gate arrays]]