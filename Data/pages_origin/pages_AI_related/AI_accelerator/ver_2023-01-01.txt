{{Short description|Hardware acceleration unit for artificial intelligence tasks}}
{{Use American English|date=January 2019}}
{{Use mdy dates|date=October 2021}}
An '''AI accelerator''' is a class of specialized [[hardware acceleration|hardware accelerator]]<ref>{{cite web |url=https://www.v3.co.uk/v3-uk/news/3014293/intel-unveils-movidius-compute-stick-usb-ai-accelerator |title=Intel unveils Movidius Compute Stick USB AI Accelerator |date=July 21, 2017 |access-date=August 11, 2017 |url-status=dead |archive-url=https://web.archive.org/web/20170811193632/https://www.v3.co.uk/v3-uk/news/3014293/intel-unveils-movidius-compute-stick-usb-ai-accelerator |archive-date=August 11, 2017 }}</ref> or computer system<ref>{{cite web |url=https://insidehpc.com/2017/06/inspurs-unveils-gx4-ai-accelerator/ |title=Inspurs unveils GX4 AI Accelerator |date=June 21, 2017}}</ref><ref>{{citation |title=Neural Magic raises $15 million to boost AI inferencing speed on off-the-shelf processors |last=Wiggers |first=Kyle |date=November 6, 2019 |url=https://venturebeat.com/2019/11/06/neural-magic-raises-15-million-to-boost-ai-training-speed-on-off-the-shelf-processors/ |publication-date=November 6, 2019 |orig-year=2019 |archive-url=https://web.archive.org/web/20200306120524/https://venturebeat.com/2019/11/06/neural-magic-raises-15-million-to-boost-ai-training-speed-on-off-the-shelf-processors/ |archive-date=March 6, 2020 |access-date=March 14, 2020}}</ref> designed to accelerate [[artificial intelligence]] and [[machine learning]] applications, including [[artificial neural network]]s and [[machine vision]]. Typical applications include algorithms for [[robotics]], [[internet of things]], and other [[data (computing)|data]]-intensive or sensor-driven tasks.<ref>{{cite web |url=https://www.eetimes.com/google-designing-ai-processors/ |title=Google Designing AI Processors}} Google using its own AI accelerators.</ref> They are often [[manycore]] designs and generally focus on [[precision (computer science)|low-precision]] arithmetic, novel [[dataflow architecture]]s or [[in-memory computing]] capability. {{As of|2018}}, a typical AI [[integrated circuit]] chip [[transistor count|contains billions]] of [[MOSFET]] transistors.<ref name="computerhistory2018">{{cite web |title=13 Sextillion & Counting: The Long & Winding Road to the Most Frequently Manufactured Human Artifact in History |url=https://computerhistory.org/blog/13-sextillion-counting-the-long-winding-road-to-the-most-frequently-manufactured-human-artifact-in-history/?key=13-sextillion-counting-the-long-winding-road-to-the-most-frequently-manufactured-human-artifact-in-history |date=April 2, 2018 |website=[[Computer History Museum]] |access-date=July 28, 2019}}</ref>
A number of vendor-specific terms exist for devices in this category, and it is an [[emerging technologies|emerging technology]] without a [[dominant design]].

== History ==
Computer systems have frequently complemented the [[central processing unit|CPU]] with special-purpose accelerators for specialized tasks, known as [[coprocessor]]s. Notable [[application-specific integrated circuit|application-specific]] [[expansion card|hardware units]] include [[video card]]s for [[computer graphics|graphic]]s, [[sound card]]s, [[graphics processing unit]]s and [[digital signal processor]]s. As [[deep learning]] and [[artificial intelligence]] workloads rose in prominence in the 2010s, specialized hardware units were developed or adapted from existing products to [[hardware acceleration|accelerate]] these tasks.

=== Early attempts ===
First attempts like [[Intel]]'s ETANN 80170NX<ref>John C. Dvorak: ''Intel’s 80170 chip has the theoretical intelligence of a cockroach'' in PC Magazine Volume 9 Number 10 (May 1990), p. 77, [https://archive.org/details/PC_Magazine_1990_05_29_v9n10/page/n83/mode/2up], retrieved May 16, 2021</ref> incorporated analog circuits to compute neural functions. Later all-digital chips like the Nestor/Intel [[Ni1000]] followed. As early as 1993, [[digital signal processor]]s were used as neural network accelerators to accelerate [[optical character recognition]] software.<ref>{{cite web |url=https://www.youtube.com/watch?v=FwFduRA_L6Q |title=convolutional neural network demo from 1993 featuring DSP32 accelerator|website=[[YouTube]] }}</ref> In the 1990s, there were also attempts to create parallel high-throughput systems for workstations aimed at various applications, including neural network simulations.<ref name="krste">{{cite web |url=http://people.eecs.berkeley.edu/~krste/papers/cns-injs1993.ps |title=design of a connectionist network supercomputer}}</ref><ref name="krste general purpose">{{cite web |title=The end of general purpose computers (not) | website=[[YouTube]] |url=https://www.youtube.com/watch?v=VtJthbiiTBQ}}This presentation covers a past attempt at neural net accelerators, notes the similarity to the modern SLI GPGPU processor setup, and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN's are just dense and sparse matrices, one of several recurring algorithms)</ref><ref>{{cite book |doi=10.1109/IPPS.1995.395862 |title=Proceedings of 9th International Parallel Processing Symposium |pages=774–781 |year=1995 |last1=Ramacher |first1=U. |last2=Raab |first2=W. |last3=Hachmann |first3=J.A.U. |last4=Beichter |first4=J. |last5=Bruls |first5=N. |last6=Wesseling |first6=M. |last7=Sicheneder |first7=E. |last8=Glass |first8=J. |last9=Wurz |first9=A. |last10=Manner |first10=R. |isbn=978-0-8186-7074-9 |citeseerx=10.1.1.27.6410 |s2cid=16364797}}</ref> [[field-programmable gate array|FPGA]]-based accelerators were also first explored in the 1990s for both inference<ref name="fpga-inference">{{cite web |url=https://www.researchgate.net/publication/2318589 |title=Space Efficient Neural Net Implementation}}</ref> and training.<ref name="fpga-training">{{cite book |chapter=A Generic Building Block for Hopfield Neural Networks with On-Chip Learning |year=1996 |doi=10.1109/ISCAS.1996.598474 |s2cid=17630664 |title=1996 IEEE International Symposium on Circuits and Systems. Circuits and Systems Connecting the World. ISCAS 96 |last1=Gschwind |first1=M. |last2=Salapura |first2=V. |last3=Maischberger |first3=O. |pages=49–52 |isbn=0-7803-3073-0}}</ref> [[Smartphone]]s began incorporating AI accelerators starting with the [[Qualcomm]] Snapdragon 820 in 2015.<ref>{{Cite web|title=Qualcomm Helps Make Your Mobile Devices Smarter With New Snapdragon Machine Learning Software Development Kit|url=https://www.qualcomm.com/news/releases/2016/05/02/qualcomm-helps-make-your-mobile-devices-smarter-new-snapdragon-machine|url-status=live|website=Qualcomm}}</ref><ref>{{Cite web|last=Rubin|first=Ben Fox|title=Qualcomm's Zeroth platform could make your smartphone much smarter|url=https://www.cnet.com/tech/mobile/qualcomms-zeroth-platform-could-make-your-smartphone-much-smarter/|access-date=September 28, 2021|website=CNET|language=en}}</ref>

=== Heterogeneous computing ===
[[Heterogeneous computing]] refers to incorporating a number of specialized processors in a single system, or even a single chip, each optimized for a specific type of task. Architectures such as the [[Cell (microprocessor)|Cell microprocessor]]<ref name="cell">{{cite journal |title=Synergistic Processing in Cell's Multicore Architecture |year=2006 |doi=10.1109/MM.2006.41 |s2cid=17834015 |last1=Gschwind |first1=Michael |last2=Hofstee |first2=H. Peter |last3=Flachs |first3=Brian |last4=Hopkins |first4=Martin |last5=Watanabe |first5=Yukio |last6=Yamazaki |first6=Takeshi |journal=IEEE Micro |volume=26 |issue=2 |pages=10–24}}</ref> have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic, [[dataflow architecture]], and prioritizing 'throughput' over latency. The Cell microprocessor was subsequently applied to a number of tasks<ref>{{cite journal |title=Performance of Cell processor for biomolecular simulations |journal=Computer Physics Communications |volume=176 |issue=11–12 |pages=660–664 |arxiv=physics/0611201 |doi=10.1016/j.cpc.2007.02.107 |year=2007 |last1=De Fabritiis |first1=G. |bibcode=2007CoPhC.176..660D |s2cid=13871063}}</ref><ref>{{cite book |title=Video Processing and Retrieval on Cell architecture |citeseerx=10.1.1.138.5133}}</ref><ref>{{cite book |doi=10.1109/RT.2006.280210 |title=2006 IEEE Symposium on Interactive Ray Tracing |pages=15–23 |year=2006 |last1=Benthin |first1=Carsten |last2=Wald |first2=Ingo |last3=Scherbaum |first3=Michael |last4=Friedrich |first4=Heiko |isbn=978-1-4244-0693-7 |citeseerx=10.1.1.67.8982 |s2cid=1198101}}</ref> including AI.<ref>{{cite web |url=https://www.teco.edu/~scholz/papers/ScholzDiploma.pdf |title=Development of an artificial neural network on a heterogeneous multicore architecture to predict a successful weight loss in obese individuals}}</ref><ref>{{cite book |doi=10.1109/ccnc08.2007.235 |title=2008 5th IEEE Consumer Communications and Networking Conference |pages=1030–1034 |year=2008 |last1=Kwon |first1=Bomjun |last2=Choi |first2=Taiho |last3=Chung |first3=Heejin |last4=Kim |first4=Geonho |isbn=978-1-4244-1457-4 |s2cid=14429828}}</ref><ref>{{cite book |doi=10.1007/978-3-540-85451-7_71 |title=Euro-Par 2008 – Parallel Processing |volume=5168 |pages=665–675 |series=Lecture Notes in Computer Science |year=2008 |last1=Duan |first1=Rubing |last2=Strey |first2=Alfred |isbn=978-3-540-85450-0}}</ref>

In the 2000s, [[central processing unit|CPU]]s also gained increasingly wide [[SIMD]] units, driven by video and gaming workloads; as well as support for packed low-precision [[data type]]s.<ref>{{cite web |title=Improving the performance of video with AVX |url=https://software.intel.com/content/www/us/en/develop/articles/improving-the-compute-performance-of-video-processing-software-using-avx-advanced-vector-extensions-instructions.html |date=February 8, 2012}}</ref> Due to increasing performance of CPUs, they are also being used for running AI workloads. CPUs are superior for [[deep neural network|DNNs]] with small or medium-scale parallelism, for sparse DNNs and in low-batch-size scenarios.

=== Use of GPU ===
[[Graphics processing unit]]s or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and [[graphics pipeline|image manipulation]] are similar, [[embarrassingly parallel]] tasks involving matrices, leading GPUs to become increasingly used for machine learning tasks.<ref>{{cite web |url=https://hal.inria.fr/inria-00112631/document |title=microsoft research/pixel shaders/MNIST}}</ref><ref>{{cite web |url=http://igoro.com/archive/how-gpu-came-to-be-used-for-general-computation/ |title=How GPU came to be used for general computation}}</ref><ref>{{cite web |url=https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf |title=ImageNet Classification with Deep Convolutional Neural Networks}}</ref> {{As of|2016}}, GPUs are popular for AI work, and they continue to evolve in a direction to facilitate deep learning, both for training<ref>{{cite web |title=nvidia driving the development of deep learning |url=https://insidehpc.com/2016/05/nvidia-driving-the-development-of-deep-learning/ |date=May 17, 2016}}</ref> and inference in devices such as [[self-driving car]]s.<ref>{{cite web |title=Nvidia introduces supercomputer for self driving cars |url=http://gas2.org/2016/01/06/nvidia-introduces-supercomputer-for-self-driving-cars/ |date=January 6, 2016}}</ref> GPU developers such as Nvidia [[NVLink]] are developing additional connective capability for the kind of dataflow workloads AI benefits from.<ref>{{cite web |title=how nvlink will enable faster easier multi GPU computing |url=https://developer.nvidia.com/blog/how-nvlink-will-enable-faster-easier-multi-gpu-computing/ |date=November 14, 2014}}</ref> As GPUs have been increasingly applied to AI acceleration, GPU manufacturers have incorporated [[neural network]]-[[application-specific integrated circuit|specific]] hardware to further accelerate these tasks.<ref>"[https://www.researchgate.net/publication/329802520_A_Survey_on_Optimized_Implementation_of_Deep_Learning_Models_on_the_NVIDIA_Jetson_Platform A Survey on Optimized Implementation of Deep Learning Models on the NVIDIA Jetson Platform]", 2019</ref><ref name="CUDA9">{{cite web |first=Mark |last=Harris |url=https://developer.nvidia.com/blog/cuda-9-features-revealed/ |title=CUDA 9 Features Revealed: Volta, Cooperative Groups and More |date=May 11, 2017 |access-date=August 12, 2017}}</ref> Tensor [[processor core|cores]] are intended to speed up the training of neural networks.<ref name="CUDA9"/>

=== Use of FPGAs ===
Deep learning frameworks are still evolving, making it hard to design custom hardware. [[Reconfigurable computing|Reconfigurable]] devices such as [[field-programmable gate array]]s (FPGA) make it easier to evolve hardware, frameworks, and software [[integrated design|alongside each other]].<ref>{{cite journal |last1=Sefat |first1=Md Syadus |last2=Aslan |first2=Semih |last3=Kellington |first3=Jeffrey W |last4=Qasem |first4=Apan |date=August 2019 |title=Accelerating HotSpots in Deep Neural Networks on a CAPI-Based FPGA |url=https://ieeexplore.ieee.org/document/8855410 |journal=2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS) |pages=248–256 |doi=10.1109/HPCC/SmartCity/DSS.2019.00048 |isbn=978-1-7281-2058-4 |s2cid=203656070}}</ref><ref name="fpga-inference" /><ref name="fpga-training" /><ref>{{cite web |url=http://www.nextplatform.com/2016/08/23/fpga-based-deep-learning-accelerators-take-asics/ |title=FPGA Based Deep Learning Accelerators Take on ASICs |date=August 23, 2016 |website=The Next Platform |access-date=September 7, 2016}}</ref>

Microsoft has used FPGA chips to accelerate inference.<ref>{{cite web |title=Project Brainwave |url=https://www.microsoft.com/en-us/research/project/project-brainwave/ |access-date=June 16, 2020 |website=Microsoft Research |language=en-US}}</ref>

=== Emergence of dedicated AI accelerator ASICs ===
While GPUs and FPGAs perform far better than CPUs for AI-related tasks, a factor of up to 10 in efficiency<ref>{{cite web |url=https://techreport.com/news/30155/google-boosts-machine-learning-with-its-tensor-processing-unit/ |title=Google boosts machine learning with its Tensor Processing Unit |date=May 19, 2016 |access-date=September 13, 2016}}</ref><ref>{{cite web |url=https://www.sciencedaily.com/releases/2016/02/160203134840.htm |title=Chip could bring deep learning to mobile devices |date=February 3, 2016 |website=www.sciencedaily.com |access-date=September 13, 2016}}</ref> may be gained with a more specific design, via an [[application-specific integrated circuit]] (ASIC).{{citation needed |date=November 2017}} These accelerators employ strategies such as optimized [[cache-aware model|memory use]]{{citation needed |date=November 2017}} and the use of [[minifloat|lower precision arithmetic]] to accelerate calculation and increase [[throughput]] of computation.<ref name="lowprecision">{{cite web |url=http://proceedings.mlr.press/v37/gupta15.pdf |title=Deep Learning with Limited Numerical Precision}}</ref><ref>{{cite arXiv |title=XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks |eprint=1603.05279 |last1=Rastegari |first1=Mohammad |last2=Ordonez |first2=Vicente |last3=Redmon |first3=Joseph |last4=Farhadi |first4=Ali |class=cs.CV |year=2016}}</ref> Some adopted low-precision [[floating-point format]]s used AI acceleration are [[half-precision floating-point format|half-precision]] and the [[bfloat16 floating-point format]].<ref>{{cite web |title=Intel unveils Nervana Neural Net L-1000 for accelerated AI training |author=Khari Johnson |work=VentureBeat |date=May 23, 2018 |access-date=May 23, 2018 |url=https://venturebeat.com/2018/05/23/intel-unveils-nervana-neural-net-l-1000-for-accelerated-ai-training/ |quote=...Intel will be extending bfloat16 support across our AI product lines, including Intel Xeon processors and Intel FPGAs.}}</ref><ref name="top5_Inte">{{cite web |title=Intel Lays Out New Roadmap for AI Portfolio |author=Michael Feldman |work=TOP500 Supercomputer Sites |date=May 23, 2018 |access-date=May 23, 2018 |url=https://www.top500.org/news/intel-lays-out-new-roadmap-for-ai-portfolio/ |quote=Intel plans to support this format across all their AI products, including the Xeon and FPGA lines}}</ref><ref name="toms_Inte">{{cite web |title=Intel To Launch Spring Crest, Its First Neural Network Processor, In 2019 |author=Lucian Armasu |work=Tom's Hardware |date=May 23, 2018 |access-date=May 23, 2018 |url=https://www.tomshardware.com/news/intel-neural-network-processor-lake-crest,37105.html |quote=Intel said that the NNP-L1000 would also support bfloat16, a numerical format that’s being adopted by all the ML industry players for neural networks. The company will also support bfloat16 in its FPGAs, Xeons, and other ML products. The Nervana NNP-L1000 is scheduled for release in 2019.}}</ref><ref name="clou_Avai">{{cite web |title=Available TensorFlow Ops {{!}} Cloud TPU {{!}} Google Cloud |work=Google Cloud |access-date=May 23, 2018 |url=https://cloud.google.com/tpu/docs/tensorflow-ops |quote=This page lists the TensorFlow Python APIs and graph operators available on Cloud TPU.}}</ref><ref name="blog_Comp">{{cite web |title=Comparing Google's TPUv2 against Nvidia's V100 on ResNet-50 |author=Elmar Haußmann |work=RiseML Blog |date=April 26, 2018 |access-date=May 23, 2018 |url=https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e |quote=For the Cloud TPU, Google recommended we use the bfloat16 implementation from the official TPU repository with TensorFlow 1.7.0. Both the TPU and GPU implementations make use of mixed-precision computation on the respective architecture and store most tensors with half-precision. |url-status=dead |archive-url=https://web.archive.org/web/20180426200043/https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e |archive-date=April 26, 2018 }}</ref><ref name="gith_tens">{{cite web |title=ResNet-50 using BFloat16 on TPU |author=Tensorflow Authors |work=Google |date=February 28, 2018 |access-date=May 23, 2018 |url=https://github.com/tensorflow/tpu/tree/master/models/experimental/resnet_bfloat16}}{{Dead link |date=April 2019 |bot=InternetArchiveBot |fix-attempted=yes}}</ref><ref name="arxiv_1711.10604">{{cite report |title=TensorFlow Distributions |author=Joshua V. Dillon |author2=Ian Langmore |author3=Dustin Tran |author4=Eugene Brevdo |author5=Srinivas Vasudevan |author6=Dave Moore |author7=Brian Patton |author8=Alex Alemi |author9=Matt Hoffman |author10=Rif A. Saurous |date=November 28, 2017 |id=Accessed May 23, 2018 |arxiv=1711.10604 |quote=All operations in TensorFlow Distributions are numerically stable across half, single, and double floating-point precisions (as TensorFlow dtypes: tf.bfloat16 (truncated floating point), tf.float16, tf.float32, tf.float64). Class constructors have a validate_args flag for numerical asserts |bibcode=2017arXiv171110604D}}</ref> Companies such as Google, Qualcomm, Amazon, Apple, Facebook, AMD and Samsung are all designing their own AI ASICs.<ref>{{cite web |title=Google Reveals a Powerful New AI Chip and Supercomputer |url=https://www.technologyreview.com/2017/05/17/151656/google-reveals-a-powerful-new-ai-chip-and-supercomputer/ |access-date=July 27, 2021 |website=MIT Technology Review |language=en}}</ref><ref>{{cite web |title=What to Expect From Apple's Neural Engine in the A11 Bionic SoC – ExtremeTech |url=https://www.extremetech.com/mobile/255780-apple-neural-engine-a11-bionic-soc |access-date=July 27, 2021 |website=www.extremetech.com}}</ref><ref>{{cite web |url=https://social.techcrunch.com/2018/04/18/facebook-has-a-new-job-posting-calling-for-chip-designers/ |title=Facebook has a new job posting calling for chip designers}}</ref><ref>{{cite news |title=Facebook joins Amazon and Google in AI chip race |url=https://www.ft.com/content/1c2aab18-3337-11e9-bd3a-8b2a211d90d5 |newspaper=Financial Times|date=February 18, 2019 }}</ref><ref>{{cite web |last=Amadeo |first=Ron |date=May 11, 2021 |title=Samsung and AMD will reportedly take on Apple's M1 SoC later this year |url=https://arstechnica.com/gadgets/2021/05/report-the-samsung-amd-exynos-soc-will-be-out-for-laptops-this-year/ |access-date=July 28, 2021 |website=Ars Technica |language=en-us}}</ref><ref>{{Cite web|last=Smith|first=Ryan|title=The AI Race Expands: Qualcomm Reveals "Cloud AI 100" Family of Datacenter AI Inference Accelerators for 2020|url=https://www.anandtech.com/show/14187/qualcomm-reveals-cloud-ai-100-family-of-datacenter-ai-inference-accelerators-for-2020|access-date=September 28, 2021|website=www.anandtech.com}}</ref> [[Cerebras|Cerebras Systems]] has also built a dedicated AI accelerator based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2), to support deep learning workloads.<ref>{{Cite web |last=Woodie |first=Alex |date=2021-11-01 |title=Cerebras Hits the Accelerator for Deep Learning Workloads |url=https://www.datanami.com/2021/11/01/cerebras-hits-the-accelerator-for-deep-learning-workloads/ |access-date=2022-08-03 |website=Datanami}}</ref><ref>{{Cite web |date=2021-04-20 |title=Cerebras launches new AI supercomputing processor with 2.6 trillion transistors |url=https://venturebeat.com/2021/04/20/cerebras-systems-launches-new-ai-supercomputing-processor-with-2-6-trillion-transistors/ |access-date=2022-08-03 |website=VentureBeat |language=en-US}}</ref>

=== In-memory computing architectures ===
{{Expand section |date=October 2018}}
In June 2017, [[IBM]] researchers announced an architecture in contrast to the [[Von Neumann architecture]] based on [[in-memory processing|in-memory computing]] and [[phase-change memory]] arrays applied to temporal [[Correlation (statistics)|correlation]] detection, intending to generalize the approach to [[heterogeneous computing]] and [[massively parallel]] systems.<ref>{{cite journal |arxiv=1706.00511 |author=Abu Sebastian |author2=Tomas Tuma |author3=Nikolaos Papandreou |author4=Manuel Le Gallo |author5=Lukas Kull |author6=Thomas Parnell |author7=Evangelos Eleftheriou |title=Temporal correlation detection using computational phase-change memory |journal=Nature Communications |volume=8 |doi=10.1038/s41467-017-01481-9 |year=2017 |issue=1 |page=1115 |pmid=29062022 |pmc=5653661|bibcode=2017NatCo...8.1115S }}</ref> In October 2018, IBM researchers announced an architecture based on [[in-memory processing]] and [[neuromorphic engineering|modeled on the human brain's synaptic network]] to accelerate [[deep neural network]]s.<ref>{{cite news |url=https://phys.org/news/2018-10-brain-inspired-architecture-advance-ai.html |title=A new brain-inspired architecture could improve how computers handle data and advance AI |date=October 3, 2018 |work=American Institute of Physics |access-date=October 5, 2018}}</ref> The system is based on [[phase-change memory]] arrays.<ref>{{cite journal |arxiv=1801.06228 |author=Carlos Ríos |author2=Nathan Youngblood |author3=Zengguang Cheng |author4=Manuel Le Gallo |author5=Wolfram H.P. Pernice |author6=C. David Wright |author7=Abu Sebastian |author8=Harish Bhaskaran |title=In-memory computing on a photonic platform |journal=Science Advances |year=2018|volume=5 |issue=2 |doi=10.1126/sciadv.aau5759 |bibcode=2019SciA....5.5759R |s2cid=7637801 }}</ref>

=== In-memory computing with analog resistive memories ===
In 2019, researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on [[in-memory computing]] with analog resistive memories which performs with high efficiencies of time and energy, via conducting [[matrix–vector multiplication]] in one step using Ohm's law and Kirchhoff's law. The researchers showed that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations, matrix eigenvectors, and differential equations in just one step. Such an approach improves computational times drastically in comparison with digital algorithms.<ref>{{cite journal |title=Solving matrix equations in one step with cross-point resistive arrays |year=2019 |author=Zhong Sun |author2=Giacomo Pedretti |author3=Elia Ambrosi |author4=Alessandro Bricalli |author5=Wei Wang |author6=Daniele Ielmini |journal=Proceedings of the National Academy of Sciences |volume=116 |issue=10 |pages=4123–4128 |doi=10.1073/pnas.1815682116 |pmid=30782810 |pmc=6410822|bibcode=2019PNAS..116.4123S |doi-access=free }}</ref>

=== Atomically thin semiconductors ===
In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on [[floating-gate]] [[field-effect transistor]]s (FGFETs).<ref name="atomthin">{{cite journal |title=Logic-in-memory based on an atomically thin semiconductor |year=2020 |doi=10.1038/s41586-020-2861-0 |last1=Marega |first1=Guilherme Migliato |last2=Zhao |first2=Yanfei |last3=Avsar |first3=Ahmet |last4=Wang |first4=Zhenyu |last5=Tripati |first5=Mukesh |last6=Radenovic |first6=Aleksandra |last7=Kis |first7=Anras |journal=Nature |volume=587 |issue=2 |pages=72–77 |pmid=33149289 |pmc=7116757|bibcode=2020Natur.587...72M }}</ref> Such atomically thin [[semiconductor]]s are considered promising for energy-efficient [[machine learning]] applications, where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting [[molybdenum disulfide]].<ref name="atomthin"/>

=== Integrated photonic tensor core ===
In 2021, J. Feldmann et al. proposed an integrated [[photonic]] [[hardware accelerator]] for parallel convolutional processing.<ref name="photonic">{{cite journal |title=Parallel convolutional processing using an integrated photonic tensor |year=2021 |doi=10.1038/s41586-020-03070-1 |last1=Feldmann |first1=J. |last2=Youngblood|first2=N. |last3=Karpov |first3=M. | last4=Gehring |first4=H. | display-authors=3 | journal=Nature |volume=589 |issue=2 |pages=52–58|pmid=33408373 |arxiv=2002.00281 |s2cid=211010976 }}</ref> The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through [[wavelength]] division [[multiplexing]] in conjunction with [[frequency comb]]s, and (2) extremely high data modulation speeds.<ref name="photonic"/> Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of [[Photonic integrated circuit|integrated]] [[photonics]] in data-heavy AI applications.<ref name="photonic"/>

== Nomenclature ==
As of 2016, the field is still in flux and vendors are pushing their own marketing term for what amounts to an "AI accelerator", in the hope that their designs and [[application programming interface|APIs]] will become the [[dominant design]]. There is no consensus on the boundary between these devices, nor the exact form they will take; however several examples clearly aim to fill this new space, with a fair amount of overlap in capabilities.

In the past when consumer [[graphics accelerator]]s emerged, the industry eventually adopted [[Nvidia]]'s self-assigned term, "the GPU",<ref>{{cite web |url=http://www.nvidia.com/object/IO_20020111_5424.html |title=NVIDIA launches the World's First Graphics Processing Unit, the GeForce 256}}</ref> as the collective noun for "graphics accelerators", which had taken many forms before settling on an overall [[graphics pipeline|pipeline]] implementing a model presented by [[Direct3D]].

== Potential applications ==
*[[Agricultural robot]]s, for example herbicide-free weed control.<ref>{{cite document |title=Design of a machine vision system for weed control |citeseerx = 10.1.1.7.342|url=https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.342&rep=rep1&type=pdf |access-date=July 29, 2021 |url-status=live |archive-url=https://web.archive.org/web/20100623062608/http://www.abe.ufl.edu/wlee/Publications/ICAME96.pdf|archive-date=June 23, 2010}}</ref>
*[[Vehicular automation|Autonomous vehicles]]: Nvidia has targeted their [[Drive PX-series]] boards at this application.<ref>{{cite web |url=https://www.nvidia.com/en-us/self-driving-cars/ |title=Self-Driving Cars Technology & Solutions from NVIDIA Automotive |website=NVIDIA}}</ref>
*[[Computer-aided diagnosis]]
*[[Industrial robot]]s, increasing the range of tasks that can be automated, by adding adaptability to variable situations.
*[[Machine translation]]
*[[Military robot]]s
*[[Natural language processing]]
*[[Search engine]]s, increasing the [[energy efficiency in computing|energy efficiency]] of [[data center]]s and ability to use increasingly advanced [[information retrieval|queries]].
*[[Unmanned aerial vehicle]]s, e.g. navigation systems, e.g. the [[Movidius Myriad 2]] has been demonstrated successfully guiding autonomous drones.<ref>{{cite web |title=movidius powers worlds most intelligent drone |url=https://www.siliconrepublic.com/machines/movidius-dji-drone |date=March 16, 2016}}</ref>
*[[Voice user interface]], e.g. in mobile phones, a target for Qualcomm [[Zeroth (software)|Zeroth]].<ref>{{cite web |title=Qualcomm Research brings server class machine learning to everyday devices–making them smarter [VIDEO] |url=https://www.qualcomm.com/news/onq/2015/10/01/qualcomm-research-brings-server-class-machine-learning-everyday-devices-making |date=October 2015}}</ref>

== See also ==
*[[Cognitive computer]]
*[[Deep learning processor]]
*[[Neuromorphic engineering]]
*[[Optical neural network]]
*[[Physical neural network]]

== References ==
{{Reflist|32em}}

== External links ==
*[https://www.nextplatform.com/2016/04/05/nvidia-puts-accelerator-metal-pascal/ Nvidia Puts The Accelerator To The Metal With Pascal.htm], The Next Platform
*[http://eyeriss.mit.edu/ Eyeriss Project], MIT
*https://alphaics.ai/

{{Hardware acceleration}}

[[Category:Application-specific integrated circuits]]
[[Category:AI accelerators| ]]
[[Category:Coprocessors]]
[[Category:Computer optimization]]
[[Category:Gate arrays]]