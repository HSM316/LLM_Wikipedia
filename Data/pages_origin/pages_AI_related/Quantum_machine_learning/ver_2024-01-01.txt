{{short description|Interdisciplinary research area at the intersection of quantum physics and machine learning}}
{{COI|date=September 2018}}
{{Cleanup rewrite|date=July 2023|it is excessively detailed, relies heavily on primary sources, and may not provide sufficient weight to criticisms}}
{{Quantum mechanics}}
'''Quantum machine learning''' is the integration of [[quantum algorithm]]s within [[machine learning]] programs.<ref>{{Cite journal |last=Ventura |first=Dan |date=2000 |title=Quantum Associative Memory |journal=Information Sciences |volume=124 |issue=1–4 |pages=273–296|doi=10.1016/S0020-0255(99)00101-2 |arxiv=quant-ph/9807053 |s2cid=7232952 }}</ref><ref name=":7">{{Cite journal |last=Trugenberger |first=Carlo A. |date=2001 |title=Probabilistic Quantum Memories |journal=Physical Review Letters |volume=87 |issue=6 |pages=067901|doi=10.1103/PhysRevLett.87.067901 |pmid=11497863 |arxiv=quant-ph/0012100 |bibcode=2001PhRvL..87f7901T |s2cid=23325931 }}</ref><ref name=":8">{{Cite journal |last=Trugenberger |first=Carlo A. |date=2002 |title=Quantum Pattern Recognition |journal=Quantum Information Processing |volume=1 |issue=6 |pages=471–493|doi=10.1023/A:1024022632303 |s2cid=1928001 }}</ref><ref name=":9">{{Cite journal |last=Trugenberger |first=C. A. |date=2002-12-19 |title=Phase Transitions in Quantum Pattern Recognition |url=http://dx.doi.org/10.1103/physrevlett.89.277903 |journal=Physical Review Letters |volume=89 |issue=27 |page=277903 |doi=10.1103/physrevlett.89.277903 |pmid=12513243 |arxiv=quant-ph/0204115 |bibcode=2002PhRvL..89A7903T |s2cid=33065081 |issn=0031-9007}}</ref><ref>{{cite journal |doi=10.1038/nature23474 |arxiv=1611.09347
 |title=Quantum machine learning |journal=Nature |volume=549 |pages=195–202 |year=2017 |last1=Biamonte |first1=Jacob |last2=Wittek |first2=Peter |last3=Nicola |first3=Pancotti|first4=Patrick|last4=Rebentrost|first5=Nathan|last5=Wiebe|first6=Seth|last6=Lloyd| issue=7671 | pmid=28905917 | bibcode=2017Natur.549..195B | s2cid=64536201 }}</ref><ref>{{Cite book|title=Supervised Learning with Quantum Computers|doi=10.1007/978-3-319-96424-9|series=Quantum Science and Technology|year=2018|last1=Schuld|first1=Maria|last2=Petruccione|first2=Francesco|isbn=978-3-319-96423-2}}</ref><ref name=":5">{{cite journal |doi=10.1080/00107514.2014.964942 |arxiv=1409.3097 |title=An introduction to quantum machine learning |journal=Contemporary Physics |volume=56 |issue=2 |pages=172–185 |year=2014 |last1=Schuld |first1=Maria |last2=Sinayskiy |first2=Ilya |last3=Petruccione |first3=Francesco |bibcode=2015ConPh..56..172S|citeseerx=10.1.1.740.5622 |s2cid=119263556 }}</ref><ref>{{cite book |last=Wittek |first=Peter |title=Quantum Machine Learning: What Quantum Computing Means to Data Mining |publisher=Academic Press |year=2014 |isbn=978-0-12-800953-6|url=http://www.sciencedirect.com/science/book/9780128009536}}</ref>

The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a [[quantum computer]], i.e. quantum-enhanced machine learning.<ref name="Nathan Wiebe 2014">{{Cite journal |arxiv=1401.2142 |last1=Wiebe |first1=Nathan |title=Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning |journal=Quantum Information & Computation |volume=15 |issue=3 |pages=0318–0358 |last2=Kapoor |first2=Ashish |last3=Svore |first3=Krysta|author3-link= Krysta Svore |year=2014}}</ref><ref>{{cite arXiv |eprint=1307.0411 |last1=Lloyd |first1=Seth |title=Quantum algorithms for supervised and unsupervised machine learning |last2=Mohseni |first2=Masoud |last3=Rebentrost |first3=Patrick |class=quant-ph |year=2013}}</ref><ref>{{Cite journal |arxiv=1303.6055 |last1=Yoo |first1=Seokwon |title=A quantum speedup in machine learning: Finding a N-bit Boolean function for a classification |journal=New Journal of Physics |volume=16 |issue=10 |pages=103014 |last2=Bang |first2=Jeongho |last3=Lee |first3=Changhyoup |last4=Lee |first4=Jinhyoung |year=2014 |doi=10.1088/1367-2630/16/10/103014|bibcode=2014NJPh...16j3014Y |s2cid=4956424 }}</ref> While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes [[qubit]]s and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program.<ref name=":12">{{Cite journal|last1=Schuld|first1=Maria|last2=Sinayskiy|first2=Ilya|last3=Petruccione|first3=Francesco|date=2014-10-15|title=An introduction to quantum machine learning|journal=Contemporary Physics|language=en|volume=56|issue=2|pages=172–185|doi=10.1080/00107514.2014.964942|arxiv=1409.3097|issn=0010-7514|citeseerx=10.1.1.740.5622|bibcode=2015ConPh..56..172S|s2cid=119263556}}</ref> This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device.<ref>{{Cite journal|last1=Benedetti|first1=Marcello|last2=Realpe-Gómez|first2=John|last3=Biswas|first3=Rupak|last4=Perdomo-Ortiz|first4=Alejandro|date=2017-11-30|title=Quantum-Assisted Learning of Hardware-Embedded Probabilistic Graphical Models|arxiv=1609.02542|journal=Physical Review X|volume=7|issue=4|pages=041052|doi=10.1103/PhysRevX.7.041052|issn=2160-3308|bibcode=2017PhRvX...7d1052B|s2cid=55331519}}</ref><ref name="Farhi">{{cite arXiv|last1=Farhi|first1=Edward|last2=Neven|first2=Hartmut|date=2018-02-16|title=Classification with Quantum Neural Networks on Near Term Processors|eprint=1802.06002|class=quant-ph}}</ref><ref>{{cite journal|last1=Schuld|first1=Maria|last2=Bocharov|first2=Alex|last3=Svore|first3=Krysta|author3-link= Krysta Svore |last4=Wiebe|first4=Nathan|title=Circuit-centric quantum classifiers|journal=Physical Review A|year=2020|volume=101|issue=3|pages=032308|doi=10.1103/PhysRevA.101.032308|arxiv=1804.00633|bibcode=2020PhRvA.101c2308S|s2cid=49577148}}</ref> These routines can be more complex in nature and executed faster on a quantum computer.<ref name=":5" /> Furthermore, quantum algorithms can be used to analyze [[quantum state]]s instead of classical data.<ref>{{cite journal|last1=Yu|first1=Shang|last2=Albarran-Arriagada|first2=F.|last3=Retamal|first3=J. C.|last4=Wang|first4=Yi-Tao|last5=Liu|first5=Wei|last6=Ke|first6=Zhi-Jin|last7=Meng|first7=Yu|last8=Li|first8=Zhi-Peng|last9=Tang|first9=Jian-Shun|date=2018-08-28|title=Reconstruction of a Photonic Qubit State with Quantum Reinforcement Learning|arxiv=1808.09241|doi=10.1002/qute.201800074|journal=Advanced Quantum Technologies|volume=2|issue=7–8|page=1800074|s2cid=85529734}}</ref><ref>{{cite journal|last1=Ghosh|first1=Sanjib|last2=Opala|first2=A.|last3=Matuszewski|first3=M.|last4=Paterek|first4= T.|last5= Liew|first5= Timothy C. H.|date=2019|title=Quantum reservoir processing|arxiv=1811.10335|doi=10.1038/s41534-019-0149-8|journal= npj Quantum Information|volume=5|pages=35|number=35|bibcode=2019npjQI...5...35G|s2cid=119197635}}</ref>

Beyond quantum computing, the term "quantum machine learning" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. [[Machine learning in physics|machine learning of quantum systems]]), such as learning the phase transitions of a quantum system<ref>{{cite arXiv|last1=Broecker|first1=Peter|last2=Assaad|first2=Fakher F.|last3=Trebst|first3=Simon|date=2017-07-03|title=Quantum phase recognition via unsupervised machine learning|eprint=1707.00663|class=cond-mat.str-el}}</ref><ref name=":10">{{Cite journal|last1=Huembeli|first1=Patrick|last2=Dauphin|first2=Alexandre|last3=Wittek|first3=Peter|year=2018|title=Identifying Quantum Phase Transitions with Adversarial Neural Networks|arxiv=1710.08382|journal=Physical Review B|volume=97|issue=13|pages=134109|doi=10.1103/PhysRevB.97.134109|issn=2469-9950|bibcode=2018PhRvB..97m4109H|s2cid=125593239 }}</ref> or creating new quantum experiments.<ref name="Krenn 090405">{{Cite journal|last=Krenn|first=Mario|date=2016-01-01|title=Automated Search for new Quantum Experiments|journal=Physical Review Letters|volume=116|issue=9|pages=090405|arxiv=1509.02749|bibcode=2016PhRvL.116i0405K|doi=10.1103/PhysRevLett.116.090405|pmid=26991161|s2cid=20182586}}</ref><ref name="Knott 073033">{{Cite journal|last=Knott|first=Paul|date=2016-03-22|title=A search algorithm for quantum state engineering and metrology|journal=New Journal of Physics|volume=18|issue=7|pages=073033|arxiv=1511.05327|bibcode=2016NJPh...18g3033K|doi=10.1088/1367-2630/18/7/073033|s2cid=2721958}}</ref><ref name=":6">{{Cite journal|last1=Dunjko|first1=Vedran|last2=Briegel|first2=Hans J|date=2018-06-19|title=Machine learning & artificial intelligence in the quantum domain: a review of recent progress|journal=Reports on Progress in Physics|volume=81|issue=7|pages=074001|doi=10.1088/1361-6633/aab406|pmid=29504942|arxiv=1709.02779|issn=0034-4885|bibcode=2018RPPh...81g4001D|hdl=1887/71084|s2cid=3681629|hdl-access=free}}</ref>

Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.<ref>{{cite journal|last1=Huggins|first1=William|last2=Patel|first2=Piyush|last3=Whaley|first3=K. Birgitta|last4=Stoudenmire|first4=E. Miles|date=2018-03-30|title=Towards Quantum Machine Learning with Tensor Networks|journal=Quantum Science and Technology|volume=4|issue=2|pages=024001|arxiv=1803.11537|doi=10.1088/2058-9565/aaea94|s2cid=4531946}}</ref><ref>{{cite journal|last1=Carleo|first1=Giuseppe|last2=Nomura|first2=Yusuke|last3=Imada|first3=Masatoshi|date=2018-02-26|title=Constructing exact representations of quantum many-body systems with deep neural networks|journal=Nature Communications|volume=9|issue=1|pages=5322|arxiv=1802.09558|doi=10.1038/s41467-018-07520-3|pmid=30552316|pmc=6294148|bibcode=2018NatCo...9.5322C}}</ref><ref>{{cite arXiv|last=Bény|first=Cédric|date=2013-01-14|title=Deep learning and the renormalization group|eprint=1301.3124 |class=quant-ph}}</ref>

Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as "quantum learning theory".<ref>{{cite arXiv|last1=Arunachalam|first1=Srinivasan|last2=de Wolf|first2=Ronald|date=2017-01-24|title=A Survey of Quantum Learning Theory|eprint=1701.06806|class=quant-ph}}</ref><ref>{{Cite journal|last1=Sergioli|first1=Giuseppe|last2=Giuntini|first2=Roberto|last3=Freytes|first3=Hector|date=2019-05-09|title=A new Quantum approach to binary classification|journal=PLOS ONE|volume=14|issue=5|doi=10.1371/journal.pone.0216224|page=e0216224|pmid=31071129|pmc=6508868|bibcode=2019PLoSO..1416224S|doi-access=free}}</ref>

[[File:Qml approaches.tif|thumb|Four different approaches to combine the disciplines of quantum computing and machine learning.<ref name="AimeurEtAl_2006">{{Cite book|last1=Aïmeur|first1=Esma|last2=Brassard|first2=Gilles|last3=Gambs|first3=Sébastien|title=Advances in Artificial Intelligence |chapter=Machine Learning in a Quantum World |date=2006-06-07|isbn=978-3-540-34628-9|series=Lecture Notes in Computer Science|volume=4013|pages=[https://archive.org/details/advancesinartifi0000cana/page/431 431–442]|language=en|doi=10.1007/11766247_37|chapter-url=https://archive.org/details/advancesinartifi0000cana/page/431}}</ref><ref name="DunjkoTaylorBriegel">{{Cite journal|last1=Dunjko|first1=Vedran|last2=Taylor|first2=Jacob M.|last3=Briegel|first3=Hans J.|date=2016-09-20|title=Quantum-Enhanced Machine Learning|journal=Physical Review Letters|volume=117|issue=13|pages=130501|arxiv=1610.08251|bibcode=2016PhRvL.117m0501D|doi=10.1103/PhysRevLett.117.130501|pmid=27715099|s2cid=12698722}}</ref> The first letter refers to whether the system under study is classical or quantum, while the second letter defines whether a classical or quantum information processing device is used.]]

== Machine learning with quantum computers ==
Quantum-enhanced machine learning refers to [[quantum algorithm]]s that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal [[quantum computer]] to be tested, others have been implemented on small-scale or special purpose quantum devices.

=== Quantum associative memories and quantum pattern recognition ===
Associative (or content-addressable memories) are able to recognize stored content on the basis of a similarity measure, rather than fixed addresses, like in random access memories. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.

Typical classical associative memories store p patterns in the <math>O(n^2)</math> interactions (synapses) of a real,  symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.

Unfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, <math>p \le O(n)</math>.

Quantum associative memories<ref name=":7" /><ref name=":8" /><ref name=":9" /> (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits. Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input. By its very quantum nature, the retrieval process is thus probabilistic. Because quantum associative memories are free from cross-talk, however, spurious memories are never generated. Correspondingly, they have a superior capacity than classical ones. The number of parameters in the unitary matrix U is <math>O(pn)</math>. One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns.

=== Linear algebra simulation with quantum amplitudes ===

A number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the [[Probability amplitude|amplitudes]] of a quantum state with the inputs and outputs of computations.<ref name="Patrick Rebentrost 2014">{{cite journal |doi=10.1103/PhysRevLett.113.130503|pmid=25302877|title=Quantum Support Vector Machine for Big Data Classification|journal=Physical Review Letters|volume=113|issue=13|pages=130503|year=2014|last1=Rebentrost|first1=Patrick|last2=Mohseni|first2=Masoud|last3=Lloyd|first3=Seth|bibcode=2014PhRvL.113m0503R|arxiv=1307.0471|hdl=1721.1/90391|s2cid=5503025}}</ref><ref name="Nathan Wiebe 2012">{{cite journal |doi=10.1103/PhysRevLett.109.050505|pmid=23006156|title=Quantum Algorithm for Data Fitting|journal=Physical Review Letters|volume=109|issue=5|pages=050505|year=2012|last1=Wiebe|first1=Nathan|last2=Braun|first2=Daniel|last3=Lloyd|first3=Seth|bibcode=2012PhRvL.109e0505W|arxiv=1204.5242|s2cid=118439810 }}</ref><ref name="Maria Schuld 2016">{{cite journal |doi=10.1103/PhysRevA.94.022342|title=Prediction by linear regression on a quantum computer|journal=Physical Review A|volume=94|issue=2|pages=022342|year=2016|last1=Schuld|first1=Maria|last2=Sinayskiy|first2=Ilya|last3=Petruccione|first3=Francesco|bibcode=2016PhRvA..94b2342S|arxiv=1601.07823|s2cid=118459345}}</ref> Since a state of <math>n</math> qubits is described by <math>2^n</math> complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose [[computational complexity|resources]] grow polynomially in the number of qubits <math>n</math>, which amounts to a logarithmic [[time complexity]] in the number of amplitudes and thereby the dimension of the input.

Many quantum machine learning algorithms in this category are based on variations of the [[quantum algorithm for linear systems of equations]]<ref>{{Cite journal |arxiv=0811.3171 |last1=Harrow |first1=Aram W. |title=Quantum algorithm for solving linear systems of equations |journal=Physical Review Letters |volume=103 |issue=15 |pages=150502 |last2=Hassidim |first2=Avinatan |last3=Lloyd |first3=Seth |year=2008 |doi=10.1103/PhysRevLett.103.150502|pmid=19905613 |bibcode=2009PhRvL.103o0502H |s2cid=5187993 }}</ref> (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse<ref>{{cite conference |chapter=Hamiltonian simulation with nearly optimal dependence on all parameters |last1=Berry |first1= Dominic W.|last2=Childs |first2=Andrew M.|last3=Kothari|first3=Robin|title=2015 IEEE 56th Annual Symposium on Foundations of Computer Science |date=2015 |publisher=IEEE |pages=792–809 |conference= 56th Annual Symposium on Foundations of Computer Science|doi=10.1109/FOCS.2015.54|arxiv=1501.01715|isbn=978-1-4673-8191-8 }}</ref> or low rank.<ref>{{cite journal |doi=10.1038/nphys3029|title=Quantum principal component analysis|journal=Nature Physics|volume=10|issue=9|pages=631|year=2014|last1=Lloyd|first1=Seth|last2=Mohseni|first2=Masoud|last3=Rebentrost|first3=Patrick|bibcode=2014NatPh..10..631L|arxiv=1307.0401|citeseerx=10.1.1.746.480|s2cid=11553314}}</ref> For reference, any known classical algorithm for [[matrix inversion]] requires a number of operations that grows [[Computational complexity of mathematical operations#Matrix algebra|more than quadratically in the dimension of the matrix]] (e.g. <math>O\mathord\left(n^{2.373}\right)</math>), but they are not restricted to sparse matrices.

Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a [[System of linear equations|linear system of equations]], for example in least-squares linear regression,<ref name="Nathan Wiebe 2012" /><ref name="Maria Schuld 2016" /> the least-squares version of [[support vector machine]]s,<ref name="Patrick Rebentrost 2014" /> and Gaussian processes.<ref name="ReferenceA">{{cite journal |arxiv=1512.03929 |last1=Zhao |first1=Zhikuan |title=Quantum assisted Gaussian process regression |journal=Physical Review A |volume=99 |issue=5 |pages=052331 |last2=Fitzsimons |first2=Jack K. |last3=Fitzsimons |first3=Joseph F. |year=2019|doi=10.1103/PhysRevA.99.052331 |bibcode=2019PhRvA..99e2331Z |s2cid=18303333 }}</ref>

A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases,<ref>{{cite journal |doi=10.1103/PhysRevA.73.012307|title=Efficient state preparation for a register of quantum bits|journal=Physical Review A|volume=73|issue=1|pages=012307|year=2006|last1=Soklakov|first1=Andrei N.|last2=Schack|first2=Rüdiger|bibcode=2006PhRvA..73a2307S|arxiv=quant-ph/0408045|s2cid=17318769}}</ref><ref>{{cite journal |doi=10.1103/PhysRevLett.100.160501|pmid=18518173|title=Quantum Random Access Memory|journal=Physical Review Letters|volume=100|issue=16|pages=160501|year=2008|last1=Giovannetti|first1=Vittorio|last2=Lloyd|first2=Seth|last3=MacCone|first3=Lorenzo|bibcode=2008PhRvL.100p0501G|arxiv=0708.1879|s2cid=570390}}</ref> this step easily hides the complexity of the task.<ref>{{Cite journal |doi=10.1038/nphys3272|title=Read the fine print|journal=Nature Physics|volume=11|issue=4|pages=291–293|year=2015|last1=Aaronson|first1=Scott|bibcode=2015NatPh..11..291A|s2cid=122167250 }}</ref><ref>{{Cite journal |doi=10.1103/PhysRevA.99.012326|title=Optimal usage of quantum random access memory in quantum machine learning
|journal=Physical Review A|volume=99|issue=1|pages=012326|year=2019|last1=Bang|first1=Jeongho|last2=Dutta|first2=Arijit|last3=Lee|first3=Seung-Woo|last4=Kim|first4=Jaewan|arxiv=1809.04814
|bibcode=2019PhRvA..99a2326B
|s2cid=62841090
}}</ref>

=== Variational quantum algorithms (VQAs) ===
VQAs are one of the most studied quantum algorithms as researchers expect that all the needed applications for the quantum computer will be using the VQAs and also VQAs seem to fulfill the expectation for gaining quantum supremacy.  VQAs is a mixed quantum-classical approach where the quantum processor prepares quantum states and measurement is made and the optimization is done by a classical computer. VQAs are considered best for NISQ as VQAs are noise tolerant compared to other algorithms and give quantum superiority with only a few hundred qubits. Researchers have studied circuit-based algorithms to solve optimization problems and find the ground state energy of complex systems, which were difficult to solve or required a large time to perform the computation using a classical computer.<ref>{{Cite web |title=Quantum Computing: The Next Big Thing For Finance By 2024 |url=https://www.realtimenewsanalysis.com/2023/06/quantum-computing-finance-2024.html |access-date=2023-06-17}}</ref><ref>{{Cite web |date=2019-06-12 |title=Classical Computing vs Quantum Computing |url=https://www.geeksforgeeks.org/classical-computing-vs-quantum-computing/ |access-date=2023-06-17 |website=GeeksforGeeks |language=en-us}}</ref>

=== Variational quantum circuits (VQCs) ===
Variational Quantum Circuits also known as Parametrized Quantum Circuits (PQCs) are based on Variational Quantum Algorithms (VQAs). VQCs consist of three parts, preparation of initial states, quantum circuit and measurement. Researchers are extensively studying VQCs, as it uses the power of quantum computation to learn in a short time and also use fewer parameters than its classical counterparts. It is theoretically and numerically proven that we can approximate non-linear functions, like those used in neural networks, on quantum circuits. Due to VQCs superiority, neural network has been replaced by VQCs in Reinforcement Learning tasks and Generative Algorithms. The intrinsic nature of quantum devices towards decoherence, random gate error and measurement errors caused to have high potential to limit the training of the variation circuits. Training the VQCs on the classical devices before employing them on quantum devices helps to overcome the problem of decoherence noise that came through the number of repetitions for training.<ref name=":2">{{Cite arXiv |last1=Peddireddy |first1=Dheeraj |last2=Bansal |first2=V. |last3=Jacob |first3=Z. |last4=Aggarwal |first4=V. |date=2022 |title=Tensor Ring Parametrized Variational Quantum Circuits for Large Scale Quantum Machine Learning |class=quant-ph |eprint=2201.08878 }}</ref><ref name=":3">{{Cite journal |last1=Griol-Barres |first1=Israel |last2=Milla |first2=Sergio |last3=Cebrián |first3=Antonio |last4=Mansoori |first4=Yashar |last5=Millet |first5=José |date=January 2021 |title=Variational Quantum Circuits for Machine Learning. An Application for the Detection of Weak Signals |journal=Applied Sciences |language=en |volume=11 |issue=14 |pages=6427 |doi=10.3390/app11146427 |issn=2076-3417|doi-access=free }}</ref><ref>{{Cite journal |last1=Chen |first1=Samuel Yen-Chi |last2=Yang |first2=Chao-Han Huck |last3=Qi |first3=Jun |last4=Chen |first4=Pin-Yu |last5=Ma |first5=Xiaoli |last6=Goan |first6=Hsi-Sheng |date=2020 |title=Variational Quantum Circuits for Deep Reinforcement Learning |url=https://ieeexplore.ieee.org/document/9144562 |journal=IEEE Access |volume=8 |pages=141007–141024 |doi=10.1109/ACCESS.2020.3010470 |s2cid=195767325 |issn=2169-3536|arxiv=1907.00397 }}</ref>

=== Quantum binary classifier ===
Pattern reorganization is one of the important tasks of machine learning, [[binary classification]] is one of the tools or algorithms to find patterns. Binary classification is used in [[supervised learning]] and in [[unsupervised learning]]. In quantum machine learning, classical bits are converted to qubits and they are mapped to Hilbert space; complex value data are used in a quantum binary classifier to use the advantage of Hilbert space.<ref name="Park 126422">{{Cite journal |last1=Park |first1=Daniel K. |last2=Blank |first2=Carsten |last3=Petruccione |first3=Francesco |date=2020-07-27 |title=The theory of the quantum kernel-based binary classifier |url=https://www.sciencedirect.com/science/article/pii/S0375960120302541 |journal=Physics Letters A |language=en |volume=384 |issue=21 |pages=126422 |doi=10.1016/j.physleta.2020.126422 |arxiv=2004.03489 |bibcode=2020PhLA..38426422P |s2cid=215238793 |issn=0375-9601}}</ref><ref name="Yi 012020">{{Cite journal |last1=Yi |first1=Teng |last2=Wang |first2=Jie |last3=Xu |first3=Fufang |date=2021-08-01 |title=Binary classification of single qubits using quantum machine learning method |journal=Journal of Physics: Conference Series |volume=2006 |issue=1 |pages=012020 |doi=10.1088/1742-6596/2006/1/012020 |s2cid=237286847 |issn=1742-6588|doi-access=free }}</ref> By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time.<ref name="Maheshwari 2022 3705–3715">{{Cite journal |last1=Maheshwari |first1=Danyal |last2=Sierra-Sosa |first2=Daniel |last3=Garcia-Zapirain |first3=Begonya |date=2022 |title=Variational Quantum Classifier for Binary Classification: Real vs Synthetic Dataset |journal=IEEE Access |volume=10 |pages=3705–3715 |doi=10.1109/ACCESS.2021.3139323 |s2cid=245614428 |issn=2169-3536|doi-access=free }}</ref>

=== Quantum machine learning algorithms based on Grover search ===
Another approach to improving classical machine learning with quantum information processing uses [[amplitude amplification]] methods based on [[Grover's algorithm|Grover's search]] algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the [[K-medians clustering|k-medians]]<ref name=":0">{{Cite journal|last1=Aïmeur|first1=Esma|last2=Brassard|first2=Gilles|last3=Gambs|first3=Sébastien|date=2013-02-01|title=Quantum speed-up for unsupervised learning|journal=Machine Learning|language=en|volume=90|issue=2|pages=261–287|doi=10.1007/s10994-012-5316-5|issn=0885-6125|doi-access=free}}</ref> and the [[k-nearest neighbour|k-nearest neighbors algorithms]].<ref name="Nathan Wiebe 2014" /> Another application is a quadratic speedup in the training of [[perceptrons|perceptron]].<ref name="wiebe2016nips">{{cite conference |arxiv=1602.04799|last1=Wiebe|first1=Nathan|title=Quantum Perceptron Models|last2=Kapoor|first2=Ashish|last3=Svore|first3=Krysta M.|author3-link= Krysta Svore |conference=Advances in Neural Information Processing Systems|volume=29|pages=3999–4007|url=https://papers.nips.cc/paper/6401-quantum-perceptron-models|bibcode=|year=2016}}</ref>

An example of amplitude amplification being used in a machine learning algorithm is Grover's search algorithm minimization. In which a subroutine uses Grover's search algorithm to find an element less than some previously defined element. This can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one. Grover's algorithm can then find an element such that our condition is met. The minimization is initialized by some random element in our data set, and iteratively does this subroutine to find the minimum element in the data set. This minimization is notably used in quantum k-medians, and it has a speed up of at least <math>O(\sqrt{n/k})</math> compared to classical versions of k-medians, where <math>n</math> is the number of data points and <math>k</math> is the number of clusters.<ref name=":0" />

Amplitude amplification is often combined with [[quantum walk]]s to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm<ref>{{cite journal |doi= 10.1038/srep00444|pmid= 22685626|pmc= 3370332|title= Google in a Quantum Network|journal= Scientific Reports|volume= 2|issue= 444|pages= 444|year= 2012|last1= Paparo|first1= Giuseppe Davide|last2= Martin-Delgado|first2= Miguel Angel|bibcode= 2012NatSR...2E.444P|arxiv= 1112.2079}}</ref> as well as the performance of reinforcement learning agents in the projective simulation framework.<ref name="paparo2014quantum">{{cite journal |doi=10.1103/PhysRevX.4.031002|title=Quantum Speedup for Active Learning Agents|journal=Physical Review X|volume=4|issue=3|pages=031002|year=2014|last1=Paparo|first1=Giuseppe Davide|last2=Dunjko|first2=Vedran|last3=Makmal|first3=Adi|last4=Martin-Delgado|first4=Miguel Angel|last5=Briegel|first5=Hans J.|bibcode=2014PhRvX...4c1002P|arxiv=1401.4997|s2cid=54652978}}</ref>

=== Quantum-enhanced reinforcement learning ===
[[Reinforcement learning]] is a branch of machine learning distinct from supervised and unsupervised learning, which also admits quantum enhancements.<ref>{{Cite journal|last2=Chen|first2=Chunlin|last3=Li|first3=Hanxiong|last4=Tarn|first4=Tzyh-Jong|year=2008|title=Quantum Reinforcement Learning|journal=IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics|volume=38|issue=5|pages=1207–1220|doi=10.1109/TSMCB.2008.925743|pmid=18784007|first1=Daoyi|last1=Dong|arxiv=0810.3828|citeseerx=10.1.1.243.5369|s2cid=17768796 }}</ref><ref name="paparo2014quantum" /><ref>{{cite arXiv|eprint=1612.05695|last1=Crawford|first1=Daniel|title=Reinforcement Learning Using Quantum Boltzmann Machines|last2=Levit|first2=Anna|last3=Ghadermarzy|first3=Navid|last4=Oberoi|first4=Jaspreet S.|last5=Ronagh|first5=Pooya|class=quant-ph|year=2018}}</ref> In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions, which allows the agent to adapt its behavior—in other words, to learn what to do in order to gain more rewards. In some situations, either because of the quantum processing capability of the agent,<ref name="paparo2014quantum" /> or due to the possibility to probe the environment in [[Quantum superposition|superpositions]],<ref name="DunjkoTaylorBriegel" /> a quantum speedup may be achieved. Implementations of these kinds of protocols have been proposed for systems of [[Trapped ion quantum computer|trapped ions]]<ref>{{Cite journal|last1=Dunjko|first1=Vedran|last2=Friis|first2=Nicolai|last3=Briegel|first3=Hans J.|date=2015-01-01|title=Quantum-enhanced deliberation of learning agents using trapped ions|journal=New Journal of Physics|language=en|volume=17|issue=2|pages=023006|doi=10.1088/1367-2630/17/2/023006|issn=1367-2630|arxiv=1407.2830|bibcode=2015NJPh...17b3006D|s2cid=119292539}}</ref> and [[Superconducting quantum computing|superconducting circuits]].<ref>{{cite journal|last1=Lamata|first1=Lucas|title=Basic protocols in quantum reinforcement learning with superconducting circuits|journal=Scientific Reports|volume=7|issue=1|pages=1609|doi=10.1038/s41598-017-01711-6|pmid=28487535|pmc=5431677|year=2017|arxiv=1701.05131|bibcode=2017NatSR...7.1609L}}</ref> A quantum speedup of the agent's internal decision-making time<ref name="paparo2014quantum" /> has been experimentally demonstrated in trapped ions,<ref name="Sriarunothai2019Quantumenhanced">{{Cite journal|last1=Sriarunothai|first1=Theeraphot |last2=Wölk|first2=Sabine |last3=Giri|first3=Gouri Shankar|last4=Friis|first4=Nicolai |last5=Dunjko|first5=Vedran |last6=Briegel|first6=Hans J.|last7=Wunderlich|first7=Christof |date=2019|title=Speeding-up the decision making of a learning agent using an ion trap quantum processor|journal=Quantum Science and Technology|language=en|volume=4|issue=1|pages=015014|doi=10.1088/2058-9565/aaef5e|issn=2058-9565|arxiv=1709.01366|bibcode=2019QS&T....4a5014S|s2cid=2429346}}</ref> while a quantum speedup of the learning time in a fully coherent (`quantum') interaction between agent and environment has been experimentally realized in a photonic setup.<ref name="SaggioEtAl2021">{{Cite journal|last1= Saggio |first1= Valeria |last2= Asenbeck |first2= Beate |last3= Hamann |first3= Arne |last4= Strömberg |first4= Teodor |last5= Schiansky |first5= Peter |last6= Dunjko |first6= Vedran |last7= Friis |first7= Nicolai |last8= Harris |first8= Nicholas C. |last9= Hochberg |first9= Michael |last10= Englund |first10= Dirk |last11= Wölk |first11= Sabine |last12= Briegel |first12= Hans J. |last13= Walther |first13= Philip |date= 10 March 2021 |title= Experimental quantum speed-up in reinforcement learning agents |journal=Nature |language=en|volume=591|issue=7849|pages=229–233 |doi=10.1038/s41586-021-03242-7 |pmid= 33692560 |pmc= 7612051 |url=https://doi.org/10.1038/s41586-021-03242-7 |issn=1476-4687|arxiv=2103.06294|bibcode= 2021Natur.591..229S |s2cid= 232185235 }}</ref>

=== Quantum annealing ===
{{Main|Quantum annealing}}
[[Quantum annealing]] is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions. This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function. The process can be distinguished from [[Simulated annealing]] by the [[Quantum tunnelling|Quantum tunneling]] process, by which particles tunnel through kinetic or potential barriers from a high state to a low state. Quantum annealing starts from a superposition of all possible states of a system, weighted equally. Then the time-dependent [[Schrödinger equation]] guides the time evolution of the system, serving to affect the amplitude of each state as time increases. Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system.

=== NISQ Circuit as Quantum Model ===
As the depth of the quantum circuit advances on [[Noisy intermediate-scale quantum era|NISQ]] devices, the noise level rises, posing a significant challenge to accurately computing costs and gradients on training models. The noise tolerance will be improved by using the quantum [[perceptron]] and the quantum algorithm on the currently accessible quantum hardware.{{Citation needed|date=January 2023}}

A regular connection of similar components known as [[neuron]]s forms the basis of even the most complex brain networks. Typically, a neuron has two operations: the inner product and an [[activation function]]. As opposed to the activation function, which is typically [[Nonlinear system|nonlinear]], the inner product is a linear process. With quantum computing, linear processes may be easily accomplished additionally,  due to the simplicity of implementation, the threshold function is preferred by the majority of quantum neurons for activation functions.{{Citation needed|date=January 2023}}

=== Quantum sampling techniques ===
Sampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include [[deep learning]], [[probabilistic programming]], and other machine learning and artificial intelligence applications.

A computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a [[Boltzmann distribution]]. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.<ref>{{cite journal |last1=Biswas |first1=Rupak |last2=Jiang |first2=Zhang |last3=Kechezi |first3=Kostya |last4=Knysh |first4=Sergey |last5=Mandrà |first5=Salvatore |last6=O’Gorman |first6=Bryan |last7=Perdomo-Ortiz |first7=Alejando |last8=Pethukov |first8=Andre |last9=Realpe-Gómez |first9=John |last10=Rieffel |first10=Eleanor|author1-link= Eleanor Rieffel |last11=Venturelli|first11=Davide|last12=Vasko|first12=Fedir|last13=Wang|first13=Zhihui |title=A NASA perspective on quantum computing: Opportunities and challenges |year=2016|doi=10.1016/j.parco.2016.11.002|journal=Parallel Computing|volume=64 |pages=81–98 |url=https://zenodo.org/record/1259293 |arxiv=1704.04836 |s2cid=27547901 }}</ref>

Some research groups have recently explored the use of quantum annealing hardware for training [[Boltzmann machine]]s and [[deep neural networks]].<ref name=Adachi2015>{{cite arXiv |last1=Adachi |first1=Steven H. |last2=Henderson |first2=Maxwell P. |date=2015 |title=Application of quantum annealing to training of deep neural networks |eprint=1510.06356 |class=quant-ph}}</ref><ref name=Benedetti2016b>{{cite journal |last1=Benedetti |first1=Marcello |last2=Realpe-Gómez |first2=John |last3=Biswas |first3=Rupak |last4=Perdomo-Ortiz |first4=Alejandro |date=2016 |title=Estimation of effective temperatures in quantum annealers for sampling applications: A case study with possible applications in deep learning |journal=Physical Review A |doi=10.1103/PhysRevA.94.022308 |volume=94 |issue=2 |pages=022308 |bibcode=2016PhRvA..94b2308B|arxiv=1510.07611 |s2cid=118602077 }}</ref><ref name="William G 1611">{{cite arXiv |last1=Korenkevych |first1=Dmytro |last2=Xue |first2=Yanbo |last3=Bian |first3=Zhengbing |last4=Chudak |first4=Fabian |last5=Macready |first5=William G. |last6=Rolfe |first6=Jason |last7=Andriyash |first7=Evgeny |date=2016 |title=Benchmarking quantum hardware for training of fully visible Boltzmann machines |eprint=1611.04528 |class=quant-ph}}</ref> The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard [[Gibbs sampling|sampling]] techniques, such as [[Markov chain Monte Carlo]] algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.

The D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures.<ref name=Benedetti2016b /> Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks.<ref name=Adachi2015 /> The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets.<ref name=Benedetti2016a /> In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward. Reverse annealing has been used as well to solve a fully connected quantum restricted Boltzmann machine.<ref>{{cite journal|doi=10.1002/qute.202000133 |issn=2511-9044 |title=Quantum Semantic Learning by Reverse Annealing of an Adiabatic Quantum Computer|year=2021|last1=Rocutto |first1=Lorenzo|last2=Destri |first2=Claudio|last3= Prati|first3=Enrico |journal= Advanced Quantum Technologies |volume=4|issue=2|page=2000133 |arxiv=2003.11945|s2cid=214667224}}</ref>

Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed.<ref>{{Cite journal |last1=Amin |first1=Mohammad H. |last2=Andriyash |first2=Evgeny |last3=Rolfe |first3=Jason |last4=Kulchytskyy |first4=Bohdan |last5=Melko |first5=Roger |year=2018 |title=Quantum Boltzmann machines |journal=Phys. Rev. X |volume=8 |issue=21050 |pages=021050 |arxiv=1601.02036 |doi=10.1103/PhysRevX.8.021050 |bibcode=2018PhRvX...8b1050A |s2cid=119198869 }}</ref> Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.<ref name=Benedetti2016a>{{Cite journal|last1=Benedetti |first1=Marcello |last2=Realpe-Gómez |first2=John |last3=Biswas |first3=Rupak |last4=Perdomo-Ortiz |first4=Alejandro |year=2017 |title=Quantum-assisted learning of graphical models with arbitrary pairwise connectivity |journal=Physical Review X |volume=7 |issue=4 |pages=041052 |arxiv=1609.02542 |doi=10.1103/PhysRevX.7.041052 |bibcode=2017PhRvX...7d1052B |s2cid=55331519 }}</ref><ref name="William G 1611" /><ref>{{Cite web|url=http://pre.aps.org/abstract/PRE/v72/i2/e026701|archive-url=https://archive.today/20140113104409/http://pre.aps.org/abstract/PRE/v72/i2/e026701|url-status=dead|archive-date=2014-01-13|title=Phys. Rev. E 72, 026701 (2005): Quantum annealing in a kinetically co…|date=2014-01-13|website=archive.is|access-date=2018-12-07}}</ref>

Quantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing.<ref>{{cite arXiv |last1=Wiebe |first1=Nathan |last2=Kapoor |first2=Ashish |last3=Svore |first3=Krysta M.|author3-link= Krysta Svore |date=2014 |title=Quantum deep learning |eprint=1412.3489 |class=quant-ph}}</ref> The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced [[Markov logic network]]s exploit the symmetries and the locality structure of the [[Graphical model|probabilistic graphical model]] generated by a [[first-order logic]] template.<ref>{{cite journal |last1=Wittek |first1=Peter |last2=Gogolin |first2=Christian |date=2017 |title=Quantum Enhanced Inference in Markov Logic Networks |journal=Scientific Reports|doi=10.1038/srep45672|pmid=28422093 |pmc=5395824 |volume=7|issue=45672 |page=45672 |arxiv=1611.08104|bibcode=2017NatSR...745672W}}</ref><ref name=":10" /> This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.

=== Quantum neural networks ===
{{Main|Quantum neural network|}}

Quantum analogues or generalizations of classical neural nets are often referred to as [[quantum neural network]]s. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models. Quantum neural networks are often defined as an expansion on Deutsch's model of a quantum computational network.<ref name=":13">{{Cite journal|date=2001-11-01|title=Quantum Neural Networks|journal=Journal of Computer and System Sciences|language=en|volume=63|issue=3|pages=355–383|doi=10.1006/jcss.2001.1769|issn=0022-0000|last1=Gupta|first1=Sanjay|last2=Zia|first2=R.K.P.|arxiv=quant-ph/0201144|s2cid=206569020}}</ref> Within this model, nonlinear and irreversible gates, dissimilar to the Hamiltonian operator, are deployed to speculate the given data set.<ref name=":13" /> Such gates make certain phases unable to be observed and generate specific oscillations.<ref name=":13" /> Quantum neural networks apply the principals quantum information and quantum computation to classical neurocomputing.<ref name=":03">{{Citation|last1=Ezhov|first1=Alexandr A.|title=Quantum Neural Networks|date=2000|work=Future Directions for Intelligent Systems and Information Sciences|pages=213–235|publisher=Physica-Verlag HD|language=en|doi=10.1007/978-3-7908-1856-7_11|isbn=978-3-7908-2470-4|last2=Ventura|first2=Dan|series=Studies in Fuzziness and Soft Computing |volume=45 |citeseerx=10.1.1.683.5972|s2cid=9099722}}</ref> Current research shows that QNN can exponentially increase the amount of computing power and the degrees of freedom for a computer, which is limited for a classical computer to its size.<ref name=":03" /> A quantum neural network has computational capabilities to decrease the number of steps, qubits used, and computation time.<ref name=":13" /> The wave function to quantum mechanics is the neuron for Neural networks. To test quantum applications in a neural network, quantum dot molecules are deposited on a substrate of GaAs or similar to record how they communicate with one another. Each quantum dot can be referred as an island of electric activity, and when such dots are close enough (approximately 10 - 20&nbsp;nm)<ref name=":23">{{Cite journal|date=2000-10-01|title=Simulations of quantum neural networks|journal=Information Sciences|language=en|volume=128|issue=3–4|pages=257–269|doi=10.1016/S0020-0255(00)00056-6|issn=0020-0255|last1=Behrman|first1=E.C.|last2=Nash|first2=L.R.|last3=Steck|first3=J.E.|last4=Chandrashekar|first4=V.G.|last5=Skinner|first5=S.R.}}</ref> electrons can tunnel underneath the islands. An even distribution across the substrate in sets of two create dipoles and ultimately two spin states, up or down. These states are commonly known as qubits with corresponding states of <math>|0\rangle</math>  and <math>|1\rangle</math> in Dirac notation.<ref name=":23" />

===Quantum Convolution Neural Network===

A novel design for multi-dimensional vectors that uses circuits as convolution filters<ref>{{Cite journal |last1=Henderson |first1=Maxwell |last2=Shakya |first2=Samriddhi |last3=Pradhan |first3=Shashindra |last4=Cook |first4=Tristan |date=2020-02-27 |title=Quanvolutional neural networks: powering image recognition with quantum circuits |url=http://dx.doi.org/10.1007/s42484-020-00012-y |journal=Quantum Machine Intelligence |volume=2 |issue=1 |doi=10.1007/s42484-020-00012-y |arxiv=1904.04767 |s2cid=104291950 |issn=2524-4906}}</ref> is QCNN. It was inspired by the advantages of CNNs<ref>{{Cite book |last=Gaikwad |first=Akash S. |url=http://worldcat.org/oclc/1197735354 |title=Pruning convolution neural network (SqueezeNet) for efficient hardware deployment |oclc=1197735354}}</ref><ref name=":02">{{Cite journal |last1=Cong |first1=Iris |last2=Choi |first2=Soonwon |last3=Lukin |first3=Mikhail D. |date=2019-08-26 |title=Quantum convolutional neural networks |url=http://dx.doi.org/10.1038/s41567-019-0648-8 |journal=Nature Physics |volume=15 |issue=12 |pages=1273–1278 |doi=10.1038/s41567-019-0648-8 |arxiv=1810.03787 |bibcode=2019NatPh..15.1273C |s2cid=53642483 |issn=1745-2473}}</ref> and the power of QML. It is made using a combination of a variational quantum circuit(VQC)<ref>{{Cite journal |last1=Mitarai |first1=K. |last2=Negoro |first2=M. |last3=Kitagawa |first3=M. |last4=Fujii |first4=K. |date=2018-09-10 |title=Quantum circuit learning |url=http://dx.doi.org/10.1103/physreva.98.032309 |journal=Physical Review A |volume=98 |issue=3 |page=032309 |doi=10.1103/physreva.98.032309 |arxiv=1803.00745 |bibcode=2018PhRvA..98c2309M |hdl=11094/77645 |s2cid=117542570 |issn=2469-9926}}</ref> and a [[deep neural network]]<ref>{{Cite journal |last1=Hochreiter |first1=Sepp |last2=Schmidhuber |first2=Jürgen |date=1997-11-01 |title=Long Short-Term Memory |url=http://dx.doi.org/10.1162/neco.1997.9.8.1735 |journal=Neural Computation |volume=9 |issue=8 |pages=1735–1780 |doi=10.1162/neco.1997.9.8.1735 |pmid=9377276 |s2cid=1915014 |issn=0899-7667}}</ref>(DNN), fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits. The main strategy is to carry out an iterative optimization process in the [[Noisy intermediate-scale quantum era|NISQ]]<ref>{{Cite journal |last=Preskill |first=John |date=2018-08-06 |title=Quantum Computing in the NISQ era and beyond |url=http://dx.doi.org/10.22331/q-2018-08-06-79 |journal=Quantum |volume=2 |pages=79 |doi=10.22331/q-2018-08-06-79 |arxiv=1801.00862 |bibcode=2018Quant...2...79P |s2cid=44098998 |issn=2521-327X}}</ref> devices, without the negative impact of noise, which is possibly incorporated into the circuit parameter, and without the need for quantum error correction.<ref>{{Citation |last=Bacon |first=Dave |title=Experimental quantum error correction |date=2013-09-12 |url=http://dx.doi.org/10.1017/cbo9781139034807.023 |work=Quantum Error Correction |pages=509–518 |publisher=Cambridge University Press |doi=10.1017/cbo9781139034807.023 |isbn=9780521897877 |access-date=2022-11-23}}</ref>

The quantum circuit must effectively handle spatial information in order for QCNN to function as CNN. The convolution filter is the most basic technique for making use of spatial information. One or more quantum convolutional filters make up a quantum convolutional neural network (QCNN), and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way. Three parts  that make up the quantum convolutional filter are:  the encoder, the parameterized quantum circuit (PQC),<ref>{{Cite journal |last1=Bharti |first1=Kishor |last2=Cervera-Lierta |first2=Alba |last3=Kyaw |first3=Thi Ha |last4=Haug |first4=Tobias |last5=Alperin-Lea |first5=Sumner |last6=Anand |first6=Abhinav |last7=Degroote |first7=Matthias |last8=Heimonen |first8=Hermanni |last9=Kottmann |first9=Jakob S. |last10=Menke |first10=Tim |last11=Mok |first11=Wai-Keong |last12=Sim |first12=Sukin |last13=Kwek |first13=Leong-Chuan |last14=Aspuru-Guzik |first14=Alán |date=2022-02-15 |title=Noisy intermediate-scale quantum algorithms |url=http://dx.doi.org/10.1103/revmodphys.94.015004 |journal=Reviews of Modern Physics |volume=94 |issue=1 |page=015004 |doi=10.1103/revmodphys.94.015004 |arxiv=2101.08448 |bibcode=2022RvMP...94a5004B |hdl=10356/161272 |s2cid=231662441 |issn=0034-6861}}</ref> and the measurement. The quantum convolutional filter can be seen as an extension of the filter in the traditional CNN because it was designed with trainable parameters.

Quantum neural networks take advantage of the hierarchical structures,<ref>{{Cite journal |last1=Grant |first1=Edward |last2=Benedetti |first2=Marcello |last3=Cao |first3=Shuxiang |last4=Hallam |first4=Andrew |last5=Lockhart |first5=Joshua |last6=Stojevic |first6=Vid |last7=Green |first7=Andrew G. |last8=Severini |first8=Simone |date=December 2018 |title=Hierarchical quantum classifiers |url=http://dx.doi.org/10.1038/s41534-018-0116-9 |journal=npj Quantum Information |volume=4 |issue=1 |page=65 |doi=10.1038/s41534-018-0116-9 |arxiv=1804.03680 |bibcode=2018npjQI...4...65G |s2cid=55479810 |issn=}}</ref> and for each subsequent layer, the number of qubits from the preceding layer is decreased by a factor of two. For n input qubits, these structure have O(log(n)) layers, allowing for shallow circuit depth. Additionally, they are able to avoid "barren plateau," one of the most significant issues with PQC-based algorithms, ensuring trainability.<ref>{{Cite journal |last1=Zhao |first1=Chen |last2=Gao |first2=Xiao-Shan |date=2021-06-04 |title=Analyzing the barren plateau phenomenon in training quantum neural networks with the ZX-calculus |url=http://dx.doi.org/10.22331/q-2021-06-04-466 |journal=Quantum |volume=5 |pages=466 |doi=10.22331/q-2021-06-04-466 |arxiv=2102.01828 |bibcode=2021Quant...5..466Z |s2cid=231786346 |issn=2521-327X}}</ref> Despite the fact that the QCNN model does not include the corresponding quantum operation, the fundamental idea of the pooling layer is also offered to assure validity. In QCNN architecture, the pooling layer is typically placed between succeeding convolutional layers. Its function is to shrink the representation's spatial size while preserving crucial features, which allows it to reduce the number of parameters, streamline network computing, and manage over-fitting.  Such process can be accomplished applying [[Tomography|full Tomography]] on the state to reduce it all the way down to one qubit and then processed it in subway. The most frequently used unit type in the [[Pooling (neural networks)|pooling layer]] is max pooling, although there are other types as well. Similar to [[Feedforward neural network|conventional feed-forward]] neural networks, the last module is a fully connected layer with full connections to all activations in the preceding layer. Translational invariance, which requires identical blocks of parameterized quantum gates within a layer, is a distinctive feature of the QCNN architecture.<ref>{{Cite journal |last1=Hur |first1=Tak |last2=Kim |first2=Leeseok |last3=Park |first3=Daniel K. |date=2022-02-10 |title=Quantum convolutional neural network for classical data classification |url=http://dx.doi.org/10.1007/s42484-021-00061-x |journal=Quantum Machine Intelligence |volume=4 |issue=1 |page=3 |doi=10.1007/s42484-021-00061-x |arxiv=2108.00661 |s2cid=236772493 |issn=2524-4906}}</ref>

==== Dissipative Quantum Neural Network ====
Dissipative QNNs (DQNNs) are constructed from layers of qubits coupled by perceptron called building blocks, which have an arbitrary unitary design. Each node in the network layer of a DQNN is given a distinct collection of qubits, and each qubit is also given a unique quantum perceptron unitary to characterize it.<ref>{{Cite journal |last1=Ostaszewski |first1=Mateusz |last2=Grant |first2=Edward |last3=Benedetti |first3=Marcello |date=2021-01-28 |title=Structure optimization for parameterized quantum circuits |journal=Quantum |volume=5 |pages=391 |doi=10.22331/q-2021-01-28-391 |bibcode=2021Quant...5..391O |s2cid=231719244 |issn=2521-327X|doi-access=free }}</ref><ref name=":1"/> The input states information are transported through the network in a feed-forward fashion, layer-to-layer transition mapping on the qubits of the two adjacent layers, as the name implies. Dissipative term also refers to the fact that the output layer is formed by the ancillary qubits while the input layers are dropped while tracing out the final layer.<ref>{{Cite book |last=J. |first=Sharma, Kunal Cerezo, M. Cincio, Lukasz Coles, Patrick |url=http://worldcat.org/oclc/1228410830 |title=Trainability of Dissipative Perceptron-Based Quantum Neural Networks |date=2020-05-25 |oclc=1228410830}}</ref> When performing a broad supervised learning task, DQNN are used to learn a unitary matrix connecting the input and output quantum states. The training data for this task consists of the quantum state and the corresponding classical labels.

Inspired by the extremely successful classical [[Generative adversarial network|Generative adversarial network(GAN)]],<ref>{{Cite arXiv |last1=Goodfellow |first1=Ian J. |last2=Pouget-Abadie |first2=Jean |last3=Mirza |first3=Mehdi |last4=Xu |first4=Bing |last5=Warde-Farley |first5=David |last6=Ozair |first6=Sherjil |last7=Courville |first7=Aaron |last8=Bengio |first8=Yoshua |date=2014-06-10 |title=Generative Adversarial Networks |class=stat.ML |eprint=1406.2661}}</ref> dissipative quantum generative adversarial network (DQGAN) is introduced for [[unsupervised learning]] of the unlabeled training data . The generator and the discriminator are the two DQNNs that make up a single DQGAN.<ref name=":1">{{Cite arXiv |last1=Beer |first1=Kerstin |last2=Müller |first2=Gabriel |date=2021-12-11 |title=Dissipative quantum generative adversarial networks |class=quant-ph |eprint=2112.06088}}</ref> The generator's goal is to create false training states that the discriminator cannot differentiate from the genuine ones, while the discriminator's objective is to separate the real training states from the fake states created by the generator. The relevant features of the training set are learned by the generator by alternate and adversarial training of the networks that aid in the production of sets that extend the training set. DQGAN has a fully quantum architecture and is trained in quantum data.

=== Hidden quantum Markov models ===
Hidden quantum Markov models<ref>{{cite book|last1=Clark|first1=Lewis A.|last2=Huang W.|first2=Wei|last3=Barlow|first3=Thomas H.|last4=Beige|first4=Almut|title=ISCS 2014: Interdisciplinary Symposium on Complex Systems |chapter=Hidden Quantum Markov Models and Open Quantum Systems with Instantaneous Feedback |series=Emergence, Complexity and Computation |editor1-last=Sanayei|editor1-first=Ali|editor2-last=Rössler|editor2-first=Otto E.|editor3-last=Zelinka|editor3-first=Ivan |volume=14|issue=14|isbn=978-3-319-10759-2|pages=131–151 |arxiv=1406.5847|doi=10.1007/978-3-319-10759-2_16 |year=2015|citeseerx=10.1.1.749.3332|s2cid=119226526}}</ref> (HQMMs) are a quantum-enhanced version of classical [[Hidden Markov model|Hidden Markov Models]] (HMMs), which are typically used to model sequential data in various fields like [[robotics]] and [[Natural-language processing|natural language processing]]. Unlike the approach taken by other quantum-enhanced machine learning algorithms, HQMMs can be viewed as models inspired by quantum mechanics that can be run on classical computers as well.<ref name=":4">{{Cite journal|last1=Srinivasan|first1=Siddarth|last2=Gordon|first2=Geoff|last3=Boots|first3=Byron|date=2018|title=Learning Hidden Quantum Markov Models|url=https://www.cc.gatech.edu/~bboots3/files/learning_hqmms.pdf|journal=Aistats}}</ref> Where classical HMMs use probability vectors to represent hidden 'belief' states, HQMMs use the quantum analogue: [[Density matrix|density matrices]]. Recent work has shown that these models can be successfully learned by maximizing the log-likelihood of the given data via classical optimization, and there is some empirical evidence that these models can better model sequential data compared to classical HMMs in practice, although further work is needed to determine exactly when and how these benefits are derived.<ref name=":4" /> Additionally, since classical HMMs are a particular kind of [[Bayesian network|Bayes net]], an exciting aspect of HQMMs is that the techniques used show how we can perform quantum-analogous [[Bayesian inference]], which should allow for the general construction of the quantum versions of [[Graphical model|probabilistic graphical models]].<ref name=":4" />

=== Fully quantum machine learning ===
In the most general case of quantum machine learning, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic.

One class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case.<ref>{{cite journal|last1=Sentís|first1=Gael|last2=Guţă|first2=Mădălin|last3=Adesso|first3=Gerardo|date=9 July 2015|title=Quantum learning of coherent states|journal=EPJ Quantum Technology|volume=2|issue=1|doi=10.1140/epjqt/s40507-015-0030-4|doi-access=free}}</ref> (This also relates to work on quantum pattern matching.<ref>{{cite journal|last1=Sasaki|first1=Masahide|last2=Carlini|first2=Alberto|date=6 August 2002|title=Quantum learning and universal quantum matching machine|journal=Physical Review A|volume=66|issue=2|pages=022303|arxiv=quant-ph/0202173|bibcode=2002PhRvA..66b2303S|doi=10.1103/PhysRevA.66.022303|s2cid=119383508}}</ref>) The problem of learning unitary transformations can be approached in a similar way.<ref>{{cite journal|last1=Bisio|first1=Alessandro|last2=Chiribella|first2=Giulio|last3=D’Ariano|first3=Giacomo Mauro|last4=Facchini|first4=Stefano|last5=Perinotti|first5=Paolo|date=25 March 2010|title=Optimal quantum learning of a unitary transformation|journal=Physical Review A|volume=81|issue=3|pages=032324|arxiv=0903.0543|bibcode=2010PhRvA..81c2324B|doi=10.1103/PhysRevA.81.032324|s2cid=119289138}}</ref>

Going beyond the specific problem of learning states and transformations, the task of [[quantum clustering|clustering]] also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum.<ref>{{Cite book|last1=Aïmeur|first1=Esma|last2=Brassard|first2=Gilles|last3=Gambs|first3=Sébastien|title=Proceedings of the 24th international conference on Machine learning |chapter=Quantum clustering algorithms |date=1 January 2007|isbn=978-1-59593-793-3|pages=1–8|citeseerx=10.1.1.80.9513|doi=10.1145/1273496.1273497|s2cid=4357684}}</ref> Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in,<ref name="DunjkoTaylorBriegel" /> where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning. Such a speedup in the reinforcement-learning paradigm has been experimentally demonstrated in a photonic setup.<ref name=" SaggioEtAl2021" />

=== Explainable quantum machine learning ===
The need for models that can be understood by humans emerges in quantum machine learning in analogy to classical machine learning and drives the research field of explainable quantum machine learning (or XQML<ref name="xqml2023">{{cite arXiv|last1=Heese|first1=Raoul|last2=Gerlach|first2=Thore|last3=Mücke|first3=Sascha|last4=Müller|first4=Sabine|last5=Jakobs|first5=Matthias|last6=Piatkowski|first6=Nico|date=22 January 2023|title=Explainable Quantum Machine Learning|eprint=2301.09138|class=quant-ph}}</ref> in analogy to [[Explainable artificial intelligence|XAI/XML]]). XQML can be considered as an alternative research direction instead of finding a quantum advantage.<ref>{{cite journal|last1=Schuld|first1=Maria|last2=Killoran|first2=Nathan|date=2 March 2022|title=Is Quantum Advantage the Right Goal for Quantum Machine Learning?|journal=PRX Quantum |volume=3 |issue=3 |page=030101 |doi=10.1103/PRXQuantum.3.030101 |arxiv=2203.01340|bibcode=2022PRXQ....3c0101S |s2cid=247222732 }}</ref> For example, XQML has been used in the context of mobile malware detection and classification.<ref>{{cite journal|last1=Mercaldo|first1=F.|last2=Ciaramella|first2=G.|last3=Iadarola|first3=G.|last4=Storto|first4=M.|last5=Martinelli|first5=F.|last6=Santone|first6=A.o|title=Towards Explainable Quantum Machine Learning for Mobile Malware Detection and Classification|journal=Applied Sciences|volume=12|number=23|year=2022|page=12025 |language=en|doi=10.3390/app122312025|doi-access=free }}</ref> Quantum [[Shapley value]]s have also been proposed to interpret gates within a circuit based on a game-theoretic approach.<ref name="xqml2023"/> For this purpose, gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest.

== Classical learning applied to quantum problems ==
{{Further|Machine learning in physics}}

The term "quantum machine learning" sometimes refers to classical machine learning performed on data from quantum systems. A basic example of this is [[quantum tomography|quantum state tomography]], where a quantum state is learned from measurement. Other applications include learning Hamiltonians<ref>{{Cite journal|last1=Cory|first1=D. G.|last2=Wiebe|first2=Nathan|last3=Ferrie|first3=Christopher|last4=Granade|first4=Christopher E.|date=2012-07-06|title=Robust Online Hamiltonian Learning|journal=New Journal of Physics|volume=14|issue=10|pages=103013|language=en|doi=10.1088/1367-2630/14/10/103013|arxiv=1207.1655|bibcode=2012NJPh...14j3013G|s2cid=9928389}}</ref> and automatically generating quantum experiments.<ref name="Krenn 090405" />

== Quantum learning theory ==
Quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical [[computational learning theory]], but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.<ref>{{cite arXiv|class=quant-ph|first2=Ronald|last2=de Wolf|title=A Survey of Quantum Learning Theory|date=2017|last1=Arunachalam|first1=Srinivasan|eprint=1701.06806}}</ref>

The starting point in learning theory is typically a concept class, a set of possible concepts. Usually a concept is a function on some domain, such as <math>\{0,1\}^n</math>. For example, the concept class could be the set of [[disjunctive normal form]] (DNF) formulas on n bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn (exactly or approximately) an unknown target concept from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it.

In active learning, a learner can make membership queries to the target concept c, asking for its value c(x) on inputs x chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of quantum exact learning, the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more.<ref name="gortlerservedioquantum">{{cite journal|last2=Gortler|first2=Steven J.|year=2004|title=Equivalences and Separations Between Quantum and Classical Learnability|journal=SIAM Journal on Computing|volume=33|issue=5|pages=1067–1092|doi=10.1137/S0097539704412910|last1=Servedio|first1=Rocco A.|citeseerx=10.1.1.69.6555}}</ref> If complexity is measured by the amount of time the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions).<ref name="gortlerservedioquantum" />

A natural model of passive learning is Valiant's [[probably approximately correct learning|probably approximately correct (PAC) learning]]. Here the learner receives random examples (x,c(x)), where x is distributed according to some unknown distribution D. The learner's goal is to output a hypothesis function h such that h(x)=c(x) with high probability when x is drawn according to D. The learner has to be able to produce such an 'approximately correct' h for every D and every target concept c in its concept class. We can consider replacing the random examples by potentially more powerful quantum examples <math>\sum_x \sqrt{D(x)}|x,c(x)\rangle</math>. In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and quantum sample complexity are the same up to constant factors.<ref>{{cite arXiv|class=quant-ph|first2=Ronald|last2=de Wolf|title=Optimal Quantum Sample Complexity of Learning Algorithms|date=2016|last1=Arunachalam|first1=Srinivasan|eprint=1607.00932}}</ref> However, for learning under some fixed distribution D, quantum examples can be very helpful, for example for learning DNF under the uniform distribution.<ref>{{cite journal|last2=Jeffrey|first2=Jackson C.|year=1999|title=Learning DNF over the Uniform Distribution Using a Quantum Example Oracle|journal=SIAM Journal on Computing|volume=28|issue=3|pages=1136–1153|doi=10.1137/S0097539795293123|last1=Nader|first1=Bshouty H.|citeseerx=10.1.1.23.5709}}</ref> When considering time complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).<ref name="gortlerservedioquantum" />

This passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis h is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.<ref>{{cite journal|first2=Gael|last2=Sentís|title=Inductive supervised quantum learning|year=2017|last1=Monràs|first1=Alex|last3=Wittek|first3=Peter|journal=Physical Review Letters|volume=118|issue=19|pages=190503|doi=10.1103/PhysRevLett.118.190503 |pmid=28548536|bibcode=2017PhRvL.118s0503M|arxiv=1605.07541|s2cid=6521971}}</ref>

== Implementations and experiments ==

The earliest experiments were conducted using the adiabatic [[D-Wave Systems|D-Wave]] quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009.<ref>{{cite web|url=http://static.googleusercontent.com/media/www.google.com/de//googleblogs/pdfs/nips_demoreport_120709_research.pdf|title=NIPS 2009 Demonstration: Binary Classification using Hardware Implementation of Quantum Annealing|publisher=Static.googleusercontent.com|access-date=26 November 2014}}</ref> Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, [[NASA]], and the [[Universities Space Research Association]] launched the [[Quantum Artificial Intelligence Lab]] which explores the use of the adiabatic D-Wave quantum computer.<ref>{{cite web|url=https://plus.google.com/+QuantumAILab|title=Google Quantum A.I. Lab Team|date=31 January 2017|website=Google Plus|access-date=31 January 2017|author=<!--Staff writer(s); no by-line.-->}}</ref><ref>{{cite web|url=https://ti.arc.nasa.gov/tech/dash/physics/quail/|title=NASA Quantum Artificial Intelligence Laboratory|date=31 January 2017|website=NASA|access-date=31 January 2017|author=<!--Staff writer(s); no by-line.-->|archive-url=https://web.archive.org/web/20170201210200/https://ti.arc.nasa.gov/tech/dash/physics/quail/|archive-date=1 February 2017|url-status=dead}}</ref> A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.<ref name="Benedetti2016a" />

Using a different annealing technology based on [[nuclear magnetic resonance]] (NMR), a quantum [[Hopfield network]] was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation.<ref>{{cite journal|last2=Neves|first2=Jorge L.|last3=Sollacher|first3=Rudolf|last4=Glaser|first4=Steffen J.|year=2009|title=Quantum pattern recognition with liquid-state nuclear magnetic resonance|journal=Physical Review A|volume=79|issue=4|pages=042321|arxiv=0802.1592|bibcode=2009PhRvA..79d2321N|doi=10.1103/PhysRevA.79.042321|last1=Neigovzen|first1=Rodion|s2cid=119115625}}</ref> NMR technology also enables universal quantum computing,{{Citation needed|date=February 2017}} and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state  quantum computer in 2015.<ref>{{cite journal|last2=Liu|first2=Xiaomei|last3=Xu|first3=Nanyang|last4=Du|first4=Jiangfeng|year=2015|title=Experimental Realization of a Quantum Support Vector Machine|journal=Physical Review Letters|volume=114|issue=14|pages=140504|arxiv=1410.1054|bibcode=2015PhRvL.114n0504L|doi=10.1103/PhysRevLett.114.140504|pmid=25910101|last1=Li|first1=Zhaokai|s2cid=119182770 }}</ref> The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the [[feature space]], the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly [[quantum tomography]] by reading out the final state in terms of direction (up/down) of the NMR signal.

Photonic implementations are attracting more attention,<ref name="WanDKGK16">{{cite journal|last1=Wan|first1=Kwok-Ho|last2=Dahlsten|first2=Oscar|last3=Kristjansson|first3=Hler|last4=Gardner|first4=Robert|last5=Kim|first5=Myungshik|year=2017|title=Quantum generalisation of feedforward neural networks|journal=npj Quantum Information|volume=3|issue=36|pages=36|arxiv=1612.01045|bibcode=2017npjQI...3...36W|doi=10.1038/s41534-017-0032-4|s2cid=51685660}}</ref> not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013.<ref>{{cite journal|last2=Soriano|first2=Miguel C.|last3=Mirasso|first3=Claudio R.|last4=Fischer|first4=Ingo|year=2013|title=Parallel photonic information processing at gigabyte per second data rates using transient states|journal=Nature Communications|volume=4|pages=1364|bibcode=2013NatCo...4.1364B|doi=10.1038/ncomms2368|pmc=3562454|pmid=23322052|last1=Brunner|first1=Daniel}}</ref> Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule.<ref>{{cite journal|last2=Mabuchi|first2=Hideo|year=2015|title=A coherent perceptron for all-optical learning|journal=EPJ Quantum Technology|volume=2|arxiv=1501.01608|doi=10.1140/epjqt/s40507-015-0023-3|last1=Tezak|first1=Nikolas|bibcode=2015arXiv150101608T|s2cid=28568346}}</ref> A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.<ref>{{cite journal|last2=Wu|first2=D.|last3=Su|first3=Z.-E.|last4=Chen|first4=M.-C.|last5=Wang|first5=X.-L.|last6=Li|first6=Li|last7=Liu|first7=N.-L.|last8=Lu|first8=C.-Y.|last9=Pan|first9=J.-W.|year=2015|title=Entanglement-Based Machine Learning on a Quantum Computer|journal=Physical Review Letters|volume=114|issue=11|pages=110504|arxiv=1409.7770|bibcode=2015PhRvL.114k0504C|doi=10.1103/PhysRevLett.114.110504|pmid=25839250|last1=Cai|first1=X.-D.|s2cid=44769024}}</ref>

Recently, based on a neuromimetic approach, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical [[memristor]].<ref>{{cite journal |doi=10.1038/srep29507 |pmid=27381511 |pmc=4933948 |title=Quantum memristors |journal=Scientific Reports |volume=6 |issue=2016 |pages=29507 |year=2016|last1=Pfeiffer |first1=P. |last2=Egusquiza |first2=I. L. |last3=Di Ventra |first3=M. |last4=Sanz |first4=M. |last5=Solano |first5=E. |bibcode=2016NatSR...629507P|arxiv=1511.02192 }}</ref> This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed,<ref>{{cite journal |doi=10.1038/srep42044|pmid= 28195193|pmc= 5307327|last1= Salmilehto|first1= J.|title= Quantum Memristors with Superconducting Circuits|last2=  Deppe|first2= F.|last3=  Di Ventra|first3= M.|last4=  Sanz|first4= M.|last5=  Solano|first5= E.|journal=Scientific Reports |volume=7 |issue= 42044|pages=42044|year= 2017|arxiv=1603.04487|bibcode=2017NatSR...742044S}}</ref> and an experiment with quantum dots performed.<ref>{{Cite journal|arxiv=1612.08409|last1=Li|first1=Ying|title=A simple and robust quantum memristor|journal=Physical Review B|volume=96|issue=7|pages=075446|last2= Holloway|first2=Gregory W.|last3= Benjamin|first3=Simon C.|last4= Briggs|first4=G. Andrew D.|last5=Baugh|first5=Jonathan|last6= Mol|first6=Jan A.|year=2017|doi=10.1103/PhysRevB.96.075446|bibcode=2017PhRvB..96g5446L|s2cid=119454549}}</ref> A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network.

Since 2016, IBM has launched an online cloud-based platform for quantum software developers, called the [[IBM Q Experience]]. This platform consists of several fully operational quantum processors accessible via the IBM Web API. In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities. New architectures are being explored on an experimental basis, up to 32 qubits, using both trapped-ion and superconductive quantum computing methods.

In October 2019, it was noted that the introduction of Quantum Random Number Generators (QRNGs) to machine learning models including Neural Networks and Convolutional Neural Networks for random initial weight distribution and Random Forests for splitting processes had a profound effect on their ability when compared to the classical method of Pseudorandom Number Generators (PRNGs).<ref>{{cite journal | last1=Bird | first1=Jordan J. | last2=Ekárt | first2=Anikó | last3=Faria | first3=Diego R. | title=On the effects of pseudorandom and quantum-random number generators in soft computing | journal=Soft Computing | publisher=Springer Science and Business Media LLC | date=2019-10-28 | issn=1432-7643 | doi=10.1007/s00500-019-04450-0 | volume=24 | issue=12 | pages=9243–9256 | doi-access=free }}</ref> However, in a more recent publication from 2021, these claims could not be reproduced for Neural Network weight initialization and no significant advantage of using QRNGs over PRNGs was found.<ref>{{cite arXiv | last1=Heese | first1=Raoul | last2=Wolter | first2=Moritz | last3=Mücke | first3=Sascha | last4=Franken | first4=Lukas | last5=Piatkowski | first5=Nico | eprint=2108.13329 | title=On the effects of biased quantum random numbers on the initialization of artificial neural networks | class=quant-ph | date=2021}}</ref> The work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non-trivial task on NISQ devices, and QRNGs are therefore typically much more difficult to use in practice than PRNGs.

A paper published in December 2018 reported on an experiment using a trapped-ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware.<ref name="Sriarunothai2019Quantumenhanced" />

In March 2021, a team of researchers from Austria, The Netherlands, the US and Germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment.<ref>{{cite news |title= A quantum trick with photons gives machine learning a speed boost |url= https://www.newscientist.com/article/2270517-a-quantum-trick-with-photons-gives-machine-learning-a-speed-boost/ |access-date=31 August 2021 |work=New Scientist |language=en}}</ref><ref name="SaggioEtAl2021" /> The relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor.

==Skepticism==

While [[machine learning]] itself is now not only a research field but an economically significant and fast growing industry and [[quantum computing]] is a well established field of both theoretical and experimental research, quantum machine learning remains a purely theoretical field of studies. Attempts to experimentally demonstrate concepts of quantum machine learning remain insufficient.{{citation needed|date=December 2020}}

Many of the leading scientists that extensively publish in the field of quantum machine learning warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future. Sophia Chen<ref>{{Cite web|date=2020-05-04|title=Can quantum machine learning move beyond its own hype?|url=https://www.protocol.com/manuals/quantum-computing/machine-learning-ai-quantum-computing-move-beyond-hype|access-date=2020-10-27|website=Protocol|language=en}}</ref> collected some of the statements made by well known scientists in the field:

* "I think we haven't done our homework yet. This is an extremely new scientific field," - physicist Maria Schuld of Canada-based quantum computing startup Xanadu.
* “When mixing machine learning with ‘quantum,’ you catalyse a hype-condensate.”<ref>{{Cite web|date=2018-01-22|title=Can quantum machine learning move beyond its own hype?|url=https://www.quantamagazine.org/job-one-for-quantum-computers-boost-artificial-intelligence-20180129/|website=quantamagazine.org|language=en}}</ref> - [[Jacob Biamonte]] a contributor to the theory of quantum computation.
* "There is a lot more work that needs to be done before claiming quantum machine learning will actually work," - computer scientist Iordanis Kerenidis, the head of quantum algorithms at the Silicon Valley-based quantum computing startup QC Ware.
* "I have not seen a single piece of evidence that there exists a meaningful [machine learning] task for which it would make sense to use a quantum computer and not a classical computer," - physicist Ryan Sweke of the Free University of Berlin in Germany.
* “Don't fall for the hype!” -  Frank Zickert,<ref>{{Cite web|last=Zickert|first=Frank|date=2020-09-23|title=Quantum Machine Learning|url=https://towardsdatascience.com/quantum-machine-learning-e1955476a6ad|access-date=2020-10-27|website=Medium|language=en}}</ref> who is the author of probably the most practical book related to the subject beware that ”quantum computers are far away from advancing machine learning for their representation ability”, and even speaking about evaluation and optimization for any kind of useful task quantum supremacy is not yet achieved. Furthermore, nobody among the active researchers in the field make any forecasts about when it could possibly become practical.{{citation needed|date=December 2020}}

== See also ==
*[[Differentiable programming]]
*[[Quantum computing]]
*[[Quantum algorithm for linear systems of equations]]
*[[Quantum annealing]]
*[[Quantum neural network]]
*[[Quantum image]]

== References ==
{{Reflist |30em}}

{{Quantum computing}}
{{Differentiable computing}}
{{emerging technologies|quantum=yes|other=yes}}

[[Category:Machine learning]]
[[Category:Quantum information science]]
[[Category:Theoretical computer science]]
[[Category:Quantum programming]]