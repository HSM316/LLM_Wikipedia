{{Short description|AI accelerator ASIC by Google}}
{{About|the chip developed by Google|the smartphone system-on-chip|Google Tensor|other devices that provide tensor processing for artificial intelligence|AI accelerator}}
{{Infobox CPU architecture
| name        = Tensor Processing Unit
| image       = Tensor Processing Unit 3.0.jpg
| caption     = Tensor Processing Unit 3.0
| designer    = [[Google]]
| bits        = 
| introduced  = 2015<ref>Jouppi et al, 2017, "In-Datacenter Performance Analysis of a Tensor Processing Unit", https://arxiv.org/abs/1704.04760</ref>
| version     = 
| design      = 
| type        = [[Artificial neural network|Neural network]]<br /> [[Machine learning]]
| encoding    = 
| branching   = 
| endianness  = 
| page size   = 
| extensions  = 
| open        = 
| predecessor = 
| successor   = 
| registers   = 
| gpr         = 
| fpr         = 
}}

'''Tensor Processing Unit''' ('''TPU''') is an [[AI accelerator]] [[application-specific integrated circuit]] (ASIC) developed by [[Google]] for [[Artificial neural network|neural network]] [[machine learning]], using Google's own [[TensorFlow]] software.<ref>{{cite web |title=Cloud Tensor Processing Units (TPUs) |url=https://cloud.google.com/tpu/docs/tpus |website=Google Cloud |access-date=20 July 2020}}</ref> Google began using TPUs internally in 2015, and in 2018 made them available for [[Third-party source|third-party]] use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.

== Comparison to CPUs and GPUs ==
Compared to a [[graphics processing unit]], TPUs are designed for a high volume of low [[Precision (computer science)|precision]] computation (e.g. as little as [[8-bit]] precision)<ref>{{Cite web |last=Armasu |first=Lucian |date=2016-05-19 |title=Google's Big Chip Unveil For Machine Learning: Tensor Processing Unit With 10x Better Efficiency (Updated) |url=http://www.tomshardware.com/news/google-tensor-processing-unit-machine-learning,31834.html |access-date=2016-06-26 |website=Tom's Hardware}}</ref> with more input/output operations per [[joule]], without hardware for rasterisation/[[texture mapping]].<ref name="GCP blog 2016">{{Cite web |last=Jouppi |first=Norm |date=May 18, 2016 |title=Google supercharges machine learning tasks with TPU custom chip |url=https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html |access-date=2017-01-22 |website=Google Cloud Platform Blog |language=en-US}}</ref> The TPU [[Application-specific integrated circuit|ASICs]] are mounted in a heatsink assembly, which can fit in a hard drive slot within a data center [[19-inch rack|rack]], according to [[Norman Jouppi]].<ref name=":0" />

Different types of processors are suited for different types of machine learning models. TPUs are well suited for [[Convolutional neural network|CNNs]], while GPUs have benefits for some fully-connected neural networks, and CPUs can have advantages for [[Recurrent neural network|RNNs]].<ref>{{cite arXiv |eprint=1907.10701 |class=cs.LG |first1=Yu Emma |last1=Wang |first2=Gu-Yeon |last2=Wei |title=Benchmarking TPU, GPU, and CPU Platforms for Deep Learning |date=2019-07-01 |last3=Brooks |first3=David}}</ref>

==History==
The tensor processing unit was announced in May 2016 at [[Google I/O]], when the company said that the TPU had already been used inside [[Google Data Centers|their data centers]] for over a year.<ref name=":0" /><ref name="GCP blog 2016" /> Google's 2017 paper describing its creation cites previous systolic matrix multipliers of similar architecture built in the 1990s.<ref name="InDatacenterPerformanceAnalysisOfATensorProcessingUnit-2017" /> The chip has been specifically designed for Google's [[TensorFlow]] framework, a symbolic math library which is used for [[machine learning]] applications such as [[neural networks]].<ref name=YoutubeClip>[https://www.youtube.com/watch?v=oZikw5k_2FM "TensorFlow: Open source machine learning"] "It is machine learning software being used for various kinds of perceptual and language understanding tasks" — Jeffrey Dean, minute 0:47 / 2:17 from Youtube clip</ref> However, as of 2017 Google still used [[Central processing unit|CPUs]] and [[Graphics processing unit|GPUs]] for other types of [[machine learning]].<ref name=":0">{{Cite news|url=http://www.techradar.com/news/computing-components/processors/google-s-tensor-processing-unit-explained-this-is-what-the-future-of-computing-looks-like-1326915|title=Google's Tensor Processing Unit explained: this is what the future of computing looks like|newspaper=TechRadar|language=en|access-date=2017-01-19}}</ref> Other [[AI accelerator]] designs are appearing from other vendors also and are aimed at [[embedded computing|embedded]] and [[robotics]] markets.

Google's TPUs are proprietary. Some models are commercially available, and on February 12, 2018, ''The New York Times'' reported that Google "would allow other companies to buy access to those chips through its cloud-computing service."<ref>{{Cite news|url=https://www.nytimes.com/2018/02/12/technology/google-artificial-intelligence-chips.html|title=Google Makes Its Special A.I. Chips Available to Others|newspaper=The New York Times|date=12 February 2018 |language=en|access-date=2018-02-12 |last1=Metz |first1=Cade }}</ref>  Google has said that they were used in the [[AlphaGo versus Lee Sedol]] series of human-versus-machine [[Go (game)|Go]] games,<ref name="GCP blog 2016" /> as well as in the [[AlphaZero]] system, which produced [[Chess]], [[Shogi]] and Go playing programs from the game rules alone and went on to beat the leading programs in those games.<ref>{{Cite web|url=https://chess24.com/en/read/news/deepmind-s-alphazero-crushes-chess|title=DeepMind's AlphaZero crushes chess|last=McGourty|first=Colin|date=6 December 2017|website=chess24.com|language=en}}</ref> Google has also used TPUs for [[Google Street View]] text processing and was able to find all the text in the Street View database in less than five days. In [[Google Photos]], an individual TPU can process over 100 million photos a day.<ref name=":0" /> It is also used in [[RankBrain]] which Google uses to provide search results.<ref>{{Cite news|url=http://www.pcworld.com/article/3072256/google-io/googles-tensor-processing-unit-said-to-advance-moores-law-seven-years-into-the-future.html|title=Google's Tensor Processing Unit could advance Moore's Law 7 years into the future|newspaper=PCWorld|language=en|access-date=2017-01-19}}</ref>

Google provides third parties access to TPUs through its ''Cloud TPU'' service as part of the [[Google Cloud Platform]]<ref>{{Cite web|title=Frequently Asked Questions {{!}} Cloud TPU|url=https://cloud.google.com/tpu/docs/faq|access-date=2021-01-14|website=Google Cloud|language=en}}</ref> and through its [[Project Jupyter|notebook-based]] services [[Kaggle]] and [[Colaboratory]].<ref>{{Cite web|title=Google Colaboratory|url=https://colab.research.google.com/notebooks/tpu.ipynb|access-date=2021-05-15|website=colab.research.google.com|language=en}}</ref><ref>{{Cite web|title=Use TPUs {{!}} TensorFlow Core|url=https://www.tensorflow.org/guide/tpu|access-date=2021-05-15|website=TensorFlow|language=en}}</ref>

==Products==

{| class="wikitable"
|+Tensor Processing Unit products<ref>{{cite conference |url=https://conferences.computer.org/iscapub/pdfs/ISCA2021-4ghucdBnCWYB7ES2Pe4YdT/333300a001/333300a001.pdf |title=Ten lessons from three generations that shaped Google's TPUv4i |author1=Jouppi, Norman P. |author2=Yoon, Doe Hyun |author3=Ashcraft, Matthew |author4=Gottscho, Mark |doi=10.1109/ISCA52012.2021.00010 |conference=International Symposium on Computer Architecture |date=June 14, 2021 |location=Valencia, Spain |isbn=978-1-4503-9086-6}}</ref><ref name=Cloud-System-Architecture>{{Cite web |title=System Architecture {{!}} Cloud TPU |url=https://cloud.google.com/tpu/docs/system-architecture-tpu-vm |access-date=2022-12-11 |website=Google Cloud |language=en}}</ref><ref name='TPU_memory'>{{cite news|last1=Kennedy|first1=Patrick|title=Case Study on the Google TPU and GDDR5 from Hot Chips 29|url=https://www.servethehome.com/case-study-google-tpu-gddr5-hot-chips-29/|access-date=23 August 2017|publisher=Serve The Home|date=22 August 2017}}</ref>
!  !! TPUv1 !! TPUv2 !! TPUv3 !! TPUv4<ref name=Cloud-System-Architecture/><ref>[https://cloud.google.com/blog/products/ai-machine-learning/google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer Stay tuned, more information on TPU v4 is coming soon], retrieved 2020-08-06.</ref> !! TPUv5e<ref name='Cloud TPU v5e Inference'>[https://cloud.google.com/tpu/docs/v5e-inference Cloud TPU v5e Inference Public Preview], retrieved 2023-11-06.</ref> !! TPUv5p<ref name='Cloud TPU v5p'>[https://cloud.google.com/tpu/docs/v5p Cloud TPU v5p] ''Google Cloud.'' retrieved 2024-04-09 </ref> <ref name='TPU v5p training'>[https://cloud.google.com/tpu/docs/v5p-training Cloud TPU v5p Training], retrieved 2024-04-09.</ref> !! '''Trillium'''<ref>{{Cite web |title=Introducing Trillium, sixth-generation TPUs |url=https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus |access-date=2024-05-29 |website=Google Cloud Blog |language=en-US}}</ref>
|-
| Date introduced || 2015 || 2017 || 2018 || 2021 || 2023 || 2023 || 2024
|-
| [[Semiconductor device fabrication|Process node]] || 28&nbsp;nm || 16&nbsp;nm|| 16&nbsp;nm || 7&nbsp;nm || Unstated || Unstated ||
|-
| [[Die (integrated circuit)|Die]] size (mm<sup>2</sup>) || 331 || < 625 || < 700 || < 400  || 300-350 || Unstated || 
|-
| On-chip memory (MiB) || 28 || 32 || 32 || 32 || 48 || 112 ||
|-
| Clock speed (MHz) || 700 || 700 || 940 || 1050 || Unstated || 1750 ||
|-
| Memory || 8 GiB [[DDR3 SDRAM|DDR3]] || 16 GiB [[High Bandwidth Memory|HBM]] || 32 GiB HBM || 32 GiB HBM || 16 GB HBM || 95 GB HBM ||32 GB ?
|-
|Memory bandwidth
|34 GB/s
|600 GB/s
|900 GB/s
|1200 GB/s
|819 GB/s
|2765 GB/s
|~1.6 TB/s ?
|-
| [[Thermal design power|TDP]] (W) || 75 || 280 || 220 || 170 || Not Listed || Not Listed ||
|-
| TOPS (Tera Operations Per Second) || 23 || 45 || 123 || 275 || 197&nbsp;(bf16) 393&nbsp;(int8) || 459&nbsp;(bf16) 918&nbsp;(int8) ||
|-
| TOPS/W || {{#expr: 23/75 round 2}} || {{#expr: 45/280 round 2}} || {{#expr: 123/220 round 2}} || {{#expr: 275/170 round 2}} || Not Listed || Not Listed ||
|}

===First generation TPU===
The first-generation TPU is an [[8-bit]] [[matrix multiplication]] engine, driven with [[CISC instruction]]s by the host processor across a [[PCI Express#PCI Express 3.0|PCIe&nbsp;3.0]] bus. It is manufactured on a 28 [[Nanometer|nm]] process with a die size ≤ 331&nbsp;[[Millimetre|mm]]<sup>2</sup>. The [[clock speed]] is 700&nbsp;[[MHz]] and it has a [[thermal design power]] of 28–40&nbsp;[[Watt|W]]. It has 28&nbsp;[[MiB]] of on chip memory, and 4&nbsp;[[MiB]] of [[32-bit]] [[accumulator (computing)|accumulator]]s taking the results of a 256×256 [[systolic array]] of 8-bit [[Binary multiplier|multiplier]]s.<ref name=InDatacenterPerformanceAnalysisOfATensorProcessingUnit-2017>
{{cite conference
	| last1 = Jouppi | first1 = Norman P.
	| last2 = Young | first2 = Cliff
	| last3 = Patil | first3 = Nishant
	| last4 = Patterson | first4 = David
	| last5 = Agrawal | first5 = Gaurav
	| last6 = Bajwa | first6 = Raminder
	| last7 = Bates | first7 = Sarah
	| last8 = Bhatia | first8 = Suresh
	| last9 = Boden | first9 = Nan
	| last10 = Borchers | first10 = Al
	| last11 = Boyle | first11 = Rick
	| last12 = Cantin | first12 = Pierre-luc
	| last13 = Chao | first13 = Clifford
	| last14 = Clark | first14 = Chris
	| last15 = Coriell | first15 = Jeremy
	| last16 = Daley | first16 = Mike
	| last17 = Dau | first17 = Matt
	| last18 = Dean | first18 = Jeffrey
	| last19 = Gelb | first19 = Ben
	| last20 = Ghaemmaghami | first20 = Tara Vazir
	| last21 = Gottipati | first21 = Rajendra
	| last22 = Gulland | first22 = William
	| last23 = Hagmann | first23 = Robert
	| last24 = Ho | first24 = C. Richard
	| last25 = Hogberg | first25 = Doug
	| last26 = Hu | first26 = John
	| last27 = Hundt | first27 = Robert
	| last28 = Hurt | first28 = Dan
	| last29 = Ibarz | first29 = Julian
	| last30 = Jaffey | first30 = Aaron
	| last31 = Jaworski | first31 = Alek
	| last32 = Kaplan | first32 = Alexander
	| last33 = Khaitan | first33 = Harshit
	| last34 = Koch | first34 = Andy
	| last35 = Kumar | first35 = Naveen
	| last36 = Lacy | first36 = Steve
	| last37 = Laudon | first37 = James
	| last38 = Law | first38 = James
	| last39 = Le | first39 = Diemthu
	| last40 = Leary | first40 = Chris
	| last41 = Liu | first41 = Zhuyuan
	| last42 = Lucke | first42 = Kyle
	| last43 = Lundin | first43 = Alan
	| last44 = MacKean | first44 = Gordon
	| last45 = Maggiore | first45 = Adriana
	| last46 = Mahony | first46 = Maire
	| last47 = Miller | first47 = Kieran
	| last48 = Nagarajan | first48 = Rahul
	| last49 = Narayanaswami | first49 = Ravi
	| last50 = Ni | first50 = Ray
	| last51 = Nix | first51 = Kathy
	| last52 = Norrie | first52 = Thomas
	| last53 = Omernick | first53 = Mark
	| last54 = Penukonda | first54 = Narayana
	| last55 = Phelps | first55 = Andy
	| last56 = Ross | first56 = Jonathan
	| last57 = Ross | first57 = Matt
	| last58 = Salek | first58 = Amir
	| last59 = Samadiani | first59 = Emad
	| last60 = Severn | first60 = Chris
	| last61 = Sizikov | first61 = Gregory
	| last62 = Snelham | first62 = Matthew
	| last63 = Souter | first63 = Jed
	| last64 = Steinberg | first64 = Dan
	| last65 = Swing | first65 = Andy
	| last66 = Tan | first66 = Mercedes
	| last67 = Thorson | first67 = Gregory
	| last68 = Tian | first68 = Bo
	| last69 = Toma | first69 = Horia
	| last70 = Tuttle | first70 = Erick
	| last71 = Vasudevan | first71 = Vijay
	| last72 = Walter | first72 = Richard
	| last73 = Wang | first73 = Walter
	| last74 = Wilcox | first74 = Eric
	| last75 = Yoon | first75 = Doe Hyun
 | title = In-Datacenter Performance Analysis of a Tensor Processing Unit™
 | date = June 26, 2017
 | location = Toronto, Canada
 | arxiv = 1704.04760
	}}
</ref> Within the TPU package is 8&nbsp;[[Gibibyte|GiB]] of [[dual-channel]] 2133&nbsp;MHz [[DDR3&nbsp;SDRAM]] offering 34&nbsp;GB/s of bandwidth.<ref name='TPU_memory'>{{cite news|last1=Kennedy|first1=Patrick|title=Case Study on the Google TPU and GDDR5 from Hot Chips 29|url=https://www.servethehome.com/case-study-google-tpu-gddr5-hot-chips-29/|access-date=23 August 2017|publisher=Serve The Home|date=22 August 2017}}</ref> Instructions transfer data to or from the host, perform matrix multiplications or [[convolution]]s, and apply [[activation function]]s.<ref name=InDatacenterPerformanceAnalysisOfATensorProcessingUnit-2017 />

===Second generation TPU===
The second-generation TPU was announced in May 2017.<ref name='TFP_v2'>{{cite news|last1=Bright|first1=Peter|title=Google brings 45 teraflops tensor flow processors to its compute cloud|url=https://arstechnica.com/information-technology/2017/05/google-brings-45-teraflops-tensor-flow-processors-to-its-compute-cloud/|access-date=30 May 2017|publisher=Ars Technica|date=17 May 2017}}</ref> Google stated the first-generation TPU design was limited by [[memory bandwidth]] and using 16 [[Gigabyte|GB]] of [[High Bandwidth Memory]] in the second-generation design increased bandwidth to 600&nbsp;GB/s and performance to 45&nbsp;tera[[FLOPS]].<ref name='TPU_memory' /> The TPUs are then arranged into four-chip modules with a performance of 180&nbsp;teraFLOPS.<ref name='TFP_v2' /> Then 64 of these modules are assembled into 256-chip pods with 11.5&nbsp;petaFLOPS of performance.<ref name='TFP_v2' /> Notably, while the first-generation TPUs were limited to integers, the second-generation TPUs can also calculate in [[Floating-point arithmetic|floating point]], introducing the [[bfloat16]] format invented by [[Google Brain]]. This makes the second-generation TPUs useful for both training and inference of machine learning models. Google has stated these second-generation TPUs will be available on the [[Google Compute Engine]] for use in TensorFlow applications.<ref>{{cite news|last1=Kennedy|first1=Patrick|title=Google Cloud TPU Details Revealed|url=https://www.servethehome.com/google-cloud-tpu-details-revealed/|access-date=30 May 2017|publisher=Serve The Home|date=17 May 2017}}</ref>

===Third generation TPU===
The third-generation TPU was announced on May 8, 2018.<ref>{{cite news|last1=Frumusanu|first1=Andre|title=Google I/O Opening Keynote Live-Blog|url=https://www.anandtech.com/show/12726/google-io-keynote-liveblog-10am-pt|access-date=9 May 2018|date=8 May 2018}}</ref> Google announced that processors themselves are twice as powerful as the second-generation TPUs, and would be deployed in pods with four times as many chips as the preceding generation.<ref>{{cite news|last1=Feldman|first1=Michael|title=Google Offers Glimpse of Third-Generation TPU Processor|url=https://www.top500.org/news/google-offers-glimpse-of-third-generation-tpu-processor/|access-date=14 May 2018|publisher=Top 500|date=11 May 2018}}</ref><ref>{{cite news|last1=Teich|first1=Paul|title=Tearing Apart Google's TPU 3.0 AI Coprocessor|url=https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/|access-date=14 May 2018|publisher=The Next Platform|date=10 May 2018}}</ref> This results in an 8-fold increase in performance per pod (with up to 1,024 chips per pod) compared to the second-generation TPU deployment.

===Fourth generation TPU===
On May 18, 2021, Google CEO Sundar Pichai spoke about TPU v4 Tensor Processing Units during his keynote at the Google I/O virtual conference. TPU v4 improved performance by more than 2x over TPU v3 chips. Pichai said "A single v4 pod contains 4,096 v4 chips, and each pod has 10x the interconnect bandwidth per chip at scale, compared to any other networking technology.”<ref>{{cite web |url=https://www.hpcwire.com/2021/05/20/google-launches-tpu-v4-ai-chips/ |title=Google Launches TPU v4 AI Chips |website=www.hpcwire.com |date= 20 May 2021|access-date=June 7, 2021}}</ref> An April 2023 paper by Google claims TPU v4 is 5-87% faster than an Nvidia [[Ampere (microarchitecture)|A100]] at machine learning [[Benchmark (computing)|benchmarks]].<ref>{{cite arXiv |last=Jouppi |first=Norman |author-link=Norman Jouppi |date=2023-04-20 |title=TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings |class=cs.AR |eprint=2304.01433}}</ref>

There is also an "inference" version, called v4i,<ref>{{cite web |url=https://www.servethehome.com/google-details-tpuv4-and-its-crazy-optically-reconfigurable-ai-network/ |title=Google Details TPUv4 and its Crazy Optically Reconfigurable AI Network
 |last=Kennedy |first=Patrick |date=2023-08-29 |website=servethehome.com |access-date=2023-12-16}}</ref> that does not require [[liquid cooling]].<ref>{{cite web |url=https://www.censtry.com/blog/why-did-google-develop-its-own-tpu-chip-in-depth-disclosure-of-team-members.html |title=Why did Google develop its own TPU chip? In-depth disclosure of team members |author=<!--Not stated--> |date=2021-10-20 |website=censtry.com |access-date=2023-12-16}}</ref>

===Fifth generation TPU===
In 2021, Google revealed the physical layout of TPU v5 is being designed with the assistance of a novel application of [[deep reinforcement learning]].<ref>{{cite journal |last1=Mirhoseini |first1=Azalia |last2=Goldie |first2=Anna |date=2021-06-01 |title=A graph placement methodology for fast chip design |url=http://176.9.41.242/doc/reinforcement-learning/model/2021-mirhoseini.pdf |journal=[[Nature (journal)|Nature]] |volume=594 |issue=7962 |pages=207–212 |doi=10.1038/s41586-022-04657-6 |pmid=35361999 |s2cid=247855593 |access-date=2023-06-04}}</ref> Google claims TPU v5 is nearly twice as fast as TPU v4,<ref>{{cite web |url=https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer |title=Enabling next-generation AI workloads: Announcing TPU v5p and AI Hypercomputer |last=Vahdat |first=Amin |date=2023-12-06 |access-date=2024-04-08}}</ref> and based on that and the relative performance of TPU v4 over A100, some speculate TPU v5 as being as fast as or faster than an [[Hopper (microarchitecture)|H100]].<ref>{{cite web |url=https://www.techradar.com/pro/google-is-rapidly-turning-into-a-formidable-opponent-to-bff-nvidia-the-tpu-v5p-ai-chip-powering-its-hypercomputer-is-faster-and-has-more-memory-and-bandwidth-than-ever-before-beating-even-the-mighty-h100 |title=Google is rapidly turning into a formidable opponent to BFF Nvidia — the TPU v5p AI chip powering its hypercomputer is faster and has more memory and bandwidth than ever before, beating even the mighty H100 |last=Afifi-Sabet |first=Keumars |date=2023-12-23 |publisher=[[TechRadar]] |access-date=2024-04-08}}</ref>

Similar to the v4i being a lighter-weight version of the v4, the fifth generation has a "cost-efficient"<ref>{{cite web |url=https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga |title=Expanding our AI-optimized infrastructure portfolio: Introducing Cloud TPU v5e and announcing A3 GA |author=<!--Not stated--> |date=2023-08-29 |access-date=2023-12-16}}</ref> version called v5e.<ref name='Cloud TPU v5e Inference' /> In December 2023, Google announced TPU v5p which is claimed to be competitive with the H100.<ref>{{cite web |url=https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer |title=Enabling next-generation AI workloads: Announcing TPU v5p and AI Hypercomputer |author=<!--Not stated--> |date=2023-12-06 |access-date=2024-04-09}}</ref>

===Sixth generation TPU===
In May 2024, at the [[Google I/O]] conference, Google announced TPU v6 which will be available later in 2024. Google claimed a 4.7 times performance increase relative to TPU v5e,<ref>{{cite web |url=https://hothardware.com/news/google-cloud-unveils-trillium-its-6th-gen-tpu-with-a-47x-performance-leap |title=Google Cloud Unveils Trillium, Its 6th-Gen TPU With A 4.7X AI Performance Leap |last=Velasco |first=Alan |date=2024-05-15 |publisher=[[HotHardware]] |access-date=2024-05-15}}</ref> via larger matrix multiplication units and an increased clock speed. High bandwidth memory (HBM) capacity and bandwidth have also doubled. A pod can contain up to 256 Trillium units.<ref>{{Cite web |title=Introducing Trillium, sixth-generation TPUs |url=https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus |access-date=2024-05-17 |website=Google Cloud Blog |language=en-US}}</ref>

===Edge TPU===
In July 2018, Google announced the Edge TPU. The Edge TPU is Google's purpose-built [[Application-specific integrated circuit|ASIC]] chip designed to run machine learning (ML) models for [[edge computing]], meaning it is much smaller and consumes far less power compared to the TPUs hosted in Google datacenters (also known as Cloud TPUs<ref>{{Cite web|title=Cloud TPU|url=https://cloud.google.com/tpu|access-date=2021-05-21|website=Google Cloud|language=en}}</ref>). In January 2019, Google made the Edge TPU available to developers with a line of products under the [[Coral (AI)|Coral]] brand. The Edge TPU is capable of 4 trillion operations per second with 2 W of electrical power.<ref>{{Cite web|url=https://coral.ai/docs/edgetpu/benchmarks/|title=Edge TPU performance benchmarks|website=Coral|language=en-us|access-date=2020-01-04}}</ref>

The product offerings include a [[single-board computer]] (SBC), a [[system on module]] (SoM), a [[USB]] accessory, a mini [[PCI-e]] card, and an [[M.2]] card. The [[single-board computer|SBC]] Coral Dev Board and Coral SoM both run Mendel Linux OS – a derivative of [[Debian]].<ref>{{Cite web|title=Dev Board|url=https://coral.ai/products/dev-board|access-date=2021-05-21|website=Coral|language=en-us}}</ref><ref>{{Cite web|title=System-on-Module (SoM)|url=https://coral.ai/products/som|access-date=2021-05-21|website=Coral|language=en-us}}</ref> The USB, PCI-e, and M.2 products function as add-ons to existing computer systems, and support Debian-based Linux systems on x86-64 and ARM64 hosts (including [[Raspberry Pi]]).

The machine learning runtime used to execute models on the Edge TPU is based on [[TensorFlow|TensorFlow Lite]].<ref>{{Cite web|url=https://www.blog.google/products/google-cloud/bringing-intelligence-to-the-edge-with-cloud-iot/|title=Bringing intelligence to the edge with Cloud IoT|date=2018-07-25|website=Google Blog|language=en-US|access-date=2018-07-25}}</ref> The Edge TPU is only capable of accelerating forward-pass operations, which means it's primarily useful for performing inferences (although it is possible to perform lightweight transfer learning on the Edge TPU<ref>{{Cite web|url=https://coral.withgoogle.com/docs/edgetpu/retrain-classification-ondevice/|title=Retrain an image classification model on-device|access-date=2019-05-03|website=Coral}}</ref>). The Edge TPU also only supports 8-bit math, meaning that for a network to be compatible with the Edge TPU, it needs to either be trained using the TensorFlow quantization-aware training technique, or since late 2019 it's also possible to use post-training quantization.

On November 12, 2019, [[Asus]] announced a pair of [[Single-board computer|single-board computer (SBCs)]] featuring the Edge TPU. The [[Asus Tinker Board|Asus Tinker Edge T and Tinker Edge R Board]] designed for [[Internet of things|IoT]] and [[Edge computing|edge]] [[AI]]. The SBCs officially support [[Android (operating system)|Android]] and [[Debian]] [[operating system]]s.<ref>{{Cite web|url=https://www.asus.com/jp/News/jr4skqts65jsuggg|title=組込み総合技術展＆IoT総合技術展「ET & IoT Technology 2019」に出展することを発表|website=Asus.com|language=ja-JP|access-date=2019-11-13}}</ref><ref>{{Cite web|url=https://www.anandtech.com/show/15095/asus-google-team-up-for-tinker-board-aifocused-creditcard-sized-computers|title=ASUS & Google Team Up for 'Tinker Board' AI-Focused Credit-Card Sized Computers|last=Shilov|first=Anton|website=Anandtech.com|access-date=2019-11-13}}</ref> ASUS has also demonstrated a mini PC called Asus PN60T featuring the Edge TPU.<ref>{{Cite web|url=https://www.cnx-software.com/2019/05/29/asus-tinker-edge-t-cr1s-cm-a-sbc-google-coral-edge-tpu-nxp-i-mx-8m-processor |title=ASUS Tinker Edge T & CR1S-CM-A SBC to Feature Google Coral Edge TPU & NXP i.MX 8M Processor|last=Aufranc|first=Jean-Luc|date=2019-05-29|website=CNX Software - Embedded Systems News|language=en-US|access-date=2019-11-14}}</ref>

On January 2, 2020, Google announced the Coral Accelerator Module and Coral Dev Board Mini, to be demonstrated at [[Consumer Electronics Show|CES 2020]] later the same month. The Coral Accelerator Module is a [[multi-chip module]] featuring the Edge TPU, PCIe and USB interfaces for easier integration. The Coral Dev Board Mini is a smaller [[single-board computer|SBC]] featuring the Coral Accelerator Module and [[MediaTek|MediaTek 8167s SoC]].<ref>{{Cite web|url=https://developers.googleblog.com/2020/01/new-coral-products-for-2020.html|title=New Coral products for 2020|website=Google Developers Blog|language=en|access-date=2020-01-04}}</ref><ref>{{Cite web|url=https://coral.ai/products/accelerator-module|title=Accelerator Module|website=Coral|language=en-us|access-date=2020-01-04}}</ref>

===Pixel Neural Core===
{{main|Pixel Neural Core}}
On October 15, 2019, Google announced the [[Pixel 4]] smartphone, which contains an Edge TPU called the [[Pixel Neural Core]]. Google describe it as "customized to meet the requirements of key camera features in Pixel 4", using a neural network search that sacrifices some accuracy in favor of minimizing latency and power use.<ref>{{Cite web|url=http://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html|title=Introducing the Next Generation of On-Device Vision Models: MobileNetV3 and MobileNetEdgeTPU|website=Google AI Blog|language=en|access-date=2020-04-16}}</ref>

===Google Tensor===
{{main|Google Tensor}}
Google followed the Pixel Neural Core by integrating an Edge TPU into a custom [[system-on-chip]] named [[Google Tensor]], which was released in 2021 with the [[Pixel 6]] line of smartphones.<ref>{{cite web |url=https://ai.googleblog.com/2021/11/improved-on-device-ml-on-pixel-6-with.html |title=Improved On-Device ML on Pixel 6, with Neural Architecture Search |author1=Gupta, Suyog |author2=White, Marie |date=November 8, 2021 |website=Google AI Blog |access-date=16 December 2022}}</ref> The Google Tensor SoC demonstrated "extremely large performance advantages over the competition" in machine learning-focused benchmarks; although instantaneous power consumption also was relatively high, the improved performance meant less energy was consumed due to shorter periods requiring peak performance.<ref>{{cite news |url=https://www.anandtech.com/show/17032/tensor-soc-performance-efficiency/5 |title=Google's Tensor inside of Pixel 6, Pixel 6 Pro: A Look into Performance & Efficiency {{!}} Google's IP: Tensor TPU/NPU |author=Frumusanu, Andrei |date=November 2, 2021 |work=AnandTech |access-date=16 December 2022}}</ref>

==Lawsuit==
In 2019, Singular Computing, founded in 2009 by Joseph Bates, a [[Visiting scholar|visiting professor]] at [[MIT]],<ref>{{cite web |url=https://news.mit.edu/2010/fuzzy-logic-0103 |title=The surprising usefulness of sloppy arithmetic |last=Hardesty |first=Larry  |date=2011-01-03 |publisher=[[MIT]] |access-date=2024-01-10}}</ref> filed suit against Google alleging [[patent infringement]] in TPU chips.<ref>{{cite news |last=Bray |first=Hiawatha |date=2024-01-10 |title=Local inventor challenges Google in billion-dollar patent fight |url=https://www.bostonglobe.com/2024/01/10/business/local-inventor-challenges-google-billion-dollar-patent-fight/ |work=[[Boston Globe]] |location=[[Boston]] |archive-url=https://web.archive.org/web/20240110130855/https://www.bostonglobe.com/2024/01/10/business/local-inventor-challenges-google-billion-dollar-patent-fight/ |archive-date=2024-01-10 |access-date=2024-01-10}}</ref> By 2020, Google had successfully lowered the number of claims the court would consider to just two: claim 53 of {{patent|US|8407273}} filed in 2012 and claim 7 of {{patent|US|9218156}} filed in 2013, both of which claim a [[dynamic range]] of 10<sup>-6</sup> to 10<sup>6</sup> for floating point numbers, which the standard [[Half-precision floating-point format|float16]] cannot do (without resorting to [[subnormal number]]s) as it only has five bits for the exponent. In a 2023 court filing, Singular Computing specifically called out Google's use of [[bfloat16]], as that exceeds the dynamic range of [[float16]].<ref>{{cite web |url=https://www.pacermonitor.com/public/filings/DCYQIDDI/Singular_Computing_LLC_v_Google_LLC__madce-24-10008__0001.0.pdf |title=SINGULAR COMPUTING LLC, Plaintiff, v. GOOGLE LLC, Defendant: Amended Complaint for Patent Infringement |author=<!--Not stated--> |date=2020-03-20 |website=rpxcorp.com |publisher=[[RPX Corporation]] |access-date=2024-01-10}}</ref> Singular claims non-standard floating point formats were [[Non-obviousness in United States patent law|non-obvious]] in 2009, but Google retorts that the VFLOAT<ref>{{cite journal |last1=Wang |first1=Xiaojun |last2=Leeser |first2=Miriam |date=2010-09-01 |title=VFloat: A Variable Precision Fixed- and Floating-Point Library for Reconfigurable Hardware |url=https://dl.acm.org/doi/abs/10.1145/1839480.1839486 |journal=ACM Transactions on Reconfigurable Technology and Systems |volume=3 |issue=3 |pages=1–34 |doi=10.1145/1839480.1839486 |access-date=2024-01-10}}</ref> format, with configurable number of exponent bits, existed as [[prior art]] in 2002.<ref>{{cite web |url=https://casetext.com/case/singular-computing-llc-v-google-llc-1 |title=Singular Computing LLC v. Google LLC |author=<!--Not stated--> |date=2023-04-06 |website=casetext.com |access-date=2024-01-10}}</ref> By January 2024, subsequent lawsuits by Singular had brought the number of patents being litigated up to eight. Towards the end of the trial later that month, Google agreed to a settlement with undisclosed terms.<ref>{{Cite web |last=Calkins |first=Laurel Brubaker |date=January 24, 2024 |title=Google Settles AI-Chip Suit That Had Sought Over $5 Billion |url=https://news.bloomberglaw.com/ip-law/google-settles-ai-chip-design-suit-that-had-sought-billions |publisher=[[Bloomberg Law]]}}</ref><ref>{{Cite web |last1=Brittain |first1=Blake |last2=Raymond |first2=Ray |date=January 24, 2024 |title=Google settles AI-related chip patent lawsuit that sought $1.67 bln |url=https://www.reuters.com/technology/google-settles-ai-related-chip-patent-lawsuit-that-sought-167-bln-2024-01-24/ |publisher=[[Reuters]]}}</ref>

==See also==
* [[Cognitive computer]]
* [[AI accelerator]]
* [[Structure tensor]], a mathematical foundation for TPU's
* [[Tensor Core]], a similar architecture by [[Nvidia]]
* [[TrueNorth]], a similar device simulating [[Spiking neural network|spiking neuron]]s instead of low-precision tensors
* [[Vision processing unit]], a similar device specialised for vision processing

==References==
{{Reflist|30em}}
{{Authority control}}

==External links==
* [https://cloud.google.com/tpu/docs/tpus Cloud Tensor Processing Units (TPUs)] (Documentation from Google Cloud)
* [https://images.anandtech.com/doci/12195/google-tpu-board-2.png Photo of Google's TPU chip and board]
* [https://cloud.google.com/images/products/tpu/cloud-tpu-v2.png Photo of Google's TPU v2 board]
* [https://cloud.google.com/images/products/tpu/cloud-tpu-v3-alpha.png Photo of Google's TPU v3 board]
* [https://cloud.google.com/images/products/tpu/cloud-tpu-v2-pod-alpha.png Photo of Google's TPU v2 pod]

{{Google AI}}
{{Differentiable computing}}
{{Digital electronics}}
{{Google LLC}}

[[Category:AI accelerators]]
[[Category:Application-specific integrated circuits]]
[[Category:Computer-related introductions in 2016]]
[[Category:Google hardware]]
[[Category:Microprocessors]]