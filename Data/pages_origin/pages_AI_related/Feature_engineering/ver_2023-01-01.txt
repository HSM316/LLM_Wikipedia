{{Machine learning|Problems}}
'''Feature engineering''' or '''feature extraction'''  or '''feature discovery''' is the process of using [[domain knowledge]] to extract [[Feature (machine learning)|features]] (characteristics, properties, attributes) from raw [[data]].<ref>{{cite web | title =Machine Learning and AI via Brain simulations | url =https://ai.stanford.edu/~ang/slides/DeepLearning-Mar2013.pptx  | website =Stanford University | access-date =2019-08-01 }}</ref>  The motivation is to use these extra features to improve the quality of results from a [[machine learning]] process, compared with supplying only the raw data to the machine learning process. 

== Process  ==
The feature engineering process is:<ref>{{cite web|title=Big Data: Week 3 Video 3 - Feature Engineering|url=https://www.youtube.com/watch?v=drUToKxEAUA|website=youtube.com}}</ref>

*[[Brainstorming]] or [[Software testing|testing]] features<ref>{{Cite journal|url=https://content.iospress.com/articles/international-journal-of-knowledge-based-and-intelligent-engineering-systems/kes180383|title=Big data and intelligent software systems|first=Ahmed Adeeb|last=Jalal|date=January 1, 2018|journal=International Journal of Knowledge-based and Intelligent Engineering Systems|volume=22|issue=3|pages=177–193|via=content.iospress.com|doi=10.3233/KES-180383|s2cid=56487811 }}</ref>
* Deciding what features to create
* Creating features
* Testing the impact of the identified features on the task
* Improving your features if needed
* Repeat

=== Typical engineered features ===
The following list<ref>{{Cite web|title=Creating Features|url=https://kaggle.com/ryanholbrook/creating-features|access-date=2021-09-30|website=kaggle.com|language=en}}</ref> provides some typical ways to engineer useful features
* Numerical transformations (like taking fractions or scaling)
* Category encoder like one-hot or target encoder (for [[Categorical variable|categorical data]])<ref>{{Cite web|title=Category Encoders — Category Encoders 2.2.2 documentation|url=https://contrib.scikit-learn.org/category_encoders/index.html|access-date=2021-10-01|website=contrib.scikit-learn.org}}</ref>
* Clustering 
* Group aggregated values
* Principal component analysis (for numerical data)
* Feature construction : building new "physical", knowledge-based parameters relevant to the problem.<ref>{{Cite journal|title=A review of some techniques for inclusion of domain-knowledge into deep neural networks|first=Dash|last=T.|date=January 2022|journal=Scientific Reports|volume=12 |issue=1 |page=1040 |doi=10.1038/s41598-021-04590-0 |pmid=35058487 |pmc=8776800 |arxiv=2107.10295 |bibcode=2022NatSR..12.1040D }}</ref> For example, in physics, construction of [[Dimensionless quantity|dimensionless numbers]] such as [[Reynolds number]] in [[fluid dynamics]], [[Nusselt number]] in [[heat transfer]], [[Archimedes number]] in [[sedimentation]], construction of first approximations of the solution such as analytical [[strength of materials]] solutions in mechanics, etc.<ref>{{Cite web|url = https://www.researchgate.net/publication/353947052|title = SOLID-LIQUID MIXING IN STIRRED TANKS : Modeling, Validation, Design Optimization and Suspension Quality Prediction|date = 2021}}</ref>

== Relevance ==
Features vary in significance.<ref>{{Cite web|url = http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf|title = Feature Engineering|date = 2010-04-22|access-date = 12 November 2015}}</ref> Even relatively insignificant features may contribute to a model. [[Feature selection]] can reduce the number of features to prevent a model from becoming too specific to the training data set (overfitting).<ref>{{Cite web|url=http://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/slides.pdf|title=Feature engineering and selection|date=October 1, 2009|publisher=Alexandre Bouchard-Côté|access-date=12 November 2015}}</ref>

== Explosion ==
Feature explosion occurs when the number of identified features grows inappropriately. Common causes include:

* Feature templates - implementing feature templates instead of coding new features
* Feature combinations - combinations that cannot be represented by a linear system

Feature explosion can be limited via techniques such as: [[Regularization (mathematics)|regularization]], [[kernel method]]s, and [[feature selection]].<ref>{{Cite web|url = https://ufal.mff.cuni.cz/~zabokrtsky/courses/npfl104/html/feature_engineering.pdf|title = Feature engineering in Machine Learning|access-date = 12 November 2015|publisher = Zdenek Zabokrtsky|archive-url = https://web.archive.org/web/20160304112056/https://ufal.mff.cuni.cz/~zabokrtsky/courses/npfl104/html/feature_engineering.pdf|archive-date = 4 March 2016|url-status = dead}}</ref>

== Automation ==
Automation of feature engineering is a research topic that dates back to the 1990s.<ref>{{Cite book| chapter-url = https://link.springer.com/content/pdf/10.1007/978-3-540-48247-5_46.pdf | chapter = Multi-relational Decision Tree Induction| doi = 10.1007/978-3-540-48247-5_46| title = Principles of Data Mining and Knowledge Discovery| volume = 1704| pages = 378–383| series = Lecture Notes in Computer Science| year = 1999| last1 = Knobbe| first1 = Arno J.| last2 = Siebes| first2 = Arno| last3 = Van Der Wallen| first3 = Daniël| isbn = 978-3-540-66490-1}}</ref> Machine learning software that incorporates [[automated feature engineering]] has been commercially available since 2016.<ref>{{Cite web|website=Reality AI Blog|title=Its all about the features|date=September 2017|url=https://reality.ai/it-is-all-about-the-features/}}</ref> Related academic literature can be roughly separated into two types:

* Multi-relational decision tree learning (MRDTL) uses a supervised algorithm that is similar to a [[decision tree]]. 
* Deep Feature Synthesis uses simpler methods.{{citation needed|date=January 2020}}

=== Multi-relational decision tree learning (MRDTL) ===
MRDTL generates features in the form of SQL queries by successively adding clauses to the queries.<ref>{{Cite journal| title = A Comparative Study Of Multi-Relational Decision Tree Learning Algorithm |citeseerx = 10.1.1.636.2932}}</ref> For instance, the algorithm might start out with

<syntaxhighlight lang="sql">
SELECT COUNT(*) FROM ATOM t1 LEFT JOIN MOLECULE t2 ON t1.mol_id = t2.mol_id GROUP BY t1.mol_id
</syntaxhighlight>

The query can then successively be refined by adding conditions, such as "WHERE t1.charge <= -0.392".<ref>{{Cite document |first1=Hector |last1=Leiva |first2=Anna |last2=Atramentov |first3=Vasant |last3=Honavar | url = http://web.cs.iastate.edu/~honavar/Papers/kddmrdmpaperfinal.pdf | title = Experiments with MRDTL – A Multi-relational Decision Tree Learning Algorithm |date=2002 }}</ref>

However, most  MRDTL studies base implementations on relational databases, which results in many redundant operations. These redundancies can be reduced by using techniques such as tuple id propagation.<ref>{{Cite book |first1=Xiaoxin |last1=Yin |first2=Jiawei |last2=Han |first3=Jiong |last3=Yang |first4=Philip S. |last4=Yu |title=Proceedings. 20th International Conference on Data Engineering |chapter = CrossMine: Efficient Classification Across Multiple Database Relations |journal=Proceedings of the 20th International Conference on Data Engineering |year=2004 |pages=399–410 |doi=10.1109/ICDE.2004.1320014 |isbn=0-7695-2065-0 |s2cid=1183403 }}</ref><ref>{{Cite book| chapter = A Method for Multi-relational Classification Using Single and Multi-feature Aggregation Functions | doi = 10.1007/978-3-540-74976-9_43 | title = Knowledge Discovery in Databases: PKDD 2007 | volume = 4702 | pages = 430–437 | series = Lecture Notes in Computer Science | year = 2007 | last1 = Frank | first1 = Richard | last2 = Moser | first2 = Flavia | last3 = Ester | first3 = Martin | isbn = 978-3-540-74975-2 }}</ref> Efficiency can be increased by using incremental updates, which eliminates redundancies.<ref>{{Cite web| url = https://get.ml/resources/how-getml-works | title = How automated feature engineering works - The most efficient feature engineering solution for relational data and time series | access-date=2019-11-21 }}</ref>{{Promotional source|date=January 2020}}

=== Open-source implementations ===

There are a number of open-source libraries and tools that automate feature engineering on relational data and time series:

* '''featuretools''' is a [[Python_(programming_language)|Python]] library for transforming time series and relational data into feature matrices for machine learning.<ref>{{Cite web| url=https://featuretools.alteryx.com/en/stable/ | title=What is Featuretools?|access-date=September 7, 2022}}</ref><ref>{{Cite web| url = https://www.featuretools.com | title=Featuretools - An open source python framework for automated feature engineering|access-date=September 7, 2022}}</ref><ref>{{Cite web| url = https://github.com/alteryx/featuretools | title=github: alteryx/featuretools|access-date=September 7, 2022}}</ref>
* '''OneBM''' or One-Button Machine combines feature transformations and feature selection on relational data with feature selection techniques.<ref>{{Cite arXiv|last1=Thanh Lam|first1=Hoang|last2=Thiebaut|first2=Johann-Michael|last3=Sinn|first3=Mathieu|last4=Chen|first4=Bei|last5=Mai|first5=Tiep|last6=Alkan|first6=Oznur|date=2017-06-01|title=One button machine for automating feature engineering in relational databases|class=cs.DB|eprint=1706.00327}}</ref> {{Cquote
| quote = [OneBM] helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time, and cost.<ref>{{Cite arXiv|last1=Thanh Lam|first1=Hoang|last2=Thiebaut|first2=Johann-Michael|last3=Sinn|first3=Mathieu|last4=Chen|first4=Bei|last5=Mai|first5=Tiep|last6=Alkan|first6=Oznur|date=2017-06-01|title=One button machine for automating feature engineering in relational databases|class=cs.DB|eprint=1706.00327}}</ref>
}}
* '''getML community''' is an open source tool for automated feature engineering on time series and relational data.<ref>{{Cite web| url=https://docs.getml.com/latest/ | title=getML documentation|access-date=September 7, 2022}}</ref><ref>{{Cite web|url=https://github.com/getml/getml-community|title=github: getml/getml-community|access-date=September 7, 2022}}</ref> It is implemented in [[C_(programming_language)|C]]/[[C%2B%2B|C++]] with a Python interface.<ref>{{Cite web|url=https://github.com/getml/getml-community|title=github: getml/getml-community|access-date=September 7, 2022}}</ref> It has been shown to be at least 60 times faster than tsflex, tsfresh, tsfel, featuretools or kats.<ref>{{Cite web|url=https://github.com/getml/getml-community|title=github: getml/getml-community|access-date=September 7, 2022}}</ref>
* '''tsfresh''' is a Python library for feature extraction on time series data.<ref>{{Cite web| url=https://tsfresh.readthedocs.io/en/latest| title=tsfresh documentation|access-date=September 7, 2022}}</ref> It evaluates the quality of the features using hypothesis testing.<ref>{{Cite web| url=https://www.researchgate.net/publication/324948288_Time_Series_FeatuRe_Extraction_on_basis_of_Scalable_Hypothesis_tests_tsfresh_-_A_Python_package | title=Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package)|access-date=September 7, 2022 }}</ref>
* '''tsflex''' is an open source Python library for extracting features from time series data.<ref>{{Cite web| url=https://github.com/predict-idlab/tsflex | title=predict-idlab/tsflex|access-date=September 7, 2022}}</ref> Despite being 100% written in Python, it has been shown to be faster and more memory efficient than tsfresh, seglearn or tsfel.<ref>{{Cite web| url=https://www.sciencedirect.com/science/article/pii/S2352711021001904 | title=tsflex: Flexible time series processing & feature extraction|access-date=September 7, 2022}}</ref>
* '''seglearn''' is an extension for multivariate, sequential time series data to the [[scikit-learn]] Python library.<ref>{{Cite web| url=https://dmbee.github.io/seglearn/user_guide.html | title=seglearn user guide|access-date=September 7, 2022}}</ref>
* '''tsfel''' is a Python package for feature extraction on time series data.<ref>{{Cite web| url=https://tsfel.readthedocs.io/en/latest/ | title=Welcome to TSFEL documentation!|access-date=September 7, 2022 }}</ref>
* '''kats''' is a Python toolkit for analyzing time series data.<ref>{{Cite web| url=https://github.com/facebookresearch/Kats | title=github: facebookresearch/Kats|access-date=September 7, 2022}}</ref>

=== Deep feature synthesis ===
The deep feature synthesis (DFS) algorithm beat 615 of 906 human teams in a competition.<ref>{{Cite web| url = https://news.mit.edu/2015/automating-big-data-analysis-1016| title = Automating big-data analysis}}</ref><ref>{{Cite book |first1=James Max |last1=Kanter |first2=Kalyan |last2=Veeramachaneni |title=2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA) |chapter=Deep Feature Synthesis: Towards Automating Data Science Endeavors |date=2015 |journal=IEEE International Conference on Data Science and Advanced Analytics |pages=1–10 |doi=10.1109/DSAA.2015.7344858 |isbn=978-1-4673-8272-4 |s2cid=206610380 }}</ref>

== Feature stores ==
The Feature Store is where the features are stored and organized for the explicit purpose of being used to either train models (by data scientists) or make predictions (by applications that have a trained model). It is a central location where you can either create or update groups of features created from multiple different data sources, or create and update new datasets from those feature groups for training models or for use in applications that do not want to compute the features but just retrieve them when it needs them to make predictions.<ref>{{Cite web|url=https://www.featurestore.org/what-is-a-feature-store|title=What is a feature store|access-date=2022-04-19}}</ref>

A feature store includes the ability to store code used to generate features, apply the code to raw data, and serve those features to models upon request. Useful capabilities include feature versioning and policies governing the circumstances under which features can be used.<ref>{{Cite web|url=https://phaseai.com/resources/intro-to-feature-stores|title=An Introduction to Feature Stores|access-date=2021-04-15}}</ref>

Feature stores can be standalone software tools or built into machine learning platforms.

==See also==
* [[Covariate]]
* [[Data transformation (statistics)|Data transformation]]
* [[Feature extraction]]
* [[Feature learning]]
* [[Hashing trick]]
* [[Kernel method]]
* [[List of datasets for machine learning research]]
* [[Space mapping]]
* [[Instrumental variables estimation]]

==References==
{{Reflist}}

==Further reading==
*{{cite book |first1=Bradley |last1=Boehmke |first2=Brandon |last2=Greenwell |chapter=Feature & Target Engineering |pages=41–75 |title=Hands-On Machine Learning with R |publisher=Chapman & Hall |year=2019 |isbn=978-1-138-49568-5 }}
*{{cite book |first1=Alice |last1=Zheng |first2=Amanda |last2=Casari |title=Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists |publisher=O'Reilly |year=2018 |isbn=978-1-4919-5324-2 }}
*{{cite book |first1=Nina |last1=Zumel |first2=John |last2=Mount |chapter=Data Engineering and Data Shaping |title=Practical Data Science with R |publisher=Manning |edition=2nd |year=2020 |isbn=978-1-61729-587-4 |pages=113–160 }}

[[Category:Machine learning]]
[[Category:Data analysis]]