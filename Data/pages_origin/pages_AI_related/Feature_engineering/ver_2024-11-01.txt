{{Short description|Extracting features from raw data for machine learning}}
{{Machine learning|Problems}}{{Update|date=February 2024}}
'''Feature engineering''' is a preprocessing step in [[supervised machine learning]] and [[statistical modeling]]<ref>{{Cite book |last1=Hastie |first1=Trevor |url=https://books.google.com/books?id=eBSgoAEACAAJ |title=The Elements of Statistical Learning: Data Mining, Inference, and Prediction |last2=Tibshirani |first2=Robert |last3=Friedman |first3=Jerome H. |date=2009 |publisher=Springer |isbn=978-0-387-84884-6 |language=en}}</ref> which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability.<ref>{{Cite journal |last1=Sharma |first1=Shubham |last2=Nayak |first2=Richi |last3=Bhaskar |first3=Ashish |date=2024-05-01 |title=Multi-view feature engineering for day-to-day joint clustering of multiple traffic datasets |journal=Transportation Research Part C: Emerging Technologies |volume=162 |pages=104607 |doi=10.1016/j.trc.2024.104607 |issn=0968-090X|doi-access=free |bibcode=2024TRPC..16204607S }}</ref><ref>{{Cite book |last1=Shalev-Shwartz |first1=Shai |title=Understanding Machine Learning: From Theory to Algorithms |last2=Ben-David |first2=Shai |publisher=Cambridge University Press |year=2014 |isbn=9781107057135 |location=Cambridge}}</ref><ref>{{Cite book |last=Murphy |first=Kevin P. |title=Probabilistic Machine Learning |publisher=The MIT Press (Copyright 2022 Massachusetts Institute of Technology, this work is subject to a Creative Commons CC-BY-NC-ND license) |year=2022 |isbn=9780262046824 |location=Cambridge, Massachusetts |pages=}}</ref> 

Beyond machine learning, the principles of feature engineering are applied in various scientific fields, including physics. For example, physicists construct [[Dimensionless quantity|dimensionless numbers]] such as the [[Reynolds number]] in [[fluid dynamics]], the [[Nusselt number]] in [[heat transfer]], and the [[Archimedes number]] in [[sedimentation]]. They also develop first approximations of solutions, such as analytical solutions for the [[strength of materials]] in mechanics.<ref>{{Cite report |url=https://www.researchgate.net/publication/353947052 |title=SOLID-LIQUID MIXING IN STIRRED TANKS : Modeling, Validation, Design Optimization and Suspension Quality Prediction |date=2021 |doi=10.13140/RG.2.2.11074.84164/1 |vauthors=MacQueron C}}</ref>

== Clustering ==
One of the applications of feature engineering has been clustering of feature-objects or sample-objects in a dataset. Especially, feature engineering based on [[matrix decomposition]] has been extensively used for data clustering under non-negativity constraints on the feature coefficients. These include ''Non-Negative Matrix Factorization'' (NMF),<ref>{{Cite journal |last1=Lee |first1=Daniel D. |last2=Seung |first2=H. Sebastian |date=1999 |title=Learning the parts of objects by non-negative matrix factorization |url=https://www.nature.com/articles/44565 |journal=Nature |language=en |volume=401 |issue=6755 |pages=788–791 |doi=10.1038/44565 |pmid=10548103 |bibcode=1999Natur.401..788L |issn=1476-4687}}</ref> ''Non-Negative Matrix-Tri Factorization'' (NMTF),<ref>{{Cite book |last1=Wang |first1=Hua |last2=Nie |first2=Feiping |last3=Huang |first3=Heng |last4=Ding |first4=Chris |chapter=Nonnegative Matrix Tri-factorization Based High-Order Co-clustering and Its Fast Implementation |date=2011 |pages=774–783 |title=2011 IEEE 11th International Conference on Data Mining |chapter-url=http://dx.doi.org/10.1109/icdm.2011.109 |publisher=IEEE |doi=10.1109/icdm.2011.109|isbn=978-1-4577-2075-8 }}</ref> ''Non-Negative Tensor Decomposition/Factorization'' (NTF/NTD),<ref>{{cite arXiv |last1=Lim |first1=Lek-Heng |title=Nonnegative approximations of nonnegative tensors |date=2009-04-12 |last2=Comon |first2=Pierre|class=cs.NA |eprint=0903.4530 }}</ref> etc. The non-negativity constraints on coefficients of the feature vectors mined by the above-stated algorithms yields a part-based representation, and different factor matrices exhibit natural clustering properties. Several extensions of the above-stated feature engineering methods have been reported in literature, including ''orthogonality-constrained factorization'' for hard clustering, and ''manifold learning'' to overcome inherent issues with these algorithms.

Other classes of feature engineering algorithms include leveraging a common hidden structure across multiple inter-related datasets to obtain a consensus (common) clustering scheme. An example is ''Multi-view Classification based on Consensus Matrix Decomposition'' (MCMD),<ref>{{Cite journal |last1=Sharma |first1=Shubham |last2=Nayak |first2=Richi |last3=Bhaskar |first3=Ashish |date=2024-05-01 |title=Multi-view feature engineering for day-to-day joint clustering of multiple traffic datasets |journal=Transportation Research Part C: Emerging Technologies |volume=162 |pages=104607 |doi=10.1016/j.trc.2024.104607 |issn=0968-090X|doi-access=free |bibcode=2024TRPC..16204607S }}</ref> which mines a common clustering scheme across multiple datasets. MCMD is designed to output two types of class labels (scale-variant and scale-invariant clustering), and:

* is [[Robustness (computer science)|computationally robust]] to missing information,
* can obtain shape- and scale-based outliers,
* and can handle high-dimensional data effectively.

Coupled matrix and tensor decompositions are popular in multi-view feature engineering.<ref>{{Cite journal |last1=Nayak |first1=Richi |last2=Luong |first2=Khanh |date=2023 |title=Multi-aspect Learning |url=https://doi.org/10.1007/978-3-031-33560-0 |journal=Intelligent Systems Reference Library |volume=242 |language=en |doi=10.1007/978-3-031-33560-0 |isbn=978-3-031-33559-4 |issn=1868-4394}}</ref>

== Predictive modelling ==
Feature engineering in [[machine learning]] and [[statistical modeling]] involves selecting, creating, transforming, and extracting data features. Key components include feature creation from existing data, transforming and imputing missing or invalid features, reducing data dimensionality through methods like [[Principal component analysis|Principal Components Analysis]] (PCA), [[Independent component analysis|Independent Component Analysis]] (ICA), and [[Linear discriminant analysis|Linear Discriminant Analysis]] (LDA), and selecting the most relevant features for model training based on importance scores and [[correlation matrices]].<ref>{{Cite web |title=Feature engineering - Machine Learning Lens |url=https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html |access-date=2024-03-01 |website=docs.aws.amazon.com}}</ref>

Features vary in significance.<ref>{{Cite web|url = http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf|title = Feature Engineering|date = 2010-04-22|access-date = 12 November 2015}}</ref> Even relatively insignificant features may contribute to a model. [[Feature selection]] can reduce the number of features to prevent a model from becoming too specific to the training data set (overfitting).<ref>{{Cite web|url=http://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/slides.pdf|title=Feature engineering and selection|date=October 1, 2009|publisher=Alexandre Bouchard-Côté|access-date=12 November 2015}}</ref>

Feature explosion occurs when the number of identified features is too large for effective model estimation or optimization. Common causes include:

* Feature templates - implementing feature templates instead of coding new features
* Feature combinations - combinations that cannot be represented by a linear system

Feature explosion can be limited via techniques such as: [[Regularization (mathematics)|regularization]], [[kernel method]]s, and [[feature selection]].<ref>{{Cite web|url = https://ufal.mff.cuni.cz/~zabokrtsky/courses/npfl104/html/feature_engineering.pdf|title = Feature engineering in Machine Learning|access-date = 12 November 2015|publisher = Zdenek Zabokrtsky|archive-url = https://web.archive.org/web/20160304112056/https://ufal.mff.cuni.cz/~zabokrtsky/courses/npfl104/html/feature_engineering.pdf|archive-date = 4 March 2016|url-status = dead}}</ref>

== Automation ==
Automation of feature engineering is a research topic that dates back to the 1990s.<ref name=":0">{{Cite book| chapter-url = https://link.springer.com/content/pdf/10.1007/978-3-540-48247-5_46.pdf | chapter = Multi-relational Decision Tree Induction| doi = 10.1007/978-3-540-48247-5_46| title = Principles of Data Mining and Knowledge Discovery| volume = 1704| pages = 378–383| series = Lecture Notes in Computer Science| year = 1999| vauthors = Knobbe AJ, Siebes A, Van Der Wallen D | isbn = 978-3-540-66490-1}}</ref> Machine learning software that incorporates [[automated feature engineering]] has been commercially available since 2016.<ref>{{Cite web|website=Reality AI Blog|title=Its all about the features|date=September 2017|url=https://reality.ai/it-is-all-about-the-features/}}</ref> Related academic literature can be roughly separated into two types:

* Multi-relational decision tree learning (MRDTL) uses a supervised algorithm that is similar to a [[decision tree]]. 
* Deep Feature Synthesis uses simpler methods.{{citation needed|date=January 2020}}

=== Multi-relational decision tree learning (MRDTL) ===
Multi-relational Decision Tree Learning (MRDTL) extends traditional decision tree methods to [[Relational database|relational databases]], handling complex data relationships across tables. It innovatively uses selection graphs as [[Decision node|decision nodes]], refined systematically until a specific termination criterion is reached.<ref name=":0" />

Most MRDTL studies base implementations on relational databases, which results in many redundant operations. These redundancies can be reduced by using techniques such as tuple id propagation.<ref>{{Cite book | vauthors = Yin X, Han J, Yang J, Yu PS |chapter=CrossMine: Efficient classification across multiple database relations |title=Proceedings. 20th International Conference on Data Engineering |year=2004 |pages=399–410 |doi=10.1109/ICDE.2004.1320014 |isbn=0-7695-2065-0 |s2cid=1183403 }}</ref><ref>{{Cite book| chapter = A Method for Multi-relational Classification Using Single and Multi-feature Aggregation Functions | doi = 10.1007/978-3-540-74976-9_43 | title = Knowledge Discovery in Databases: PKDD 2007 | volume = 4702 | pages = 430–437 | series = Lecture Notes in Computer Science | year = 2007 | vauthors = Frank R, Moser F, Ester M  | isbn = 978-3-540-74975-2 }}</ref> 

=== Open-source implementations ===

There are a number of open-source libraries and tools that automate feature engineering on relational data and time series:

* '''featuretools''' is a [[Python (programming language)|Python]] library for transforming time series and relational data into feature matrices for machine learning.<ref>{{Cite web| url=https://featuretools.alteryx.com/en/stable/ | title=What is Featuretools?|access-date=September 7, 2022}}</ref><ref>{{Cite web| url = https://www.featuretools.com | title=Featuretools - An open source python framework for automated feature engineering|access-date=September 7, 2022}}</ref><ref>{{Cite web| url = https://github.com/alteryx/featuretools | title=github: alteryx/featuretools| website=[[GitHub]]|access-date=September 7, 2022}}</ref>
* '''MCMD:''' An open-source feature engineering algorithm for joint clustering of multiple datasets . <ref>{{Citation |last=Sharma |first=Shubham |title=mcmd: Multi-view Classification framework based on Consensus Matrix Decomposition developed by Shubham Sharma at QUT |url=https://github.com/ashishbhaskar/MCMD |access-date=2024-04-14}}</ref><ref>{{Cite journal |last1=Sharma |first1=Shubham |last2=Nayak |first2=Richi |last3=Bhaskar |first3=Ashish |date=2024-05-01 |title=Multi-view feature engineering for day-to-day joint clustering of multiple traffic datasets |journal=Transportation Research Part C: Emerging Technologies |volume=162 |pages=104607 |doi=10.1016/j.trc.2024.104607 |issn=0968-090X|doi-access=free |bibcode=2024TRPC..16204607S }}</ref>
* '''OneBM''' or One-Button Machine combines feature transformations and feature selection on relational data with feature selection techniques.<ref>{{Cite arXiv|last1=Thanh Lam|first1=Hoang|last2=Thiebaut|first2=Johann-Michael|last3=Sinn|first3=Mathieu|last4=Chen|first4=Bei|last5=Mai|first5=Tiep|last6=Alkan|first6=Oznur|date=2017-06-01|title=One button machine for automating feature engineering in relational databases|class=cs.DB|eprint=1706.00327}}</ref> {{Cquote
| quote = [OneBM] helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time, and cost.<ref>{{Cite arXiv|last1=Thanh Lam|first1=Hoang|last2=Thiebaut|first2=Johann-Michael|last3=Sinn|first3=Mathieu|last4=Chen|first4=Bei|last5=Mai|first5=Tiep|last6=Alkan|first6=Oznur|date=2017-06-01|title=One button machine for automating feature engineering in relational databases|class=cs.DB|eprint=1706.00327}}</ref>
}}
* '''getML community''' is an open source tool for automated feature engineering on time series and relational data.<ref>{{Cite web| url=https://docs.getml.com/latest/ | title=getML documentation|access-date=September 7, 2022}}</ref><ref>{{Cite web|url=https://github.com/getml/getml-community|title=github: getml/getml-community|website=[[GitHub]] |access-date=September 7, 2022}}</ref> It is implemented in [[C (programming language)|C]]/[[C++]] with a Python interface.<ref>{{Cite web|url=https://github.com/getml/getml-community|title=github: getml/getml-community|website=[[GitHub]] |access-date=September 7, 2022}}</ref> It has been shown to be at least 60 times faster than tsflex, tsfresh, tsfel, featuretools or kats.<ref>{{Cite web|url=https://github.com/getml/getml-community|title=github: getml/getml-community|website=[[GitHub]] |access-date=September 7, 2022}}</ref>
* '''tsfresh''' is a Python library for feature extraction on time series data.<ref>{{Cite web| url=https://tsfresh.readthedocs.io/en/latest| title=tsfresh documentation|access-date=September 7, 2022}}</ref> It evaluates the quality of the features using hypothesis testing.<ref>{{Cite web| url=https://www.researchgate.net/publication/324948288 | title=Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package)|access-date=September 7, 2022 }}</ref>
* '''tsflex''' is an open source Python library for extracting features from time series data.<ref>{{Cite web| url=https://github.com/predict-idlab/tsflex | title=predict-idlab/tsflex| website=[[GitHub]]|access-date=September 7, 2022}}</ref> Despite being 100% written in Python, it has been shown to be faster and more memory efficient than tsfresh, seglearn or tsfel.<ref>{{Cite journal| url=https://www.sciencedirect.com/science/article/pii/S2352711021001904 | title=tsflex: Flexible time series processing & feature extraction| year=2022| doi=10.1016/j.softx.2021.100971|access-date=September 7, 2022| last1=Van Der Donckt| first1=Jonas| last2=Van Der Donckt| first2=Jeroen| last3=Deprost| first3=Emiel| last4=Van Hoecke| first4=Sofie| journal=SoftwareX| volume=17| page=100971| arxiv=2111.12429| bibcode=2022SoftX..1700971V| s2cid=244527198}}</ref>
* '''seglearn''' is an extension for multivariate, sequential time series data to the [[scikit-learn]] Python library.<ref>{{Cite web| url=https://dmbee.github.io/seglearn/user_guide.html | title=seglearn user guide|access-date=September 7, 2022}}</ref>
* '''tsfel''' is a Python package for feature extraction on time series data.<ref>{{Cite web| url=https://tsfel.readthedocs.io/en/latest/ | title=Welcome to TSFEL documentation!|access-date=September 7, 2022 }}</ref>
* '''kats''' is a Python toolkit for analyzing time series data.<ref>{{Cite web| url=https://github.com/facebookresearch/Kats | title=github: facebookresearch/Kats| website=[[GitHub]]|access-date=September 7, 2022}}</ref>

=== Deep feature synthesis ===
The deep feature synthesis (DFS) algorithm beat 615 of 906 human teams in a competition.<ref>{{Cite web| url = https://news.mit.edu/2015/automating-big-data-analysis-1016| title = Automating big-data analysis| date = 16 October 2015}}</ref><ref>{{Cite book |first1=James Max |last1=Kanter |first2=Kalyan |last2=Veeramachaneni |chapter=Deep feature synthesis: Towards automating data science endeavors |title=2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA) |date=2015 |pages=1–10 |doi=10.1109/DSAA.2015.7344858 |isbn=978-1-4673-8272-4 |s2cid=206610380 }}</ref>

== Feature stores ==
The Feature Store is where the features are stored and organized for the explicit purpose of being used to either train models (by data scientists) or make predictions (by applications that have a trained model). It is a central location where you can either create or update groups of features created from multiple different data sources, or create and update new datasets from those feature groups for training models or for use in applications that do not want to compute the features but just retrieve them when it needs them to make predictions.<ref>{{Cite web|url=https://www.featurestore.org/what-is-a-feature-store|title=What is a feature store|access-date=2022-04-19}}</ref>

A feature store includes the ability to store code used to generate features, apply the code to raw data, and serve those features to models upon request. Useful capabilities include feature versioning and policies governing the circumstances under which features can be used.<ref>{{Cite web|url=https://phaseai.com/resources/intro-to-feature-stores|title=An Introduction to Feature Stores|access-date=2021-04-15}}</ref>

Feature stores can be standalone software tools or built into machine learning platforms.

== Alternatives ==
Feature engineering can be a time-consuming and error-prone process, as it requires domain expertise and often involves trial and error.<ref>{{Cite web |title=Feature Engineering in Machine Learning |url=https://www.section.io/engineering-education/feature-engineering-in-machine-learning/ |access-date=2023-03-21 |website=Engineering Education (EngEd) Program {{!}} Section |language=en-us}}</ref><ref>{{Cite web |last=explorium_admin |date=2021-10-25 |title=5 Reasons Why Feature Engineering is Challenging |url=https://www.explorium.ai/blog/5-reasons-why-feature-engineering-is-challenging/ |access-date=2023-03-21 |website=Explorium |language=en}}</ref> [[Deep learning|Deep learning algorithms]] may be used to process a large raw dataset without having to resort to feature engineering.<ref>{{Cite book |last=Spiegelhalter |first=D. J. |url=https://www.worldcat.org/oclc/1064776283 |title=The art of statistics : learning from data |date=2019 |isbn=978-0-241-39863-0 |location=[London] UK |oclc=1064776283}}</ref> However, deep learning algorithms still require careful preprocessing and cleaning of the input data.<ref>{{cite journal | vauthors = Sarker IH | title = Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions | journal = SN Computer Science | volume = 2 | issue = 6 | pages = 420 | date = November 2021 | pmid = 34426802 | pmc = 8372231 | doi = 10.1007/s42979-021-00815-1 }}</ref> In addition, choosing the right architecture, hyperparameters, and optimization algorithm for a deep neural network can be a challenging and iterative process.<ref>{{Citation |last=Bengio |first=Yoshua |title=Neural Networks: Tricks of the Trade |chapter=Practical Recommendations for Gradient-Based Training of Deep Architectures |date=2012 |chapter-url=http://dx.doi.org/10.1007/978-3-642-35289-8_26 |series=Lecture Notes in Computer Science |volume=7700 |pages=437–478 |access-date=2023-03-21 |place=Berlin, Heidelberg |publisher=Springer Berlin Heidelberg |doi=10.1007/978-3-642-35289-8_26 |isbn=978-3-642-35288-1|s2cid=10808461 |arxiv=1206.5533 }}</ref>

== See also ==
* [[Covariate]]
* [[Data transformation (statistics)|Data transformation]]
* [[Feature extraction]]
* [[Feature learning]]
* [[Hashing trick]]
* [[Instrumental variables estimation]]
* [[Kernel method]]
* [[List of datasets for machine learning research]]
* [[Scale co-occurrence matrix]]
* [[Space mapping]]

== References ==
{{Reflist}}

== Further reading ==
{{refbegin}}
* {{cite book | vauthors = Boehmke B, Greenwell B  |chapter=Feature & Target Engineering |pages=41–75 |title=Hands-On Machine Learning with R |publisher=Chapman & Hall |year=2019 |isbn=978-1-138-49568-5 }}
* {{cite book | vauthors = Zheng A, Casari A |title=Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists |publisher=O'Reilly |year=2018 |isbn=978-1-4919-5324-2 }}
* {{cite book | vauthors = Zumel N, Mount  |chapter=Data Engineering and Data Shaping |title=Practical Data Science with R |publisher=Manning |edition=2nd |year=2020 |isbn=978-1-61729-587-4 |pages=113–160 }}
* {{citation | vauthors=((Abououf, M.)), ((Singh, S.)), ((Mizouni, R.)), ((Otrok, H.)) | year=2024 | title=Feature engineering and deep learning-based approach for event detection in Medical Internet of Things (MIoT) | publisher=Elsevier BV | url=http://dx.doi.org/10.1016/j.iot.2024.101191}}
* {{cite journal 
    | vauthors = Chicco D, Oneto L, Tavazzi E
    | title = Eleven quick tips for data cleaning and feature engineering
    | journal = PLOS Computational Biology 
    | volume = 18 
    | issue = 12
    | pages = e1010718 
    | date = December 2022 
    | pmid = 36520712 
    | pmc = 9754225 
    | doi =  10.1371/journal.pcbi.1010718
    | s2cid =  254733288
    | doi-access = free 
 }}

{{refend}}

[[Category:Machine learning]]
[[Category:Data analysis]]