{{Machine learning bar}}
'''Feature engineering''' is the process of using [[domain knowledge]] of the data to create [[Feature (machine learning)|features]] that make [[machine learning]] algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated [[feature learning]].

Feature engineering is an informal topic, but it is considered essential in applied machine learning.

{{Quote|text=Coming up with features is difficult, time-consuming, requires expert knowledge. "Applied machine learning" is basically feature engineering.|sign=[[Andrew Ng]]|source=''Machine Learning and AI via Brain simulations''<ref>{{cite web | title =Machine Learning and AI via Brain simulations | url =https://ai.stanford.edu/~ang/slides/DeepLearning-Mar2013.pptx  | website =Stanford University | date =  | accessdate =2019-08-01 }}</ref>}}

== Features ==
A [[Feature (machine learning)|feature]] is an attribute or property shared by all of the independent units on which analysis or prediction is to be done. Any attribute could be a feature, as long as it is useful to the model.

The purpose of a feature, other than being an attribute, would be much easier to understand in the context of a problem. A feature is a characteristic that might help when solving the problem.<ref name=":0">{{Cite web|title = Discover Feature Engineering, How to Engineer Features and How to Get Good at It - Machine Learning Mastery|url = http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/|website = Machine Learning Mastery|accessdate = 2015-11-11}}</ref>

== Importance  ==
Features are important to [[Predictive modelling|predictive models]] and influence results.<ref>{{Cite web|title = Feature Engineering: How to transform variables and create new ones?|url = http://www.analyticsvidhya.com/blog/2015/03/feature-engineering-variable-transformation-creation/|website = Analytics Vidhya|date = 2015-03-12|accessdate = 2015-11-12}}</ref>

You could say the better the features are, the better the result is. This isn't entirely true, because the results achieved also depend on the model and the data, not just the chosen features. That said, choosing the right features is still very important. Better features can produce simpler and more flexible models, and they often yield better results.<ref name=":0" />

{{Quote|text=The algorithms we used are very standard for [[Kaggle]]rs. […]  We spent most of our efforts in feature engineering. [...] We were also very careful to discard features likely to expose us to the risk of [[over-fitting]] our model.|sign=Xavier Conort|source="Q&A with Xavier Conort"<ref>{{cite web |work=kaggle.com |date=2013-04-10 |title=Q&A with Xavier Conort |url=http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/ |accessdate=12 November 2015 }}</ref>}}

{{Quote|text=…some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.|sign=Pedro Domingos|source="A Few Useful Things to Know about Machine Learning"<ref>{{Cite journal|last=Domingos|first=Pedro|date=2012-10-01|title=A few useful things to know about machine learning|url=http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf|journal=Communications of the ACM|language=en|volume=55|issue=10|pages=78–87|doi=10.1145/2347736.2347755}}</ref>}}

== Process  ==
The feature engineering process is:<ref>{{cite web|title=Big Data: Week 3 Video 3 - Feature Engineering|url=https://www.youtube.com/watch?v=drUToKxEAUA|website=youtube.com}}</ref>

*[[Brainstorming]] or [[Software testing|testing]] features;<ref>{{Cite journal|url=https://content.iospress.com/articles/international-journal-of-knowledge-based-and-intelligent-engineering-systems/kes180383|title=Big data and intelligent software systems|first=Ahmed Adeeb|last=Jalal|date=January 1, 2018|journal=International Journal of Knowledge-based and Intelligent Engineering Systems|volume=22|issue=3|pages=177–193|via=content.iospress.com|doi=10.3233/KES-180383}}</ref>
* Deciding what features to create;
* Creating features;
* Checking how the features work with your model;
* Improving your features if needed;
* Go back to brainstorming/creating more features until the work is done.

== Relevance ==
A feature could be strongly relevant (i.e., the feature has information that doesn't exist in any other feature), relevant, weakly relevant (some information that other features include) or irrelevant.<ref>{{Cite web|url = http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf|title = Feature Engineering|date = 2010-04-22|accessdate = 12 November 2015|website = |publisher = |last = |first = }}</ref> Even if some features are irrelevant, having too many is better than missing those that are important. [[Feature selection]] can be used to prevent overfitting.<ref>{{Cite web|url=http://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/slides.pdf|title=Feature engineering and selection|last=|first=|date=October 1, 2009|website=|publisher=Alexandre Bouchard-Côté|archive-url=|archive-date=|url-status=|access-date=12 November 2015}}</ref>

== Feature explosion ==
Feature explosion can be caused by feature combination or feature templates, both leading to a quick growth in the total number of features.

* Feature templates - implementing feature templates instead of coding new features
* Feature combinations - combinations that cannot be represented by the linear system

Feature explosion can be stopped via techniques such as: [[Regularization (mathematics)|regularization]], [[kernel method]], [[feature selection]].<ref>{{Cite web|url = https://ufal.mff.cuni.cz/~zabokrtsky/courses/npfl104/html/feature_engineering.pdf|title = Feature engineering in Machine Learning|date = |access-date = 12 November 2015|website = |publisher = Zdenek Zabokrtsky|last = |first = |archive-url = https://web.archive.org/web/20160304112056/https://ufal.mff.cuni.cz/~zabokrtsky/courses/npfl104/html/feature_engineering.pdf|archive-date = 4 March 2016|url-status = dead}}</ref>

== Automation ==
Automation of feature engineering is a research topic that dates back to at least the late 1990s.<ref>{{Cite book| chapter-url = https://link.springer.com/content/pdf/10.1007/978-3-540-48247-5_46.pdf | chapter = Multi-relational Decision Tree Induction| doi = 10.1007/978-3-540-48247-5_46| title = Principles of Data Mining and Knowledge Discovery| volume = 1704| pages = 378–383| series = Lecture Notes in Computer Science| year = 1999| last1 = Knobbe| first1 = Arno J.| last2 = Siebes| first2 = Arno| last3 = Van Der Wallen| first3 = Daniël| isbn = 978-3-540-66490-1}}</ref> The academic literature on the topic can be roughly separated into two strings: 

First, Multi-relational decision tree learning (MRDTL), which uses a supervised algorithm that is similar to a [[decision tree]].

Second, more recent approaches, like Deep Feature Synthesis, which use simpler methods.

Multi-relational decision tree learning (MRDTL) generates features in the form of SQL queries by successively adding new clauses to the queries.<ref>{{Cite journal| title = A Comparative Study Of Multi-Relational Decision Tree Learning Algorithm |citeseerx = 10.1.1.636.2932}}</ref> For instance, the algorithm might start out with "SELECT COUNT(*) FROM ATOM t1 LEFT JOIN MOLECULE t2 ON t1.mol_id = t2.mol_id GROUP BY t1.mol_id". The query can then successively be refined by adding conditions, such as "WHERE t1.charge <= -0.392".<ref>{{Cite web| url = http://web.cs.iastate.edu/~honavar/Papers/kddmrdmpaperfinal.pdf | title = Experiments with MRDTL -- A Multi-relational Decision Tree Learning Algorithm }}</ref>

However, most of the academic studies on MRDTL use implementations based on existing relational databases, which results in many redundant operations. These redundancies can be reduced by using tricks such as tuple id propagation.<ref>{{Cite web| url = https://www.researchgate.net/publication/220965932 | title = CrossMine: Efficient Classification Across Multiple Database Relations }}</ref><ref>{{Cite book| chapter-url = https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_43.pdf | chapter = A Method for Multi-relational Classification Using Single and Multi-feature Aggregation Functions | doi = 10.1007/978-3-540-74976-9_43 | title = Knowledge Discovery in Databases: PKDD 2007 | volume = 4702 | pages = 430–437 | series = Lecture Notes in Computer Science | year = 2007 | last1 = Frank | first1 = Richard | last2 = Moser | first2 = Flavia | last3 = Ester | first3 = Martin | isbn = 978-3-540-74975-2 }}</ref> More recently, it has been demonstrated that the efficiency can be increased further by using incremental updates, which completely eliminates redundancies.<ref>{{Cite web| url = https://get.ml/resources/how-getml-works | title = How automated feature engineering works - The most efficient feature engineering solution for relational data and time series | access-date=2019-11-21 }}</ref> A free implementation of this approach is made available by getML.<ref>{{Cite web| url = https://get.ml | title = getML adds automated feature engineering to AutoML | access-date=2019-11-21 }}</ref>

In 2015, researchers at MIT presented the Deep Feature Synthesis algorithm and demonstrated its effectiveness in online data science competitions where it beat 615 of 906 human teams.<ref>{{Cite web| url = https://news.mit.edu/2015/automating-big-data-analysis-1016| title = Automating big-data analysis}}</ref><ref>{{Cite web|url = https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf| title = Deep Feature Synthesis: Towards Automating Data Science Endeavors}}</ref> Deep Feature Synthesis is available as an open source library called Featuretools.<ref>{{Cite web|url=https://www.featuretools.com/|title=Featuretools {{!}} An open source framework for automated feature engineering Quick Start|website=www.featuretools.com|access-date=2019-08-22}}</ref> That work was followed by other researchers including IBM's OneBM<ref>{{Cite document| title = One button machine for automating feature engineering in relational databases|arxiv = 1706.00327|author1 = Hoang Thanh Lam|last2 = Thiebaut|first2 = Johann-Michael|last3 = Sinn|first3 = Mathieu|last4 = Chen|first4 = Bei|last5 = Mai|first5 = Tiep|last6 = Alkan|first6 = Oznur|year = 2017}}</ref> and Berkeley's ExploreKit.<ref>{{Cite web|url = https://people.eecs.berkeley.edu/~dawnsong/papers/icdm-2016.pdf| title = ExploreKit: Automatic Feature Generation and Selection}}</ref> The researchers at IBM stated that feature engineering automation "helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time, and cost." The Bourgain Embedding Theorem allows one to do automatic feature engineering.<ref>{{Cite web|url=http://www.orges-leka.de/automatic_feature_engineering.html|title=Automatic Feature Engineering for Data Science|author=Orges Leka|website=orges-leka.de|access-date=2019-09-29}}</ref>

==See also==
* [[Covariate]]
* [[Hashing trick]]
* [[Kernel method]]
* [[List of datasets for machine learning research]]
* [[Space mapping]]

==References==
{{Reflist}}

[[Category:Machine learning]]