{{Infobox software
| name = openSMILE
| logo =
| author = 
| developer = audEERING GmbH
| released = {{Start date and age|2010|9}}
| latest release version = 3.0.1<ref>{{Cite news |url = https://github.com/audeering/opensmile/releases/tag/v3.0.1 |title = Release openSMILE 3.0.1 |accessdate = 5 January 2022 |language = en-US }}</ref>
| latest release date = {{Start date and age|2022|01|04}}
| repo = 
| programming language = [[C++]]
| platform = [[Linux]], [[macOS]], [[Windows]], [[Android (operating system)|Android]], [[iOS]]
| genre = [[Machine learning]]
| license = [[Source-available]], proprietary
| website = {{URL|https://www.audeering.com/opensmile/|audeering.com}}
}}

'''openSMILE'''<ref>F. Eyben, M. Wöllmer, B. Schuller: „[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.364.5788&rep=rep1&type=pdf openSMILE - The Munich Versatile and Fast Open-Source Audio Feature Extractor]“, In Proc. ACM Multimedia (MM), ACM, Florence, Italy, ACM, pp. 1459-1462, October 2010.</ref> is [[source-available]] software for automatic extraction of features from [[audio signal]]s and for classification of speech and music signals. "SMILE" stands for "Speech & Music Interpretation by Large-space Extraction". The software is mainly applied in the area of automatic [[emotion recognition]] and is widely used in the [[affective computing]] research community. The openSMILE project exists since 2008 and is maintained by the German company audEERING GmbH since 2013. openSMILE is provided free of charge for research purposes and personal use under a source-available license. For commercial use of the tool, the company audEERING offers custom license options.

== Application Areas ==
openSMILE is used for academic research as well as for commercial applications in order to automatically analyze speech and music signals in real-time. In contrast to [[automatic speech recognition]] which extracts the spoken content out of a speech signal, openSMILE is capable of recognizing the characteristics of a given speech or music segment. Examples for such characteristics encoded in human speech are a speaker's [[emotion]],<ref>B. Schuller, B. Vlasenko, F. Eyben, M. Wöllmer, A. Stuhlsatz, A. Wendemuth, G. Rigoll, "[http://publications.idiap.ch/downloads/papers/2015/Schuller_ACII2015_2015.pdf Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies (Extended Abstract)]," in Proc. of ACII 2015, Xi'an, China, invited for the Special Session on Most Influential Articles in IEEE Transactions on Affective Computing.</ref> age, gender, and personality, as well as speaker states like [[Depression (mood)|depression]], [[Alcohol intoxication|intoxication]], or vocal pathological disorders. The software further includes music classification technology for automatic music mood detection and recognition of [[Refrain|chorus]] segments, key, [[Chord (music)|chords]], tempo, meter, dance-style, and genre.

The openSMILE toolkit serves as benchmark in manifold research competitions such as Interspeech ComParE,<ref>B. Schuller, S. Steidl, A. Batliner, J. Hirschberg, J. K. Burgoon, A. Elkins, Y. Zhang, E. Coutinho: "[http://emotion-research.net/sigs/speech-sig/is16-compare The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception & Sincerity] {{Webarchive|url=https://web.archive.org/web/20170609195546/http://emotion-research.net/sigs/speech-sig/is16-compare |date=2017-06-09 }}", Proceedings INTERSPEECH 2016, ISCA, San Francisco, USA, 2016.</ref> AVEC,<ref>F. Ringeval, B. Schuller, M. Valstar, R. Cowie, M. Pantic,
“[https://diuf.unifr.ch/diva/recola/data/AVEC_2015_challenge.pdf AVEC 2015 - The 5th International Audio/Visual Emotion Challenge and Workshop],” in Proceedings of the 23rd ACM International Conference on Multimedia, MM 2015, (Brisbane, Australia), ACM, October 2015.</ref> MediaEval,<ref>M. Eskevich, R. Aly, D. Racca, R. Ordelman, S. Chen, G. J. Jones, "[http://doras.dcu.ie/20386/1/SearchAndHyperlinkingOverview2014.pdf The search and hyperlinking task at MediaEval 2014]".</ref> and EmotiW.<ref>F. Ringeval, S. Amiriparian, F. Eyben, K. Scherer, B. Schuller, “[https://diuf.unifr.ch/diva/recola/data/ICMI14.pdf Emotion Recognition in the Wild: Incorporating Voice and Lip Activity in Multimodal Decision-Level Fusion],” in Proceedings of the ICMI 2014 EmotiW – Emotion Recognition In The Wild Challenge and Workshop (EmotiW 2014), Satellite of the 16th ACM International Conference on Multimodal Interaction (ICMI 2014), (Istanbul, Turkey), pp. 473– 480, ACM, November 2014</ref>

== History ==
The openSMILE project was started in 2008 by Florian Eyben, Martin Wöllmer, and [[:de:Björn Schuller|Björn Schuller]] at the [[Technical University of Munich]] within the [[European Union]] research project SEMAINE. The goal of the SEMAINE project was to develop a virtual agent with emotional and [[social intelligence]]. In this system, openSMILE was applied for real-time analysis of speech and emotion. The final SEMAINE software release is based on openSMILE version 1.0.1.

In 2009, the emotion recognition toolkit (openEAR) was published based on openSMILE. "EAR" stands for "Emotion and Affect Recognition".

In 2010, openSMILE version 1.0.1 was published and was introduced and awarded at the [[ACM Multimedia]] Open-Source Software Challenge.

Between 2011 and 2013, the technology of openSMILE was extended and improved by Florian Eyben and Felix Weninger in the context of their doctoral thesis at the [[Technical University of Munich]]. The software was also applied for the project ASC-Inclusion, which was funded by the [[European Union]]. For this project, the software was extended by Erik Marchi in order to teach emotional expression to [[autistic]] children, based on automatic emotion recognition and visualization.

In 2013, the company audEERING acquired the rights to the code-base from the [[Technical University of Munich]] and version 2.0 was published under a source-available research license.

Until 2016, openSMILE was downloaded more than 50,000 times worldwide and has established itself as a standard toolkit for emotion recognition.

== Awards ==
openSMILE was awarded in 2010 in the context of the [[ACM Multimedia]] Open Source Competition. The software tool is applied in numerous scientific publications on automatic emotion recognition. openSMILE<ref>{{cite book|url=https://scholar.google.de/citations?view_op=view_citation&hl=de&user=72yq_tkAAAAJ&citation_for_view=72yq_tkAAAAJ:9yKSN-GCB0IC|title=Opensmile: the munich versatile and fast open-source audio feature extractor|first1=Florian|last1=Eyben|first2=Martin|last2=Wöllmer|first3=Björn|last3=Schuller|date=26 April 2018|publisher=ACM|pages=1459–1462|doi=10.1145/1873951.1874246 |isbn=978-1-60558-933-6 |via=Google Scholar}}</ref> and its extension openEAR<ref>{{cite web|url=https://scholar.google.de/citations?view_op=view_citation&hl=de&user=72yq_tkAAAAJ&citation_for_view=72yq_tkAAAAJ:u5HHmVD_uO8C|title=OpenEAR—introducing the Munich open-source emotion and affect recognition toolkit|first1=Florian|last1=Eyben|first2=Martin|last2=Wöllmer|first3=Björn|last3=Schuller|date=26 April 2018|publisher=IEEE|pages=1–6|via=Google Scholar}}</ref> have been cited in more than 1000 scientific publications until today.

== References ==
{{reflist}}

== External links ==
* [https://www.audeering.com/opensmile/ openSMILE website]
* [https://github.com/audeering/opensmile openSMILE on GitHub]
* [https://audeering.github.io/opensmile/ openSMILE documentation]
* [https://scholar.google.de/citations?view_op=view_citation&hl=de&user=72yq_tkAAAAJ&citation_for_view=72yq_tkAAAAJ:9yKSN-GCB0IC Google Scholar page on openSMILE]
* [https://scholar.google.de/citations?view_op=view_citation&hl=de&user=72yq_tkAAAAJ&citation_for_view=72yq_tkAAAAJ:u5HHmVD_uO8C Google Scholar page on openEAR]
* [https://www.startupvalley.news/uk/audeering-audio-intelligence-technology/ Article on startupvalley.com]

[[Category:Audio software]]
[[Category:Affective computing]]