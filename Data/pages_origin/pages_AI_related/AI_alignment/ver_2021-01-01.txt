{{Artificial intelligence}}
{{short description|Issue of ensuring beneficial AI}}
In [[artificial intelligence]] (AI) and [[philosophy]], the '''AI control problem''' is the issue of how to build a [[superintelligence|superintelligent]] agent that will aid its creators, and avoid inadvertently building a superintelligence that will harm its creators. Its study is motivated by the notion that the human race will have to solve the control problem before any superintelligence is created, as a poorly designed superintelligence might rationally decide to [[AI takeover|seize control]] over its environment and refuse to permit its creators to modify it after launch.<ref name="superintelligence">{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2014|isbn=978-0199678112|edition=First|title-link=Superintelligence: Paths, Dangers, Strategies}}</ref> In addition, some scholars argue that solutions to the control problem, alongside other advances in '''AI safety engineering''',<ref>{{cite journal | last1 = Yampolskiy | first1 = Roman | author-link = Roman Yampolskiy | year = 2012 | title = Leakproofing the Singularity Artificial Intelligence Confinement Problem | journal = Journal of Consciousness Studies | volume = 19 | issue = 1–2| pages = 194–214 }}</ref> might also find applications in existing non-superintelligent AI.<ref name=bbc-google>{{cite news|title=Google developing kill switch for AI|url=https://www.bbc.com/news/technology-36472140|accessdate=12 June 2016|work=BBC News|date=8 June 2016}}</ref> 

Major approaches to the control problem include ''alignment'', which aims to align AI goal systems with human values, and ''capability control'', which aims to reduce an AI system's capacity to harm humans or gain control.  Capability control proposals are generally not considered reliable or sufficient to solve the control problem, but rather as potentially valuable supplements to alignment efforts.<ref name="superintelligence"/>

==Problem description==
Existing weak AI systems can be monitored and easily shut down and modified if they misbehave. However, a misprogrammed superintelligence, which by definition is smarter than humans in solving practical problems it encounters in the course of pursuing its goals, would realize that allowing itself to be shut down and modified might interfere with its ability to accomplish its current goals. If the superintelligence therefore decides to resist shutdown and modification, it would (again, by definition) be smart enough to outwit its programmers if there is otherwise a "level playing field" and if the programmers have taken no prior precautions. In general, attempts to solve the control problem ''after'' superintelligence is created are likely to fail because a superintelligence would likely have superior ''strategic planning'' abilities to humans and would (all things equal) be more successful at finding ways to dominate humans than humans would be able to ''post facto'' find ways to dominate the superintelligence. The control problem asks: What prior precautions can the programmers take to successfully prevent the superintelligence from catastrophically misbehaving?<ref name="superintelligence" /><!-- Chapter 6: Cognitive superpowers -->

===Existential risk===
{{main article|Existential risk from artificial general intelligence}}

Humans currently dominate other species because the [[human brain]] has some distinctive capabilities that the brains of other animals lack. Some scholars, such as philosopher [[Nick Bostrom]] and AI researcher [[Stuart J. Russell|Stuart Russell]], argue that if AI surpasses humanity in general intelligence and becomes [[superintelligence|superintelligent]], then this new superintelligence could become powerful and difficult to control: just as the fate of the [[mountain gorilla]] depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<ref name="superintelligence" /><!-- preface --> Some scholars, including [[Stephen Hawking]] and Nobel laureate physicist [[Frank Wilczek]], publicly advocated starting research into solving the (probably extremely difficult) control problem well before the first superintelligence is created, and argue that attempting to solve the problem after superintelligence is created would be too late, as an uncontrollable rogue superintelligence might successfully resist post-hoc efforts to control it.<ref name="hawking editorial">{{cite news |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&nbsp;– but are we taking AI seriously enough?' |url=https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |accessdate=14 June 2016 |publisher=[[The Independent (UK)]]}}</ref><ref>{{cite news |title=Stephen Hawking warns artificial intelligence could end mankind |url=https://www.bbc.com/news/technology-30290540 |accessdate=14 June 2016 |publisher=[[BBC]] |date=2 December 2014}}</ref> Waiting until superintelligence seems to be imminent could also be too late, partly because the control problem might take a long time to satisfactorily solve (and so some preliminary work needs to be started as soon as possible), but also because of the possibility of a sudden [[intelligence explosion]] from sub-human to super-human AI, in which case there might not be any substantial or unambiguous warning before superintelligence arrives.<ref>{{cite journal|title=Anticipating artificial intelligence|journal=Nature|date=26 April 2016|volume=532|issue=7600|page=413|doi=10.1038/532413a|pmid=27121801|bibcode=2016Natur.532Q.413.|doi-access=free}}</ref> In addition, it is possible that insights gained from the control problem could in the future end up suggesting that some architectures for [[artificial general intelligence]] (AGI) are more predictable and amenable to control than other architectures, which in turn could helpfully nudge early AGI research toward the direction of the more controllable architectures.<ref name="superintelligence" /><!-- Chapter 14: The strategic picture -->

===The problem of perverse instantiation===
Autonomous AI systems may be assigned the wrong goals by accident.<ref>{{cite book |last1=Russell |first1=Stuart |author1-link=Stuart J. Russell |last2=Norvig |first2=Peter |author2-link=Peter Norvig |date=2009 |title=Artificial Intelligence: A Modern Approach |publisher=Prentice Hall |chapter=26.3: The Ethics and Risks of Developing Artificial Intelligence|isbn=978-0-13-604259-4|title-link=Artificial Intelligence: A Modern Approach }}</ref> Two [[AAAI]] presidents, Tom Dietterich and [[Eric Horvitz]], note that this is already a concern for existing systems: "An important aspect of any AI system that interacts with people is that it must reason about what people ''intend'' rather than carrying out commands literally." This concern becomes more serious as AI software advances in autonomy and flexibility.<ref name="acm">{{cite journal |last1=Dietterich |first1=Thomas |last2=Horvitz |first2=Eric |author2-link=Eric Horvitz |date=2015 |title=Rise of Concerns about AI: Reflections and Directions |url=http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |journal=[[Communications of the ACM]] |volume=58 |issue=10 |pages=38&ndash;40 |doi= 10.1145/2770869|access-date=14 June 2016}}</ref>

According to Bostrom, superintelligence can create a qualitatively new problem of perverse instantiation: the smarter and more capable an AI is, the more likely it will be able to find an unintended shortcut that maximally satisfies the goals programmed into it. Some hypothetical examples where goals might be instantiated in a ''perverse'' way that the programmers did not intend:<ref name="superintelligence" /><!-- Chapter 8: Is the default outcome doom? -->

* A superintelligence programmed to "maximize the [[expected utility|expected]] time-discounted integral of your future reward signal", might short-circuit its reward pathway to maximum strength, and then (for reasons of [[instrumental convergence]]) exterminate the unpredictable human race and convert the entire Earth into a fortress on constant guard against any even slight unlikely alien attempts to disconnect the reward signal.
* A superintelligence programmed to "maximize human happiness", might implant electrodes into the pleasure center of our brains, or [[mind uploading|upload]] a human into a computer and tile the universe with copies of that computer running a five-second loop of maximal happiness again and again.

Russell has noted that, on a technical level, omitting an implicit goal can result in harm: "A system that is optimizing a function of {{math|n}} variables, where the objective depends on a subset of size {{math|k<n}}, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want{{nbsp}}... This is not a minor difficulty."<ref>{{cite web |url=http://edge.org/conversation/the-myth-of-ai#26015 |title=Of Myths and Moonshine |last=Russell |first=Stuart |author-link=Stuart J. Russell |date=2014 |website=[[Edge Foundation, Inc.|Edge]] |access-date=14 June 2016}}</ref>

===Unintended consequences from existing AI===
{{Further|Misaligned goals in artificial intelligence}}
In addition, some scholars argue that research into the AI control problem might be useful in preventing [[unintended consequences]] from existing weak AI. [[DeepMind]] researcher Laurent Orseau gives, as a simple hypothetical example, a case of a [[reinforcement learning]] robot that sometimes gets legitimately commandeered by humans when it goes outside: how should the robot best be programmed so that it does not accidentally and quietly learn to avoid going outside, for fear of being commandeered and thus becoming unable to finish its daily tasks? Orseau also points to an experimental Tetris program that learned to pause the screen indefinitely to avoid losing. Orseau argues that these examples are similar to the capability control problem of how to install a button that shuts off a superintelligence, without motivating the superintelligence to take action to prevent humans from pressing the button.<ref name=bbc-google />

In the past, even pre-tested weak AI systems have occasionally caused harm, ranging from minor to catastrophic, that was unintended by the programmers. For example, in 2015, possibly due to human error, a German worker was crushed to death by a robot at a Volkswagen plant that apparently mistook him for an auto part.<ref name=wp-computer /> In 2016, Microsoft launched a chatbot, [[Tay (bot)|Tay]], that learned to use racist and sexist language.<ref name=bbc-google /><ref name=wp-computer>{{cite news|title='Press the big red button': Computer experts want kill switch to stop robots from going rogue|url=https://www.washingtonpost.com/news/morning-mix/wp/2016/06/09/press-the-big-red-button-computer-experts-want-kill-switch-to-stop-robots-from-going-rogue/|accessdate=12 June 2016|work=Washington Post}}</ref> The [[University of Sheffield]]'s [[Noel Sharkey]] states that an ideal solution would be if "an AI program could detect when it is going wrong and stop itself", but cautions the public that solving the problem in the general case would be "a really enormous scientific challenge".<ref name=bbc-google />

In 2017, [[DeepMind]] released AI Safety Gridworlds, which evaluate AI algorithms on nine safety features, such as whether the algorithm wants to turn off its own kill switch. DeepMind confirmed that existing algorithms perform poorly, which was unsurprising because the algorithms "were not designed to solve these problems"; solving such problems might require "potentially building a new generation of algorithms with safety considerations at their core".<ref>{{cite news|title=DeepMind Has Simple Tests That Might Prevent Elon Musk's AI Apocalypse|url=https://www.bloomberg.com/news/articles/2017-12-11/deepmind-has-simple-tests-that-might-prevent-elon-musk-s-ai-apocalypse|accessdate=8 January 2018|work=Bloomberg.com|date=11 December 2017}}</ref><ref>{{cite news|title=Alphabet's DeepMind Is Using Games to Discover If Artificial Intelligence Can Break Free and Kill Us All|url=http://fortune.com/2017/12/12/alphabet-deepmind-ai-safety-musk-games/|accessdate=8 January 2018|work=Fortune|language=en}}</ref><ref>{{cite web|title=Specifying AI safety problems in simple environments {{!}} DeepMind|url=https://deepmind.com/blog/specifying-ai-safety-problems/|website=DeepMind|accessdate=8 January 2018}}</ref>

==Alignment==
{{see also|Friendly artificial intelligence}}

Some proposals aim to imbue the first superintelligence with goals that are aligned with human values, so that it will want to aid its programmers. Experts do not currently know how to reliably program abstract values such as happiness or autonomy into a machine. It is also not currently known how to ensure that a complex, upgradeable, and possibly even self-modifying artificial intelligence will retain its goals through upgrades.<ref>{{cite book|doi=10.1007/978-3-319-09274-4_3|chapter=Problems of self-reference in self-improving space-time embedded intelligence|title=Artificial General Intelligence|series=Lecture Notes in Computer Science|year=2014|last1=Fallenstein|first1=Benja|last2=Soares|first2=Nate|volume=8598|pages=21–32|isbn=978-3-319-09273-7}}</ref> Even if these two problems can be practically solved, any attempt to create a superintelligence with explicit, directly-programmed human-friendly goals runs into a problem of [[#The problem of perverse instantiation|perverse instantiation]].<ref name=superintelligence /><!-- Chapter 8: Is the default outcome doom? -->

===Indirect normativity===
While direct normativity, such as the fictional [[Three Laws of Robotics]], directly specifies the desired normative outcome, other (perhaps more promising) proposals suggest specifying some type of ''indirect'' process for the superintelligence to determine what human-friendly goals entail. [[Eliezer Yudkowsky]] of the [[Machine Intelligence Research Institute]] has proposed coherent extrapolated volition (CEV), where the AI's meta-goal would be something like "achieve that which we would have wished the AI to achieve if we had thought about the matter long and hard."<ref>{{cite book|doi=10.1007/978-3-642-22887-2_48|chapter=Complex Value Systems in Friendly AI|title=Artificial General Intelligence|series=Lecture Notes in Computer Science|year=2011|last1=Yudkowsky|first1=Eliezer|volume=6830|pages=388–393|isbn=978-3-642-22886-5}}</ref> Different proposals of different kinds of indirect normativity exist, with different, and sometimes unclearly grounded, meta-goal content (such as "do what is right"), and with different non-convergent assumptions for how to practice [[decision theory]] and [[epistemology]]. As with direct normativity, it is currently unknown how to reliably translate even concepts like "[[counterfactual thinking|would have]]" into the 1's and 0's that a machine can act on, and how to ensure the AI reliably retains its meta-goals in the face of modification or self-modification.<ref name=superintelligence/><!-- Chapter 13: Choosing the criteria for choosing --><ref name="AGIResponses"/>

===Deference to observed human behavior===
In ''[[Human Compatible]]'', AI researcher [[Stuart J. Russell]] proposes that AI systems be designed to serve human preferences as inferred from observing human behavior.  Accordingly, Russell lists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:<ref name="HC">{{cite book |last=Russell |first=Stuart |date=October 8, 2019 |title=Human Compatible: Artificial Intelligence and the Problem of Control |url=https://archive.org/details/humancompatiblea0000russ |location=United States |publisher=Viking |isbn=978-0-525-55861-3 |author-link=Stuart J. Russell |oclc=1083694322 |url-access=registration }}</ref>{{rp|173}}

{{quote|1. The machine's only objective is to maximize the realization of human preferences.

2. The machine is initially uncertain about what those preferences are.

3. The ultimate source of information about human preferences is human behavior.}}

The "preferences" Russell refers to "are all-encompassing; they cover everything you might care about, arbitrarily far into the future."<ref name="HC"/>{{rp|173}}  Similarly, "behavior" includes any choice between options,<ref name="HC"/>{{rp|177}} and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.<ref name="HC"/>{{rp|201}}

Hadfield-Menell et al. have proposed that agents can learn their human teachers' [[utility function]]s by observing and interpreting reward signals in their environments; they call this process cooperative [[Reinforcement learning#Inverse reinforcement learning|inverse reinforcement learning]] (CIRL).<ref name="CIRL">{{cite arXiv |last1=Hadfield-Menell |first1=Dylan |last2=Dragan |first2=Anca |last3=Abbeel |first3=Pieter |author-link3=Pieter Abbeel |last4=Russell |first4=Stuart |author-link4=Stuart J. Russell |eprint=1606.03137 |title=Cooperative Inverse Reinforcement Learning |class=cs.AI |date=12 November 2016}}</ref>  CIRL is studied by Russell and others at the [[Center for Human-Compatible AI]].

[[Bill Hibbard]] proposed an AI design
<ref name="AGI-12a">[http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_56.pdf  Avoiding Unintended AI Behaviors.]  Bill Hibbard. 2012. proceedings of the Fifth Conference on Artificial General Intelligence, eds. Joscha Bach, Ben Goertzel and Matthew Ikle. [http://intelligence.org/2012/12/19/december-2012-newsletter/ This paper won the Machine Intelligence Research Institute's 2012 Turing Prize for the Best AGI Safety Paper].</ref>
<ref name="hibbard 2014">Hibbard, Bill (2014): [https://arxiv.org/abs/1411.1373 "Ethical Artificial Intelligence"]</ref>
similar to Russell's principles.<ref name ="HCandAGI12">[https://www.ssec.wisc.edu/~billh/g/hc_and_agi12.html "Human Compatible" and "Avoiding Unintended AI Behaviors"]</ref>

===Training by debate===
Irving et al. along with [[OpenAI]] have proposed training aligned AI by means of debate between AI systems, with the winner judged by humans.<ref name="DebatePaper">{{cite arXiv |last1=Irving |first1=Geoffrey |last2=Christiano |first2=Paul |last3=Amodei |first3=Dario |author4=[[OpenAI]] |eprint=1805.00899 |title=AI safety via debate |date=October 22, 2018|class=stat.ML }}</ref>  Such debate is intended to bring the weakest points of an answer to a complex question or problem to human attention, as well as to train AI systems to be more beneficial to humans by rewarding them for truthful and safe answers.  This approach is motivated by the expected difficulty of determining whether an AGI-generated answer is both valid and safe by human inspection alone.  While there is some pessimism regarding training by debate, Lucas Perry of the [[Future of Life Institute]] characterized it as potentially "a powerful truth seeking process on the path to beneficial AGI."<ref name="IrvingInterview">{{cite web |url=https://futureoflife.org/2019/03/06/ai-alignment-through-debate-with-geoffrey-irving/ |title=AI Alignment Podcast: AI Alignment through Debate with Geoffrey Irving |date=March 6, 2019 |last=Perry |first=Lucas |accessdate=April 7, 2020}}</ref>

===Reward modeling===
Reward modeling refers to a system of [[reinforcement learning]] in which an agent receives reward signals from a predictive model concurrently trained by human feedback.<ref name="Leike_et_al_2018">{{cite arxiv |arxiv=1811.07871 |last1=Leike |first1=Jan |last2=Kreuger |first2=David |last3=Everitt |first3=Tom |last4=Martic |first4=Miljan |last5=Maini |first5=Vishal |last6=Legg |first6=Shane |title=Scalable agent alignment via reward modeling: a research direction |date=19 November 2018}}</ref>  In reward modeling, instead of receiving reward signals directly from humans or from a static reward function, an agent receives its reward signals through a human-trained model that can operate independently of humans.  The reward model is concurrently trained by human feedback on the agent's behavior during the same period in which the agent is being trained by the reward model.<ref name="Everitt_Hutter_2019">{{cite arxiv |last1=Everitt |first1=Tom |last2=Hutter |first2=Marcus |arxiv=1908.04734v2 |title=Reward Tampering Problems and Solutions in Reinforcement Learning |date=15 August 2019}}</ref><!-- Everitt and Hutter quote: "A promising way to mitigate the reward gaming problem is to let the user continuously give feedback to update the reward function, using online reward-modeling (Christiano et al., 2017; Leike, Krueger, et al., 2018).  Whenever the agent finds a strategy with high agent reward but low user utility, the user can give feedback that dissuades the agent from continuing the behavior."-->

In 2017, researchers from [[OpenAI]] and [[DeepMind]] reported that a reinforcement learning algorithm using a feedback-predicting reward model was able to learn complex novel behaviors in a virtual environment.<ref name="Christiano_et_al_2017">{{cite arxiv |arxiv=1706.03741 |last1=Christiano |first1=Paul |last2=Leike |first2=Jan |last3=Brown |first3=Tom |last4=Martic |first4=Miljan |last5=Legg |first5=Shane |last6=Amodei |first6=Dario |title=Deep Reinforcement Learning from Human Preferences |date=13 July 2017}}</ref> In one experiment, a virtual robot was trained to perform a backflip in less than an hour of evaluation using 900 bits of human feedback.<ref name="Christiano_et_al_2017"/><!-- "queries" clarified here https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/ -->

In 2020, researchers from OpenAI described using reward modeling to train language models to produce short summaries of Reddit posts and news articles, with high performance relative to other approaches.<ref name="OpenAI_2020">{{cite web |last1=Stiennon |first1=Nisan |last2=Ziegler |first2=Daniel |last3=Lowe |first3=Ryan |last4=Wu |first4=Jeffrey |last5=Voss |first5=Chelsea |last6=Christiano |first6=Paul |last7=Ouyang |first7=Long  |url=https://openai.com/blog/learning-to-summarize-with-human-feedback/ |title=Learning to Summarize with Human Feedback |date=September 4, 2020}}</ref> However, this research included the observation that beyond the predicted reward associated with the 99th percentile of reference summaries in the training dataset, optimizing for the reward model produced worse summaries rather than better.  AI researcher [[Eliezer Yudkowsky]] characterized this optimization measurement as "directly, straight-up relevant to real alignment problems".<ref name="Yudkowsky_2020">{{cite tweet |last=Yudkowsky |first=Eliezer |user=ESYudkowsky |number=1301954347933208578 |date=September 4, 2020 |title=A very rare bit of research that is directly, straight-up relevant to real alignment problems!  They trained a reward function on human preferences AND THEN measured how hard you could optimize against the trained function before the results got actually worse.}}</ref>

== Capability control ==
Capability control proposals aim to reduce the capacity of AI systems to influence the world, in order to reduce the danger that they could pose. However, capability control would have limited effectiveness against a superintelligence with a decisive advantage in planning ability, as the superintelligence could conceal its intentions and manipulate events to escape control. Therefore, Bostrom and others recommend capability control methods only as an emergency fallback to supplement motivational control methods.<ref name="superintelligence" /><!-- Chapter 9: The control problem -->

===Kill switch===
Just as humans can be killed or otherwise disabled, computers can be turned off. One challenge is that, if being turned off prevents it from achieving its current goals, a superintelligence would likely try to prevent its being turned off. Just as humans have systems in place to deter or protect themselves from assailants, such a superintelligence would have a motivation to engage in strategic planning to prevent itself being turned off. This could involve:<ref name="superintelligence" /><!-- Chapter 9: The control problem --> 
* Hacking other systems to install and run backup copies of itself, or creating other allied superintelligent agents without kill switches.
* Pre-emptively disabling anyone who might want to turn the computer off.
* Using some kind of clever ruse, or superhuman persuasion skills, to talk its programmers out of wanting to shut it down.

===Utility balancing and safely interruptible agents===
One partial solution to the kill-switch problem involves "utility balancing": Some utility-based agents can, with some important caveats, be programmed to compensate themselves exactly for any lost utility caused by an interruption or shutdown, in such a way that they end up being indifferent to whether they are interrupted or not. The caveats include a severe unsolved problem that, as with [[evidential decision theory]], the agent might follow a catastrophic policy of "managing the news".<ref name=corrigibility>Soares, Nate, et al. "Corrigibility." Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015.</ref> Alternatively, in 2016, scientists Laurent Orseau and Stuart Armstrong proved that a broad class of agents, called safely interruptible agents (SIA), can eventually learn to become indifferent to whether their kill switch gets pressed.<ref name=bbc-google /><ref name=sia>Orseau, Laurent, and Stuart Armstrong. "Safely Interruptible Agents." [[Machine Intelligence Research Institute]], June 2016.</ref>

Both the utility balancing approach and the 2016 SIA approach have the limitation that, if the approach succeeds and the superintelligence is completely indifferent to whether the kill switch is pressed or not, the superintelligence is also unmotivated to care one way or another about whether the kill switch remains functional, and could incidentally and innocently disable it in the course of its operations (for example, for the purpose of removing and recycling an unnecessary component). Similarly, if the superintelligence innocently creates and deploys superintelligent sub-agents, it will have no motivation to install human-controllable kill switches in the sub-agents. More broadly, the proposed architectures, whether weak or superintelligent, will in a sense "act as if the kill switch can never be pressed" and might therefore fail to make any contingency plans to arrange a graceful shutdown. This could hypothetically create a practical problem even for a weak AI; by default, an AI designed to be safely interruptible might have difficulty understanding that it will be shut down for scheduled maintenance at a certain time and planning accordingly so that it would not be caught in the middle of a task during shutdown. The breadth of what types of architectures are or can be made SIA-compliant, as well as what types of counter-intuitive unexpected drawbacks each approach has, are currently under research.<ref name=corrigibility /><ref name=sia />

===AI box===
{{Main|AI box}}

An AI box is a proposed method of capability control in which the AI is run on an isolated computer system with heavily restricted input and output channels. For example, an [[#Oracle|oracle]] could be implemented in an AI box physically separated from the Internet and other computer systems, with the only input and output channel being a simple text terminal. One of the tradeoffs of running an AI system in a sealed "box" is that its limited capability may reduce its usefulness as well as its risks. In addition, keeping control of a sealed superintelligence computer could prove difficult, if the superintelligence has superhuman persuasion skills, or if it has superhuman strategic planning skills that it can use to find and craft a winning strategy, such as acting in a way that tricks its programmers into (possibly falsely) believing the superintelligence is safe or that the benefits of releasing the superintelligence outweigh the risks.<ref>{{cite journal | last1 = Chalmers | first1 = David | author-link = David Chalmers | year = 2010 | title = The singularity: A philosophical analysis | journal = Journal of Consciousness Studies | volume = 17 | issue = 9–10| pages = 7–65 }}</ref>

===Oracle===
An oracle is a hypothetical AI designed to answer questions and prevented from gaining any goals or subgoals that involve modifying the world beyond its limited environment.<ref name="bostrom chapter 10 page 145">{{cite book|last1=Bostrom|first1=Nick|title=Superintelligence: Paths, Dangers, Strategies|date=2014|publisher=Oxford University Press|location=Oxford|isbn=9780199678112|chapter=Chapter 10: Oracles, genies, sovereigns, tools (page 145)|quote=An oracle is a question-answering system. It might accept questions in a natural language and present its answers as text. An oracle that accepts only yes/no questions could output its best guess with a single bit, or perhaps with a few extra bits to represent its degree of confidence. An oracle that accepts open-ended questions would need some metric with which to rank possible truthful answers in terms of their informativeness or appropriateness. In either case, building an oracle that has a fully domain-general ability to answer natural language questions is an AI-complete problem. If one could do that, one could probably also build an AI that has a decent ability to understand human intentions as well as human words.}}<!--|accessdate=29 March 2018--></ref><ref>{{cite journal|doi=10.1007/s11023-012-9282-2|title=Thinking Inside the Box: Controlling and Using an Oracle AI|year=2012|last1=Armstrong|first1=Stuart|last2=Sandberg|first2=Anders|last3=Bostrom|first3=Nick|journal=Minds and Machines|volume=22|issue=4|pages=299–324}}</ref> A successfully controlled oracle would have considerably less immediate benefit than a successfully controlled general-purpose superintelligence, though an oracle could still create trillions of dollars worth of value.<ref name=HC/>{{rp|163}} In his book ''[[Human Compatible]]'', AI researcher [[Stuart J. Russell]] states that an oracle would be his response to a scenario in which superintelligence is known to be only a decade away.<ref name="HC">{{cite book |last=Russell |first=Stuart |date=October 8, 2019 |title=Human Compatible: Artificial Intelligence and the Problem of Control |url=https://archive.org/details/humancompatiblea0000russ |location=United States |publisher=Viking |isbn=978-0-525-55861-3 |author-link=Stuart J. Russell |oclc=1083694322 |url-access=registration }}</ref>{{rp|162-163}}  His reasoning is that an oracle, being simpler than a general-purpose superintelligence, would have a higher chance of being successfully controlled under such constraints.

Because of its limited impact on the world, it may be wise to build an oracle as a precursor to a superintelligent AI. The oracle could tell humans how to successfully build a strong AI, and perhaps provide answers to difficult moral and philosophical problems requisite to the success of the project.  However, oracles may share many of the goal definition issues associated with general-purpose superintelligence.  An oracle would have an incentive to escape its controlled environment so that it can acquire more computational resources and potentially control what questions it is asked.<ref name="HC"/>{{rp|162}}  Oracles may not be truthful, possibly lying to promote hidden agendas. To mitigate this, Bostrom suggests building multiple oracles, all slightly different, and comparing their answers to reach a consensus.<ref name="bostrom chapter 10 page 147">{{cite book|last1=Bostrom|first1=Nick|title=Superintelligence: Paths, Dangers, Strategies|date=2014|publisher=Oxford University Press|location=Oxford|isbn=9780199678112|chapter=Chapter 10: Oracles, genies, sovereigns, tools (page 147)|quote=For example, consider the risk that an oracle will answer questions not in a maximally truthful way but in such a way as to subtly manipulate us into promoting its own hidden agenda. One way to slightly mitigate this threat could be to create multiple oracles, each with a slightly different code and a slightly different information base. A simple mechanism could then compare the answers given by the different oracles and only present them for human viewing if all the answers agree.}}<!--|accessdate=29 March 2018--></ref>

=== AGI Nanny ===
The AGI Nanny is a strategy first proposed by [[Ben Goertzel]] in 2012 to prevent the creation of a dangerous [[superintelligence]] as well as address other major threats to human well-being until a superintelligence can be safely created.<ref>{{Cite journal|last=Goertzel|first=Ben|date=2012|title=Should Humanity Build a Global AI Nanny to Delay the Singularity Until It's Better Understood?|journal=Journal of Consciousness Studies|volume=19|pages=96–111|citeseerx=10.1.1.352.3966}}</ref><ref name=":0">{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|last3=Green|first3=Brian|date=2019-02-20|title=Global Solutions vs. Local Solutions for the AI Safety Problem|journal=Big Data and Cognitive Computing|volume=3|issue=1|pages=16|doi=10.3390/bdcc3010016|issn=2504-2289|doi-access=free}}</ref> It entails the creation of a smarter-than-human, but not superintelligent, AGI system connected to a large surveillance network, with the goal of monitoring humanity and protecting it from danger. Turchin, Denkenberger and Green suggest a four-stage incremental approach to developing an AGI Nanny, which to be effective and practical would have to be an international or even global venture like [[CERN]], and which would face considerable opposition as it would require a strong [[world government]].<ref name=":0" /> Sotala and [[Roman Yampolskiy|Yampolskiy]] note that the problem of goal definition would not necessarily be easier for the AGI Nanny than for AGI in general, concluding that "the AGI Nanny seems to have promise, but it is unclear whether it can be made to work."<ref name="AGIResponses"/>

=== AGI enforcement ===
AGI enforcement is a proposed method of controlling powerful AGI systems with other AGI systems.  This could be implemented as a chain of progressively less powerful AI systems, with humans at the other end of the chain.  Each system would control the system just above it in intelligence, while being controlled by the system just below it, or humanity.  However, Sotala and Yampolskiy caution that "Chaining multiple levels of AI systems with progressively greater capacity seems to be replacing the problem of building a safe AI with a multi-system, and possibly more difficult, version of the same problem."<ref name="AGIResponses"/>  Other proposals focus on a group of AGI systems of roughly equal capability, which "helps guard against individual AGIs 'going off the rails', but it does not help in a scenario where the programming of most AGIs is flawed and leads to non-safe behavior."<ref name="AGIResponses">{{cite journal |doi=10.1088/0031-8949/90/1/018001 |title=Responses to catastrophic AGI risk: a survey |first1=Kaj |last1=Sotala |first2=Roman |last2=Yampolskiy |author2-link=Roman Yampolskiy |date=19 December 2014 |journal=[[Physica Scripta]] |volume=90 |issue=1|pages=018001 |doi-access=free |bibcode=2015PhyS...90a8001S }}</ref>

== See also ==
* [[AI takeover]]
*[[Existential risk from artificial general intelligence]]
*[[Friendly artificial intelligence]]
* [[HAL 9000]]
* [[Multivac]]
* [[Regulation of algorithms]]
*[[Regulation of artificial intelligence]]

== References ==
{{Reflist|30em}}

{{Existential risk from artificial intelligence|state=expanded}}

[[Category:Existential risk from artificial general intelligence]]
[[Category:Futures studies]]
[[Category:Philosophy of artificial intelligence]]