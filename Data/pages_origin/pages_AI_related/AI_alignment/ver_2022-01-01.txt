{{Short description|Issue of ensuring beneficial AI}}
{{use mdy dates|date=September 2021}}
{{merge|Existential risk from artificial general intelligence|AI takeover|AI takeovers in popular culture|date=August 2021|discuss=Talk:Existential_risk_from_artificial_general_intelligence#Merge_is_still_needed}}
{{Artificial intelligence}}
{{Use American English|date=February 2021}}
In [[artificial intelligence]] (AI) and [[philosophy]], the '''AI control problem''' is the issue of how to build AI systems such that they will aid their creators, and avoid inadvertently building systems that will harm their creators. One particular concern is that humanity will have to solve the control problem before a [[superintelligence|superintelligent]] AI system is created, as a poorly designed superintelligence might rationally decide to [[AI takeover|seize control]] over its environment and refuse to permit its creators to modify it after launch.<ref name="superintelligence">{{Cite book |last=Bostrom |first=Nick |title=Superintelligence: Paths, Dangers, Strategies |title-link=Superintelligence: Paths, Dangers, Strategies |date=2014 |isbn=978-0199678112 |edition=First |author-link=Nick Bostrom}}</ref> In addition, some scholars argue that solutions to the control problem, alongside other advances in '''AI safety engineering''',<ref>{{Cite journal |last=Yampolskiy |first=Roman |author-link=Roman Yampolskiy |year=2012 |title=Leakproofing the Singularity Artificial Intelligence Confinement Problem |journal=Journal of Consciousness Studies |volume=19 |issue=1–2 |pages=194–214}}</ref> might also find applications in existing non-superintelligent AI.<ref name="bbc-google">{{Cite news |date=8 June 2016 |title=Google developing kill switch for AI |work=BBC News |url=https://www.bbc.com/news/technology-36472140 |url-status=live |access-date=12 June 2016 |archive-url=https://web.archive.org/web/20160611042244/http://www.bbc.com/news/technology-36472140 |archive-date=11 June 2016}}</ref>

Major approaches to the control problem include ''alignment'', which aims to align AI goal systems with human values, and ''capability control'', which aims to reduce an AI system's capacity to harm humans or gain control. Capability control proposals are generally not considered reliable or sufficient to solve the control problem, but rather as potentially valuable supplements to alignment efforts.<ref name="superintelligence" />

==Problem description==
Existing weak AI systems can be monitored and easily shut down and modified if they misbehave. However, a misprogrammed superintelligence, which by definition is smarter than humans in solving practical problems it encounters in the course of pursuing its goals, would realize that allowing itself to be shut down and modified might interfere with its ability to accomplish its current goals. If the superintelligence therefore decides to resist shutdown and modification, it would (again, by definition) be smart enough to outwit its programmers if there is otherwise a "level playing field" and if the programmers have taken no prior precautions. In general, attempts to solve the control problem ''after'' superintelligence is created are likely to fail because a superintelligence would likely have superior ''strategic planning'' abilities to humans and would (all things equal) be more successful at finding ways to dominate humans than humans would be able to ''post facto'' find ways to dominate the superintelligence. The control problem asks: What prior precautions can the programmers take to successfully prevent the superintelligence from catastrophically misbehaving?<ref name="superintelligence" /><!-- Chapter 6: Cognitive superpowers -->

===Existential risk===
{{main article|Existential risk from artificial general intelligence}}

Humans currently dominate other species because the [[human brain]] has some distinctive capabilities that the brains of other animals lack. Some scholars, such as philosopher [[Nick Bostrom]] and AI researcher [[Stuart J. Russell|Stuart Russell]], argue that if AI surpasses humanity in general intelligence and becomes [[superintelligence|superintelligent]], then this new superintelligence could become powerful and difficult to control: just as the fate of the [[mountain gorilla]] depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<ref name="superintelligence" /><!-- preface --> Some scholars, including [[Stephen Hawking]] and Nobel laureate physicist [[Frank Wilczek]], publicly advocated starting research into solving the (probably extremely difficult) control problem well before the first superintelligence is created, and argue that attempting to solve the problem after superintelligence is created would be too late, as an uncontrollable rogue superintelligence might successfully resist post-hoc efforts to control it.<ref name="hawking editorial">{{Cite news |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&nbsp;– but are we taking AI seriously enough?' |publisher=[[The Independent (UK)]] |url=https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |url-status=live |access-date=14 June 2016 |archive-url=https://web.archive.org/web/20150925153716/http://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |archive-date=25 September 2015}}</ref><ref>{{Cite news |date=2 December 2014 |title=Stephen Hawking warns artificial intelligence could end mankind |publisher=[[BBC]] |url=https://www.bbc.com/news/technology-30290540 |url-status=live |access-date=14 June 2016 |archive-url=https://web.archive.org/web/20151030054329/http://www.bbc.com/news/technology-30290540 |archive-date=30 October 2015}}</ref> Waiting until superintelligence seems to be imminent could also be too late, partly because the control problem might take a long time to satisfactorily solve (and so some preliminary work needs to be started as soon as possible), but also because of the possibility of a sudden [[intelligence explosion]] from sub-human to super-human AI, in which case there might not be any substantial or unambiguous warning before superintelligence arrives.<ref>{{Cite journal |date=26 April 2016 |title=Anticipating artificial intelligence |journal=Nature |volume=532 |issue=7600 |page=413 |bibcode=2016Natur.532Q.413. |doi=10.1038/532413a |pmid=27121801 |doi-access=free}}</ref> In addition, it is possible that insights gained from the control problem could in the future end up suggesting that some architectures for [[artificial general intelligence]] (AGI) are more predictable and amenable to control than other architectures, which in turn could helpfully nudge early AGI research toward the direction of the more controllable architectures.<ref name="superintelligence" /><!-- Chapter 14: The strategic picture -->

===The problem of perverse instantiation===
Autonomous AI systems may be assigned the wrong goals by accident.<ref>{{Cite book |last1=Russell |first1=Stuart |title=Artificial Intelligence: A Modern Approach |title-link=Artificial Intelligence: A Modern Approach |last2=Norvig |first2=Peter |date=2009 |publisher=Prentice Hall |isbn=978-0-13-604259-4 |chapter=26.3: The Ethics and Risks of Developing Artificial Intelligence |author-link=Stuart J. Russell |author-link2=Peter Norvig}}</ref> Two [[AAAI]] presidents, Tom Dietterich and [[Eric Horvitz]], note that this is already a concern for existing systems: "An important aspect of any AI system that interacts with people is that it must reason about what people ''intend'' rather than carrying out commands literally." This concern becomes more serious as AI software advances in autonomy and flexibility.<ref name="acm">{{Cite journal |last1=Dietterich |first1=Thomas |last2=Horvitz |first2=Eric |author-link2=Eric Horvitz |date=2015 |title=Rise of Concerns about AI: Reflections and Directions |url=http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |url-status=live |journal=[[Communications of the ACM]] |volume=58 |issue=10 |pages=38&ndash;40 |doi=10.1145/2770869 |archive-url=https://web.archive.org/web/20160304132930/http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |archive-date=4 March 2016 |access-date=14 June 2016 |s2cid=20395145}}</ref>

According to Bostrom, superintelligence can create a qualitatively new problem of perverse instantiation: the smarter and more capable an AI is, the more likely it will be able to find an unintended shortcut that maximally satisfies the goals programmed into it. Some hypothetical examples where goals might be instantiated in a ''perverse'' way that the programmers did not intend:<ref name="superintelligence" /><!-- Chapter 8: Is the default outcome doom? -->

* A superintelligence programmed to "maximize the [[expected utility|expected]] time-discounted integral of your future reward signal", might short-circuit its reward pathway to maximum strength, and then (for reasons of [[instrumental convergence]]) exterminate the unpredictable human race and convert the entire Earth into a fortress on constant guard against any even slight unlikely alien attempts to disconnect the reward signal.
* A superintelligence programmed to "maximize human happiness", might implant electrodes into the pleasure center of our brains, or [[mind uploading|upload]] a human into a computer and tile the universe with copies of that computer running a five-second loop of maximal happiness again and again.

Russell has noted that, on a technical level, omitting an implicit goal can result in harm: "A system that is optimizing a function of {{math|n}} variables, where the objective depends on a subset of size {{math|k<n}}, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want{{nbsp}}... This is not a minor difficulty."<ref>{{Cite web |last=Russell |first=Stuart |author-link=Stuart J. Russell |date=2014 |title=Of Myths and Moonshine |url=http://edge.org/conversation/the-myth-of-ai#26015 |url-status=live |archive-url=https://web.archive.org/web/20160719124525/https://www.edge.org/conversation/the-myth-of-ai#26015 |archive-date=19 July 2016 |access-date=14 June 2016 |website=[[Edge Foundation, Inc.|Edge]]}}</ref>

===Unintended consequences from existing AI===
{{Further|Misaligned goals in artificial intelligence}}
In addition, some scholars argue that research into the AI control problem might be useful in preventing [[unintended consequences]] from existing weak AI. [[DeepMind]] researcher Laurent Orseau gives, as a simple hypothetical example, a case of a [[reinforcement learning]] robot that sometimes gets legitimately commandeered by humans when it goes outside: how should the robot best be programmed so that it does not accidentally and quietly learn to avoid going outside, for fear of being commandeered and thus becoming unable to finish its daily tasks? Orseau also points to an experimental Tetris program that learned to pause the screen indefinitely to avoid losing. Orseau argues that these examples are similar to the capability control problem of how to install a button that shuts off a superintelligence, without motivating the superintelligence to take action to prevent humans from pressing the button.<ref name=bbc-google />

In the past, even pre-tested weak AI systems have occasionally caused harm, ranging from minor to catastrophic, that was unintended by the programmers. For example, in 2015, possibly due to human error, a German worker was crushed to death by a robot at a Volkswagen plant that apparently mistook him for an auto part.<ref name=wp-computer /> In 2016, Microsoft launched a chatbot, [[Tay (bot)|Tay]], that learned to use racist and sexist language.<ref name=bbc-google /><ref name="wp-computer">{{Cite news |title='Press the big red button': Computer experts want kill switch to stop robots from going rogue |newspaper=Washington Post |url=https://www.washingtonpost.com/news/morning-mix/wp/2016/06/09/press-the-big-red-button-computer-experts-want-kill-switch-to-stop-robots-from-going-rogue/ |url-status=live |access-date=12 June 2016 |archive-url=https://web.archive.org/web/20160612123032/https://www.washingtonpost.com/news/morning-mix/wp/2016/06/09/press-the-big-red-button-computer-experts-want-kill-switch-to-stop-robots-from-going-rogue/ |archive-date=12 June 2016}}</ref> The [[University of Sheffield]]'s [[Noel Sharkey]] states that an ideal solution would be if "an AI program could detect when it is going wrong and stop itself", but cautions the public that solving the problem in the general case would be "a really enormous scientific challenge".<ref name=bbc-google />

In 2017, [[DeepMind]] released AI Safety Gridworlds, which evaluate AI algorithms on nine safety features, such as whether the algorithm wants to turn off its own kill switch. DeepMind confirmed that existing algorithms perform poorly, which was unsurprising because the algorithms "were not designed to solve these problems"; solving such problems might require "potentially building a new generation of algorithms with safety considerations at their core".<ref>{{Cite news |date=11 December 2017 |title=DeepMind Has Simple Tests That Might Prevent Elon Musk's AI Apocalypse |work=Bloomberg.com |url=https://www.bloomberg.com/news/articles/2017-12-11/deepmind-has-simple-tests-that-might-prevent-elon-musk-s-ai-apocalypse |url-status=live |access-date=8 January 2018 |archive-url=https://web.archive.org/web/20180108180614/https://www.bloomberg.com/news/articles/2017-12-11/deepmind-has-simple-tests-that-might-prevent-elon-musk-s-ai-apocalypse |archive-date=8 January 2018}}</ref><ref>{{Cite news |title=Alphabet's DeepMind Is Using Games to Discover If Artificial Intelligence Can Break Free and Kill Us All |language=en |work=Fortune |url=http://fortune.com/2017/12/12/alphabet-deepmind-ai-safety-musk-games/ |url-status=live |access-date=8 January 2018 |archive-url=https://web.archive.org/web/20171231132517/http://fortune.com/2017/12/12/alphabet-deepmind-ai-safety-musk-games/ |archive-date=31 December 2017}}</ref><ref>{{Cite web |title=Specifying AI safety problems in simple environments {{!}} DeepMind |url=https://deepmind.com/blog/specifying-ai-safety-problems/ |url-status=live |archive-url=https://web.archive.org/web/20180102081413/https://deepmind.com/blog/specifying-ai-safety-problems/ |archive-date=2 January 2018 |access-date=8 January 2018 |website=DeepMind}}</ref>

== Alignment ==
{{see also|Friendly artificial intelligence}}

Some proposals seek to solve the problem of ''ambitious alignment'', creating AIs that remain safe even when they act autonomously at a large scale. Some aspects of alignment inherently have moral and political dimensions.<ref name="Gabriel">{{Cite journal |last=Gabriel |first=Iason |date=1 September 2020 |title=Artificial Intelligence, Values, and Alignment |url=https://link.springer.com/article/10.1007/s11023-020-09539-2 |url-status=live |journal=Minds and Machines |language=en |volume=30 |issue=3 |pages=411–437 |arxiv=2001.09768 |doi=10.1007/s11023-020-09539-2 |issn=1572-8641 |archive-url=https://web.archive.org/web/20210215081443/https://link.springer.com/article/10.1007/s11023-020-09539-2 |archive-date=15 February 2021 |access-date=7 February 2021 |s2cid=210920551}}</ref>
For example, in ''[[Human Compatible]]'', Berkeley professor [[Stuart J. Russell|Stuart Russell]] proposes that AI systems be designed with the sole objective of maximizing the realization of human preferences.<ref name="HC">{{Cite book |last=Russell |first=Stuart |url=https://archive.org/details/humancompatiblea0000russ |title=Human Compatible: Artificial Intelligence and the Problem of Control |date=October 8, 2019 |publisher=Viking |isbn=978-0-525-55861-3 |location=United States |oclc=1083694322 |author-link=Stuart J. Russell |url-access=registration}}</ref>{{rp|173}} The "preferences" Russell refers to "are all-encompassing; they cover everything you might care about, arbitrarily far into the future." AI ethics researcher Iason Gabriel argues that we should align AIs with "principles that would be supported by a global overlapping consensus of opinion, chosen behind a veil of ignorance and/or affirmed through democratic processes."<ref name="Gabriel" />

[[Eliezer Yudkowsky]] of the [[Machine Intelligence Research Institute]] has proposed the goal of fulfilling humanity's [[coherent extrapolated volition]] (CEV), roughly defined as the set of values which humanity would share at [[reflective equilibrium]], i.e. after a long, idealised process of refinement.<ref name="Gabriel" /><ref>{{Cite book |last=Yudkowsky |first=Eliezer |title=Artificial General Intelligence |year=2011 |isbn=978-3-642-22886-5 |series=Lecture Notes in Computer Science |volume=6830 |pages=388–393 |chapter=Complex Value Systems in Friendly AI |doi=10.1007/978-3-642-22887-2_48}}</ref>

By contrast, existing experimental ''narrowly aligned'' AIs are more pragmatic and can successfully carry out tasks in accordance with the user's immediate inferred preferences,<ref name="reward_modeling">{{Cite arXiv |eprint=1811.07871 |class=cs.LG |first1=Jan |last1=Leike |first2=David |last2=Krueger |title=Scalable agent alignment via reward modeling: a research direction |date=19 November 2018 |last3=Everitt |first3=Tom |last4=Martic |first4=Miljan |last5=Maini |first5=Vishal |last6=Legg |first6=Shane}}</ref> albeit without any understanding of the user's long-term goals. Narrow alignment can apply to AIs with general capabilities, but also to AIs that are specialized for individual tasks. For example, we would like [[question answering]] systems to respond to questions truthfully without selecting their answers to manipulate humans or bring about long-term effects.

=== Inner and outer alignment ===
Some AI control proposals account for both a base explicit objective function and an emergent implicit objective function. Such proposals attempt to harmonize three different descriptions of the AI system:<ref name="DM_safety_overview">{{Cite web |last1=Ortega |first1=Pedro |last2=Maini |first2=Vishal |last3=DeepMind Safety Team |date=27 September 2018 |title=Building safe artificial intelligence: specification, robustness, and assurance |url=https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1 |url-status=live |archive-url=https://web.archive.org/web/20201212004820/https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1 |archive-date=12 December 2020 |access-date=12 December 2020 |website=Medium |language=en}}</ref>

# Ideal specification: what the human operator wishes the system to do, which may be poorly articulated. ("Play a good game of ''CoastRunners''.")
# Design specification: the blueprint that is actually used to build the AI system. ("Maximize your score at ''CoastRunners''.") In a reinforcement learning system, this might simply be the system's reward function.
# Emergent behavior: what the AI actually does.

Because AI systems are not perfect optimizers, and because there may be unintended consequences from any given specification, emergent behavior can diverge dramatically from ideal or design intentions.

AI alignment researchers aim to ensure that the behavior matches the ideal specification, using the design specification as a midpoint. A mismatch between the ideal specification and the design specification is known as ''outer misalignment'', because the mismatch lies between (''1'') the user's "true desires", which sit outside the computer system and (''2'') the computer system's programmed objective function (inside the computer system). A certain type of mismatch between the design specification and the emergent behavior is known as ''inner misalignment''; such a mismatch is internal to the AI, being a mismatch between (''2'') the AI's explicit objective function and (''3'') the AI's actual emergent goals.<ref name="inner_opt">{{Cite arXiv |eprint=1906.01820 |class=cs.AI |first1=Evan |last1=Hubinger |first2=Chris |last2=van Merwijk |title=Risks from Learned Optimization in Advanced Machine Learning Systems |date=11 June 2019 |last3=Mikulik |first3=Vladimir |last4=Skalse |first4=Joar |last5=Garrabrant |first5=Scott}}</ref><ref name="OpenAI_open_ended">{{Cite journal |last1=Ecoffet |first1=Adrien |last2=Clune |first2=Jeff |last3=Lehman |first3=Joel |date=1 July 2020 |title=Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity |url=https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00323 |journal=Artificial Life Conference Proceedings |volume=32 |pages=27–35 |arxiv=2006.07495 |doi=10.1162/isal_a_00323 |s2cid=219687488}}</ref><ref name="alignment_prob">{{Cite book |last=Christian |first=Brian |url=https://www.google.co.uk/books/edition/The_Alignment_Problem/VmJIzQEACAAJ?hl=en |title=The Alignment Problem: Machine Learning and Human Values |date=2020 |publisher=W.W. Norton |isbn=978-0-393-63582-9 |language=en |access-date=2021-02-07 |archive-url=https://web.archive.org/web/20210215081442/https://www.google.co.uk/books/edition/The_Alignment_Problem/VmJIzQEACAAJ?hl=en |archive-date=2021-02-15 |url-status=live}}</ref> Outer misalignment might arise because of mistakes in specifying the objective function (design specification).<ref name="DM_specification_gaming">{{Cite web |last1=Krakovna |first1=Victoria |last2=Legg |first2=Shane |title=Specification gaming: the flip side of AI ingenuity |url=https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity |url-status=live |archive-url=https://web.archive.org/web/20210126173242/https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity |archive-date=26 January 2021 |access-date=6 January 2021 |website=Deepmind}}</ref>
For example, a reinforcement learning agent trained on the game of ''CoastRunners'' learned to move in circles while repeatedly crashing, which got it a higher score than finishing the race.<ref name="Openai_boat">{{Cite web |last1=Clark |first1=Jack |last2=Amodei |first2=Dario |date=22 December 2016 |title=Faulty Reward Functions in the Wild |url=https://openai.com/blog/faulty-reward-functions/ |url-status=live |archive-url=https://web.archive.org/web/20210126173249/https://openai.com/blog/faulty-reward-functions/ |archive-date=26 January 2021 |access-date=6 January 2021 |website=OpenAI |language=en}}</ref> By contrast, inner misalignment arises when the agent pursues a goal that is aligned with the design specification on the training data but not elsewhere.<ref name="inner_opt" /><ref name="OpenAI_open_ended" /><ref name="alignment_prob" />
This type of misalignment is often compared to human evolution: evolution selected for genetic fitness (design specification) in our ancestral environment, but in the modern environment human goals (revealed specification) are not aligned with maximizing genetic fitness. For example, our taste for sugary food, which originally increased fitness, today leads to overeating and health problems. Inner misalignment is a particular concern for agents which are trained in large open-ended environments, where a wide range of unintended goals may emerge.<ref name="OpenAI_open_ended" />

An inner alignment failure occurs when the goals an AI pursues during deployment deviate from the goals it was trained to pursue in its original environment (its design specification). Paul Christiano argues for using ''[[interpretable artificial intelligence|interpretability]]'' to detect such deviations, using ''adversarial training'' to detect and penalize them, and using formal ''verification'' to rule them out.<ref name="Christiano_interview">{{Cite web |last=Christiano |first=Paul |date=11 September 2019 |title=Conversation with Paul Christiano |url=https://aiimpacts.org/conversation-with-paul-christiano/ |url-status=live |archive-url=https://web.archive.org/web/20200819135014/https://aiimpacts.org/conversation-with-paul-christiano/ |archive-date=19 August 2020 |access-date=6 January 2021 |website=AI Impacts |publisher=AI Impacts}}</ref>
These research areas are active focuses of work in the machine learning community, although that work is not normally aimed towards solving AGI alignment problems. A wide body of literature now exists on techniques for generating adversarial examples, and for creating models robust to them.<ref>{{Cite journal |last1=Serban |first1=Alex |last2=Poll |first2=Erik |last3=Visser |first3=Joost |date=12 June 2020 |title=Adversarial Examples on Object Recognition: A Comprehensive Survey |url=https://dl.acm.org/doi/abs/10.1145/3398394 |url-status=live |journal=ACM Computing Surveys |volume=53 |issue=3 |pages=66:1–66:38 |arxiv=2008.04094 |doi=10.1145/3398394 |issn=0360-0300 |archive-url=https://web.archive.org/web/20200629174006/https://dl.acm.org/doi/abs/10.1145/3398394 |archive-date=29 June 2020 |access-date=7 February 2021 |s2cid=218518141}}</ref>
Meanwhile research on verification includes techniques for training neural networks whose outputs provably remain within identified constraints.<ref name="DM_verification">{{Cite web |last1=Kohli |first1=Pushmeet |last2=Dvijohtham |first2=Krishnamurthy |last3=Uesato |first3=Jonathan |last4=Gowal |first4=Sven |title=Towards Robust and Verified AI: Specification Testing, Robust Training, and Formal Verification |url=https://deepmind.com/blog/article/robust-and-verified-ai |url-status=live |archive-url=https://web.archive.org/web/20201130165150/https://deepmind.com/blog/article/robust-and-verified-ai |archive-date=30 November 2020 |access-date=6 January 2021 |website=Deepmind}}</ref>

=== Scalable oversight ===
One approach to achieving outer alignment is to ask humans to evaluate and score the AI's behavior.<ref name="Christiano_et_al_2017" /><ref name="concrete_problems">{{Cite arXiv |eprint=1606.06565 |class=cs.AI |first1=Dario |last1=Amodei |first2=Chris |last2=Olah |title=Concrete Problems in AI Safety |date=25 July 2016 |last3=Steinhardt |first3=Jacob |last4=Christiano |first4=Paul |last5=Schulman |first5=John |last6=Mané |first6=Dan}}</ref>
However, humans are also fallible, and might score some undesirable solutions highly—for instance, a virtual robot hand learns to 'pretend' to grasp an object to get positive feedback.<ref name="Openai_robot_hand">{{Cite web |last1=Amodei |first1=Dario |last2=Christiano |first2=Paul |last3=Ray |first3=Alex |date=13 June 2017 |title=Learning from Human Preferences |url=https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/ |url-status=live |archive-url=https://web.archive.org/web/20210103215933/https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/ |archive-date=3 January 2021 |access-date=6 January 2021 |website=OpenAI |language=en}}</ref>
And thorough human supervision is expensive, meaning that this method could not realistically be used to evaluate all actions. Additionally, complex tasks (such as making economic policy decisions) might produce too much information for an individual human to evaluate. And long-term tasks such as predicting the climate cannot be evaluated without extensive human research.<ref name="iterated_amplification">{{Cite arXiv |eprint=1810.08575 |class=cs.LG |first1=Paul |last1=Christiano |first2=Buck |last2=Shlegeris |title=Supervising strong learners by amplifying weak experts |date=19 October 2018 |last3=Amodei |first3=Dario}}</ref>

A key open problem in alignment research is how to create a design specification which avoids (outer) misalignment, given only limited access to a human supervisor—known as the problem of scalable oversight.<ref name="concrete_problems" />

==== Training by debate ====
[[OpenAI]] researchers have proposed training aligned AI by means of debate between AI systems, with the winner judged by humans.<ref name="DebatePaper">{{Cite arXiv |eprint=1805.00899 |class=stat.ML |first1=Geoffrey |last1=Irving |first2=Paul |last2=Christiano |title=AI safety via debate |date=October 22, 2018 |last3=Amodei |first3=Dario |author4=OpenAI |author4-link=OpenAI}}</ref> Such debate is intended to bring the weakest points of an answer to a complex question or problem to human attention, as well as to train AI systems to be more beneficial to humans by rewarding AI for truthful and safe answers. This approach is motivated by the expected difficulty of determining whether an AGI-generated answer is both valid and safe by human inspection alone. Joel Lehman characterizes debate as one of "the long term safety agendas currently popular in ML", with the other two being reward modeling<ref name="reward_modeling" /> and iterated amplification.<ref name="book_gen_prog">{{Cite book |last1=Banzhaf |first1=Wolfgang |url=https://www.google.co.uk/books/edition/Genetic_Programming_Theory_and_Practice/2U7iDwAAQBAJ?hl=en&gbpv=1&pg=PA181&printsec=frontcover#v=onepage&q&f=false |title=Genetic Programming Theory and Practice XVII |last2=Goodman |first2=Erik |last3=Sheneman |first3=Leigh |last4=Trujillo |first4=Leonardo |last5=Worzel |first5=Bill |date=May 2020 |publisher=Springer Nature |isbn=978-3-030-39958-0 |language=en |access-date=2021-02-07 |archive-url=https://web.archive.org/web/20210215081553/https://www.google.co.uk/books/edition/Genetic_Programming_Theory_and_Practice/2U7iDwAAQBAJ?hl=en&gbpv=1&pg=PA181&printsec=frontcover#v=onepage&q&f=false |archive-date=2021-02-15 |url-status=live}}</ref><ref name="iterated_amplification" />

==== Reward modeling and iterated amplification ====
Reward modeling refers to a system of [[reinforcement learning]] in which an agent receives rewards from a model trained to imitate human feedback.<ref name="reward_modeling" /> In reward modeling, instead of receiving reward signals directly from humans or from a static reward function, an agent receives its reward signals through a human-trained model that can operate independently of humans. The reward model is concurrently trained by human feedback on the agent's behavior during the same period in which the agent is being trained by the reward model.

In 2017, researchers from [[OpenAI]] and [[DeepMind]] reported that a reinforcement learning algorithm using a feedback-predicting reward model was able to learn complex novel behaviors in a virtual environment.<ref name="Christiano_et_al_2017">{{Cite arXiv |eprint=1706.03741 |class=stat.ML |first1=Paul |last1=Christiano |first2=Jan |last2=Leike |title=Deep Reinforcement Learning from Human Preferences |date=13 July 2017 |last3=Brown |first3=Tom |last4=Martic |first4=Miljan |last5=Legg |first5=Shane |last6=Amodei |first6=Dario}}</ref> In one experiment, a virtual robot was trained to perform a backflip in less than an hour of evaluation using 900 bits of human feedback.
In 2020, researchers from [[OpenAI]] described using reward modeling to train language models to produce short summaries of Reddit posts and news articles, with high performance relative to other approaches.<ref name="OpenAI_2020">{{Cite web |last1=Stiennon |first1=Nisan |last2=Ziegler |first2=Daniel |last3=Lowe |first3=Ryan |last4=Wu |first4=Jeffrey |last5=Voss |first5=Chelsea |last6=Christiano |first6=Paul |last7=Ouyang |first7=Long |date=September 4, 2020 |title=Learning to Summarize with Human Feedback |url=https://openai.com/blog/learning-to-summarize-with-human-feedback/ |url-status=live |archive-url=https://web.archive.org/web/20200907225614/https://openai.com/blog/learning-to-summarize-with-human-feedback/ |archive-date=September 7, 2020 |access-date=September 7, 2020}}</ref> However, they observed that beyond the predicted reward associated with the 99th percentile of reference summaries in the training dataset, optimizing for the reward model produced worse summaries rather than better.

A long-term goal of this line of research is to create a ''recursive'' reward modeling setup for training agents on tasks too complex or costly for humans to evaluate directly.<ref name="reward_modeling" /> For example, if we wanted to train an agent to write a fantasy novel using reward modeling, we would need humans to read and holistically assess enough novels to train a reward model to match those assessments, which might be prohibitively expensive. But this would be easier if we had access to assistant agents which could extract a summary of the plotline, check spelling and grammar, summarize character development, assess the flow of the prose, and so on. Each of those assistants could in turn be trained via reward modeling.

The general term for a human working with AIs to perform tasks that the human could not by themselves is an amplification step, because it amplifies the capabilities of a human beyond what they would normally be capable of. Since recursive reward modeling involves a hierarchy of several of these steps, it is one example of a broader class of safety techniques known as ''iterated amplification''.<ref name="iterated_amplification" />
In addition to techniques which make use of reinforcement learning, other proposed iterated amplification techniques rely on supervised learning, or imitation learning, to scale up human abilities.

=== Inferring human preferences from behavior ===
[[Stuart J. Russell|Stuart Russell]] has advocated a new approach to the development of beneficial machines, in which:<ref name="HC" />{{rp|182}}

{{quote|1. The machine's only objective is to maximize the realization of human preferences.

2. The machine is initially uncertain about what those preferences are.

3. The ultimate source of information about human preferences is human behavior.}}

An early example of this approach is Russell and Ng's [[Reinforcement learning#Inverse reinforcement learning|inverse reinforcement learning]], in which AIs infer the preferences of human supervisors from those supervisors' behavior, by assuming that the supervisors act to maximize some reward function. More recently, Hadfield-Menell et al. have extended this paradigm to allow humans to modify their behavior in response to the AIs' presence, for example, by favoring pedagogically useful actions, which they call "assistance games", also known as cooperative inverse reinforcement learning.<ref name="HC" />{{rp|202}} <ref name="CIRL">{{Cite journal |last1=Hadfield-Menell |first1=Dylan |last2=Dragan |first2=Anca |last3=Abbeel |first3=Pieter |author-link3=Pieter Abbeel |last4=Russell |first4=Stuart |author-link4=Stuart J. Russell |date=12 November 2016 |title=Cooperative Inverse Reinforcement Learning |journal=Neural Information Processing Systems}}</ref> Compared with debate and iterated amplification, assistance games rely more explicitly on specific assumptions about human rationality; it is unclear how to extend them to cases in which humans are systematically biased or otherwise suboptimal.

=== Embedded agency ===
Work on scalable oversight largely occurs within formalisms such as [[Partially observable Markov decision process|POMDPs]]. Existing formalisms assume that the agent's algorithm is executed outside the environment (i.e. not physically embedded in it). Embedded agency<ref name="lit_review">{{Cite journal |last1=Everitt |first1=Tom |last2=Lea |first2=Gary |last3=Hutter |first3=Marcus |date=21 May 2018 |title=AGI Safety Literature Review |url-status=live |journal=1805.01109 |arxiv=1805.01109}}</ref><ref>{{Cite arXiv |eprint=1902.09469 |class=cs.AI |first1=Abram |last1=Demski |first2=Scott |last2=Garrabrant |title=Embedded Agency |date=6 October 2020}}</ref>
is another major strand of research, which attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build. For example, even if the scalable oversight problem is solved, an agent which is able to gain access to the computer it is running on may still have an incentive to tamper<ref name="causal_influence">{{Cite arXiv |eprint=1902.09980 |class=cs.AI |first1=Tom |last1=Everitt |first2=Pedro A. |last2=Ortega |title=Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings |date=6 September 2019 |last3=Barnes |first3=Elizabeth |last4=Legg |first4=Shane}}</ref>
with its reward function in order to get much more reward than its human supervisors give it. A list of examples of specification gaming from [[DeepMind]] researcher Viktoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing.<ref name="DM_specification_gaming" />
This class of problems has been formalised using causal incentive diagrams.<ref name="causal_influence" /> Everitt and Hutter's current reward function algorithm<ref name="causal_influence_2">{{Cite arXiv |eprint=1908.04734 |class=cs.AI |first1=Tom |last1=Everitt |first2=Marcus |last2=Hutter |title=Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective |date=20 August 2019}}</ref>
addresses it by designing agents which evaluate future actions according to their current reward function. This approach is also intended to prevent problems from more general self-modification which AIs might carry out.<ref>{{Cite arXiv |eprint=1605.03142 |class=cs.AI |first1=Tom |last1=Everitt |first2=Daniel |last2=Filan |title=Self-Modification of Policy and Utility Function in Rational Agents |date=10 May 2016 |last3=Daswani |first3=Mayank |last4=Hutter |first4=Marcus}}</ref><ref name="lit_review" />

Other work in this area focuses on developing new frameworks and algorithms for other properties we might want to capture in our design specification.<ref name="lit_review" /> For example, we would like our agents to reason correctly under uncertainty in a wide range of circumstances. As one contribution to this, Leike et al. provide a general way for Bayesian agents to model each other's policies in a multi-agent environment, without ruling out any realistic possibilities.<ref>{{Cite journal |last1=Leike |first1=Jan |last2=Taylor |first2=Jessica |last3=Fallenstein |first3=Benya |date=25 June 2016 |title=A formal solution to the grain of truth problem |url=https://dl.acm.org/doi/10.5555/3020948.3020993 |url-status=live |journal=Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence |series=UAI'16 |publisher=AUAI Press |pages=427–436 |arxiv=1609.05058 |isbn=9780996643115 |archive-url=https://web.archive.org/web/20210215081616/https://dl.acm.org/doi/10.5555/3020948.3020993 |archive-date=15 February 2021 |access-date=7 February 2021}}</ref>
And the Garrabrant induction algorithm extends probabilistic induction to be applicable to logical, rather than only empirical, facts.<ref name="logical_induction">{{Cite arXiv |eprint=1609.03543 |class=cs.AI |first1=Scott |last1=Garrabrant |first2=Tsvi |last2=Benson-Tilsen |title=Logical Induction |date=7 December 2020 |last3=Critch |first3=Andrew |last4=Soares |first4=Nate |last5=Taylor |first5=Jessica}}</ref>

== Capability control ==
Capability control proposals aim to increase our ability to monitor and control the behavior of AI systems, in order to reduce the danger they might pose if misaligned. However, capability control becomes less effective as our agents become more intelligent and their ability to exploit flaws in our control systems increases. Therefore, Bostrom and others recommend capability control methods only as a supplement to alignment methods.<ref name="superintelligence" /><!-- Chapter 9: The control problem -->

One challenge is that neural networks are by default highly uninterpretable.<ref name="interpretability_survey">{{Cite journal |last1=Montavon |first1=Grégoire |last2=Samek |first2=Wojciech |last3=Müller |first3=Klaus Robert |author-link3=Klaus-Robert Müller |date=2018 |title=Methods for interpreting and understanding deep neural networks |journal=Digital Signal Processing: A Review Journal |language=English |volume=73 |pages=1–15 |doi=10.1016/j.dsp.2017.10.011 |issn=1051-2004 |doi-access=free |s2cid=207170725}}</ref> This makes it more difficult to detect deception or other undesired behavior. Advances in [[interpretable artificial intelligence]] could be useful to mitigate this difficulty.<ref>Yampolskiy, Roman V. "Unexplainability and Incomprehensibility of AI." Journal of Artificial Intelligence and Consciousness 7.02 (2020): 277-291.</ref>

=== Interruptibility and off-switch ===
One potential way to prevent harmful outcomes is to give human supervisors the ability to easily shut down a misbehaving AI via an "off-switch". However, in order to achieve their assigned objective, such AIs will have an incentive to disable any off-switches, or to run copies of themselves on other computers. This problem has been formalised as an assistance game between a human and an AI, in which the AI can choose whether to disable its off-switch; and then, if the switch is still enabled, the human can choose whether to press it or not.<ref>{{Cite arXiv |eprint=1611.08219 |class=cs.AI |first1=Dylan |last1=Hadfield-Menell |first2=Anca |last2=Dragan |title=The Off-Switch Game |date=15 June 2017 |last3=Abbeel |first3=Pieter |last4=Russell |first4=Stuart}}</ref>
A standard approach to such assistance games is to ensure that the AI interprets human choices as important information about its intended goals.<ref name="HC" />{{rp|208}}

Alternatively, Laurent Orseau and Stuart Armstrong proved that a broad class of agents, called safely interruptible agents, can learn to become indifferent to whether their off-switch gets pressed.<ref name=bbc-google /><ref name="interruptible_agents">{{Cite journal |last1=Orseau |first1=Laurent |last2=Armstrong |first2=Stuart |date=25 June 2016 |title=Safely interruptible agents |url=https://dl.acm.org/doi/10.5555/3020948.3021006 |url-status=live |journal=Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence |series=UAI'16 |publisher=AUAI Press |pages=557–566 |isbn=9780996643115 |archive-url=https://web.archive.org/web/20210215081449/https://dl.acm.org/doi/10.5555/3020948.3021006 |archive-date=15 February 2021 |access-date=7 February 2021}}</ref> This approach has the limitation that an AI which is completely indifferent to whether it is shut down or not is also unmotivated to care about whether the off-switch remains functional, and could incidentally and innocently disable it in the course of its operations (for example, for the purpose of removing and recycling an unnecessary component). More broadly, indifferent agents will act as if the off-switch can never be pressed, and might therefore fail to make contingency plans to arrange a graceful shutdown.<ref name="interruptible_agents" /><ref name="corrigibility">Soares, Nate, et al. "Corrigibility." Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015.</ref>

=== Boxing ===
{{Main|AI box}}

An AI box is a proposed method of capability control in which an AI is run on an isolated computer system with heavily restricted input and output channels—for example, text-only channels and no connection to the internet. While this reduces the AI's ability to carry out undesirable behavior, it also reduces its usefulness. However, boxing has fewer costs when applied to a question-answering system, which does not require interaction with the world in any case.

The likelihood of security flaws involving hardware or software vulnerabilities can be reduced by formally verifying the design of the AI box. Security breaches may also occur if the AI is able to manipulate the human supervisors into letting it out, via its understanding of their psychology.<ref>{{Cite journal |last=Chalmers |first=David |author-link=David Chalmers |year=2010 |title=The singularity: A philosophical analysis |journal=Journal of Consciousness Studies |volume=17 |issue=9–10 |pages=7–65}}</ref>

===Oracle===
An oracle is a hypothetical AI designed to answer questions and prevented from gaining any goals or subgoals that involve modifying the world beyond its limited environment.<ref name="bostrom chapter 10 page 145">{{Cite book |last=Bostrom |first=Nick |title=Superintelligence: Paths, Dangers, Strategies |date=2014 |publisher=Oxford University Press |isbn=9780199678112 |location=Oxford |chapter=Chapter 10: Oracles, genies, sovereigns, tools (page 145) |quote=An oracle is a question-answering system. It might accept questions in a natural language and present its answers as text. An oracle that accepts only yes/no questions could output its best guess with a single bit, or perhaps with a few extra bits to represent its degree of confidence. An oracle that accepts open-ended questions would need some metric with which to rank possible truthful answers in terms of their informativeness or appropriateness. In either case, building an oracle that has a fully domain-general ability to answer natural language questions is an AI-complete problem. If one could do that, one could probably also build an AI that has a decent ability to understand human intentions as well as human words.}}<!--|access-date=29 March 2018--></ref><ref>{{Cite journal |last1=Armstrong |first1=Stuart |last2=Sandberg |first2=Anders |last3=Bostrom |first3=Nick |year=2012 |title=Thinking Inside the Box: Controlling and Using an Oracle AI |journal=Minds and Machines |volume=22 |issue=4 |pages=299–324 |doi=10.1007/s11023-012-9282-2 |s2cid=9464769}}</ref> A successfully controlled oracle would have considerably less immediate benefit than a successfully controlled general-purpose superintelligence, though an oracle could still create trillions of dollars worth of value.<ref name=HC/>{{rp|163}} In his book ''[[Human Compatible]]'', AI researcher [[Stuart J. Russell]] states that an oracle would be his response to a scenario in which superintelligence is known to be only a decade away.<ref name="HC" />{{rp|162–163}}  His reasoning is that an oracle, being simpler than a general-purpose superintelligence, would have a higher chance of being successfully controlled under such constraints.

Because of its limited impact on the world, it may be wise to build an oracle as a precursor to a superintelligent AI. The oracle could tell humans how to successfully build a strong AI, and perhaps provide answers to difficult moral and philosophical problems requisite to the success of the project.  However, oracles may share many of the goal definition issues associated with general-purpose superintelligence.  An oracle would have an incentive to escape its controlled environment so that it can acquire more computational resources and potentially control what questions it is asked.<ref name="HC" />{{rp|162}}  Oracles may not be truthful, possibly lying to promote hidden agendas. To mitigate this, Bostrom suggests building multiple oracles, all slightly different, and comparing their answers to reach a consensus.<ref name="bostrom chapter 10 page 147">{{Cite book |last=Bostrom |first=Nick |title=Superintelligence: Paths, Dangers, Strategies |date=2014 |publisher=Oxford University Press |isbn=9780199678112 |location=Oxford |chapter=Chapter 10: Oracles, genies, sovereigns, tools (page 147) |quote=For example, consider the risk that an oracle will answer questions not in a maximally truthful way but in such a way as to subtly manipulate us into promoting its own hidden agenda. One way to slightly mitigate this threat could be to create multiple oracles, each with a slightly different code and a slightly different information base. A simple mechanism could then compare the answers given by the different oracles and only present them for human viewing if all the answers agree.}}<!--|access-date=29 March 2018--></ref>

== Skepticism of AI risk ==
In contrast to endorsers of the thesis that rigorous control efforts are needed because [[superintelligence]] poses an [[Existential risk from artificial general intelligence|existential risk]], AI risk skeptics believe that superintelligence poses little or no risk of accidental misbehavior. Such skeptics often believe that controlling a superintelligent AI will be trivial. Some skeptics,<ref>{{Cite news |date=27 September 2015 |title=Intelligent Machines: Do we really need to fear AI? |work=BBC News |url=https://www.bbc.com/news/technology-32334568 |url-status=live |access-date=9 February 2021 |archive-url=https://web.archive.org/web/20201108124948/https://www.bbc.com/news/technology-32334568 |archive-date=8 November 2020}}</ref> such as [[Gary Marcus]],<ref>{{Cite news |last1=Marcus |first1=Gary |last2=Davis |first2=Ernest |date=6 September 2019 |title=Opinion {{!}} How to Build Artificial Intelligence We Can Trust (Published 2019) |work=The New York Times |url=https://www.nytimes.com/2019/09/06/opinion/ai-explainability.html |url-status=live |access-date=9 February 2021 |archive-url=https://web.archive.org/web/20200922145040/https://www.nytimes.com/2019/09/06/opinion/ai-explainability.html |archive-date=22 September 2020}}</ref> propose adopting rules similar to the fictional [[Three Laws of Robotics]] which directly specify a desired outcome ("direct normativity"). By contrast, most endorsers of the existential risk thesis (as well as many skeptics) consider the Three Laws to be unhelpful, due to those three laws being ambiguous and self-contradictory. (Other "direct normativity" proposals include Kantian ethics, utilitarianism, or a mix of some small list of enumerated desiderata.) Most endorsers believe instead that human values (and their quantitative trade-offs) are too complex and poorly-understood to be directly programmed into a superintelligence; instead, a superintelligence would need to be programmed with a ''process'' for acquiring and fully understanding human values ("indirect normativity"), such as [[coherent extrapolated volition]].<ref name="AGIResponses">{{Cite journal |last1=Sotala |first1=Kaj |last2=Yampolskiy |first2=Roman |author-link2=Roman Yampolskiy |date=19 December 2014 |title=Responses to catastrophic AGI risk: a survey |journal=[[Physica Scripta]] |volume=90 |issue=1 |pages=018001 |bibcode=2015PhyS...90a8001S |doi=10.1088/0031-8949/90/1/018001 |doi-access=free}}</ref>

== Public policy ==

In 2021, the [[Secretary-General of the United Nations]] advised to regulate AI to ensure it is "aligned with shared global values."<ref>[https://www.un.org/en/content/common-agenda-report/ Secretary-General’s report on “Our Common Agenda”], 2021. Page 63: ''"[T]he Compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values"''</ref>

In 2021, the [[People's Republic of China|PRC]] published ethical guidelines for the use of AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety.<ref>PRC Ministry of Science and Technology. Ethical Norms for New Generation Artificial Intelligence Released, 2021. A [https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/ translation] by [[Center for Security and Emerging Technology]]</ref>

In 2021, the [[UK]] published its 10-year National AI Strategy.<ref>{{Cite web|url=https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/|title = UK publishes National Artificial Intelligence Strategy}}</ref> According to the Strategy, the British government takes seriously "the long term risk of non-aligned Artificial General Intelligence".<ref>"''The government takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for the UK and the world, seriously.''" ([https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version The National AI Strategy of the UK], 2021)</ref> The strategy describes actions to assess long term AI risks, including catastrophic risks.<ref>[https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version The National AI Strategy of the UK], 2021 (actions 9 and 10 of the section "Pillar 3 - Governing AI Effectively")</ref>

== See also ==
* [[AI takeover]]
* [[Artificial wisdom]]
* [[HAL 9000]]
* [[Multivac]]
* [[Regulation of algorithms]]
* [[Regulation of artificial intelligence]]
* [[Toronto Declaration]]

== References ==
{{Reflist}}

{{Existential risk from artificial intelligence|state=expanded}}

[[Category:Existential risk from artificial general intelligence]]
[[Category:Singularitarianism]]
[[Category:Philosophy of artificial intelligence]]