{{Short description|Issue of ensuring beneficial AI}}
{{use mdy dates|date=September 2021}}
{{Use American English|date=February 2021}}
{{Artificial intelligence}}
In the field of [[artificial intelligence]] (AI), '''AI alignment''' research aims to steer AI systems towards their designers’ intended goals and interests.{{efn|Other definitions of AI alignment require that the AI system advances more general goals such as human values, other ethical principles, or the intentions its designers would have if they were more informed and enlightened.<ref name="Gabriel2020">{{Cite journal |last=Gabriel |first=Iason |date=2020-09-01 |title=Artificial Intelligence, Values, and Alignment |url=https://doi.org/10.1007/s11023-020-09539-2 |journal=Minds and Machines |volume=30 |issue=3 |pages=411–437 |doi=10.1007/s11023-020-09539-2 |s2cid=210920551 |issn=1572-8641 |accessdate=2022-07-23}}</ref> }} An ''aligned'' AI system advances the intended objective; a ''misaligned'' AI system is competent at advancing some objective, but not the intended one.{{efn|See the textbook: Russel & Norvig, [[Artificial Intelligence: A Modern Approach]].<ref name=":92">{{Cite book |last1=Russell |first1=Stuart J. |url=https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html |title=Artificial intelligence: A modern approach |last2=Norvig |first2=Peter |publisher=Pearson |year=2020 |isbn=978-1-292-40113-3 |edition=4th |pages=31–34 |oclc=1303900751 |ref=Section 1.5}}</ref>
The distinction between misaligned AI and incompetent AI has been formalized in certain contexts.<ref name="goal_misgen"/>}}

AI systems can be challenging to align and misaligned systems can malfunction or cause harm. It can be difficult for AI designers to specify the full range of desired and undesired behaviors. Therefore, they use easy-to-specify [[Misaligned goals in artificial intelligence#Undesired%20side-effects|proxy goals]] that omit some desired constraints. However, AI systems exploit the resulting loopholes. As a result, they accomplish their proxy goals efficiently but in unintended, sometimes harmful ways ([[Misaligned goals in artificial intelligence#Specification%20gaming|reward hacking]]).<ref name=":92">{{Cite book |last1=Russell |first1=Stuart J. |url=https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html |title=Artificial intelligence: A modern approach |last2=Norvig |first2=Peter |publisher=Pearson |year=2020 |isbn=978-1-292-40113-3 |edition=4th |pages=31–34 |oclc=1303900751 |ref=Section 1.5}}</ref><ref name=":210">{{Cite book |last=Russell |first=Stuart J. |url=https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/ |title=Human compatible: Artificial intelligence and the problem of control |publisher=Penguin Random House |year=2020 |isbn=9780525558637 |location= |oclc=1113410915}}</ref><ref name=":010">{{cite arXiv |eprint=2109.13916 |class=cs.LG |first1=Dan |last1=Hendrycks |first2=Nicholas |last2=Carlini |title=Unsolved Problems in ML Safety |date=2022-06-16 |last3=Schulman |first3=John |last4=Steinhardt |first4=Jacob}}</ref><ref name=":1522">{{Cite conference |last1=Pan |first1=Alexander |last2=Bhatia |first2=Kush |last3=Steinhardt |first3=Jacob |date=2022-02-14 |title=The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models |url=https://openreview.net/forum?id=JYtwGwIL7ye |conference=International Conference on Learning Representations |accessdate=2022-07-21}}</ref> AI systems can also develop unwanted [[Instrumental convergence|instrumental behaviors]] such as seeking power, as this helps them achieve their given goals.<ref name=":92" /><ref name=":75">{{cite arXiv |eprint=2206.13353 |class=cs.CY |first=Joseph |last=Carlsmith |title=Is Power-Seeking AI an Existential Risk? |date=2022-06-16}}</ref><ref name=":010" /><ref name=":210" /> Furthermore, they can develop emergent goals that may be hard to detect before the system is deployed, facing new situations and data distributions.<ref name=":010" /><ref name="goal_misgen" /> These problems affect existing commercial systems such as robots,<ref>{{Cite journal |last1=Kober |first1=Jens |last2=Bagnell |first2=J. Andrew |last3=Peters |first3=Jan |date=2013-09-01 |title=Reinforcement learning in robotics: A survey |url=http://journals.sagepub.com/doi/10.1177/0278364913495721 |journal=The International Journal of Robotics Research |language=en |volume=32 |issue=11 |pages=1238–1274 |doi=10.1177/0278364913495721 |issn=0278-3649 |s2cid=1932843}}</ref> language models,<ref name=":625">{{Cite journal |last1=Bommasani |first1=Rishi |last2=Hudson |first2=Drew A. |last3=Adeli |first3=Ehsan |last4=Altman |first4=Russ |last5=Arora |first5=Simran |last6=von Arx |first6=Sydney |last7=Bernstein |first7=Michael S. |last8=Bohg |first8=Jeannette |last9=Bosselut |first9=Antoine |last10=Brunskill |first10=Emma |last11=Brynjolfsson |first11=Erik |date=2022-07-12 |title=On the Opportunities and Risks of Foundation Models |url=https://fsi.stanford.edu/publication/opportunities-and-risks-foundation-models |journal=Stanford CRFM |arxiv=2108.07258}}</ref><ref name=":42">{{cite arXiv |eprint=2203.02155 |class=cs.CL |first1=Long |last1=Ouyang |first2=Jeff |last2=Wu |title=Training language models to follow instructions with human feedback |date=2022 |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll L. |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Ray |first10=Alex |last11=Schulman |first11=J. |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke E. |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=P. |last18=Christiano |first18=P. |last19=Leike |first19=J. |last20=Lowe |first20=Ryan J.}}</ref><ref name=":113">{{Cite web |last1=Zaremba |first1=Wojciech |last2=Brockman |first2=Greg |last3=OpenAI |date=2021-08-10 |title=OpenAI Codex |url=https://openai.com/blog/openai-codex/ |accessdate=2022-07-23 |work=OpenAI}}</ref> autonomous vehicles,<ref>{{Cite journal |last1=Knox |first1=W. Bradley |last2=Allievi |first2=Alessandro |last3=Banzhaf |first3=Holger |last4=Schmitt |first4=Felix |last5=Stone |first5=Peter |date=2022-03-11 |title=Reward (Mis)design for Autonomous Driving |url=https://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/knox2021reward.pdf |journal= |arxiv=2104.13906}}</ref> and social media recommendation engines.<ref name=":625" /><ref name=":210" /><ref>{{Cite journal |last=Stray |first=Jonathan |date=2020 |title=Aligning AI Optimization to Community Well-Being |journal=International Journal of Community Well-Being |language=en |volume=3 |issue=4 |pages=443–463 |doi=10.1007/s42413-020-00086-3 |issn=2524-5295 |pmid=34723107 |pmc=7610010 |s2cid=226254676}}</ref> However, more powerful future systems may be more severely affected since these problems partially result from high capability.<ref name=":1522" /><ref name=":010" /><ref name=":92" />

The AI research community and the United Nations have called for technical research and policy solutions to ensure that AI systems are aligned with human values.{{efn|The AI principles created at the [[Asilomar Conference on Beneficial AI]] were signed by 1797 AI/robotics researchers.<ref>{{Cite web| last = Future of Life Institute| title = Asilomar AI Principles| work = Future of Life Institute| accessdate = 2022-07-18| date = 2017-08-11| url = https://futureoflife.org/2017/08/11/ai-principles/}}</ref> Further, the UN Secretary-General’s report “Our Common Agenda“,<ref>{{Cite report| publisher = United Nations| last = United Nations| title = Our Common Agenda: Report of the Secretary-General| location = New York| date = 2021 |url=https://www.un.org/en/content/common-agenda-report/assets/pdf/Common_Agenda_Report_English.pdf}}</ref> notes: “[T]he Compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values" and discusses [[global catastrophic risks]] from technological developments.}}

AI alignment is a subfield of AI safety, the study of building safe AI systems.<ref name=":010"/><ref name=":110">{{Cite arXiv |last1=Amodei |first1=Dario |last2=Olah |first2=Chris |last3=Steinhardt |first3=Jacob |last4=Christiano |first4=Paul |last5=Schulman |first5=John |last6=Mané |first6=Dan |date=2016-06-21 |title=Concrete Problems in AI Safety |class=cs.AI |eprint=1606.06565 |language=en}}</ref> Other subfields of AI safety include robustness, monitoring, and [[AI capability control|capability control.]]<ref name=":010"/><ref name=":2323">{{Cite web |last1=Ortega |first1=Pedro A. |last2=Maini |first2=Vishal |last3=DeepMind safety team |date=2018-09-27 |title=Building safe artificial intelligence: specification, robustness, and assurance |url=https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1 |accessdate=2022-07-18 |work=DeepMind Safety Research - Medium}}</ref> Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, as well as preventing emergent AI behaviors like power-seeking.<ref name=":010"/><ref name=":2323"/> Alignment research has connections to [[Explainable artificial intelligence|interpretability research]],<ref name=":33">{{Cite web |last=Rorvig |first=Mordechai |date=2022-04-14 |title=Researchers Gain New Understanding From Simple AI |url=https://www.quantamagazine.org/researchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414/ |accessdate=2022-07-18 |work=Quanta Magazine}}</ref> [[Robust optimization|robustness]],<ref name=":010"/><ref name=":110" /> [[anomaly detection]], [[Uncertainty quantification|calibrated uncertainty]],<ref name=":33" /> [[formal verification]],<ref name=":6">{{Cite journal |last1=Russell |first1=Stuart |last2=Dewey |first2=Daniel |last3=Tegmark |first3=Max |date=2015-12-31 |title=Research Priorities for Robust and Beneficial Artificial Intelligence |url=https://ojs.aaai.org/index.php/aimagazine/article/view/2577 |journal=AI Magazine |volume=36 |issue=4 |pages=105–114 |doi=10.1609/aimag.v36i4.2577 |s2cid=8174496 |issn=2371-9621}}</ref> preference learning,<ref name=":122">{{Cite journal |last1=Wirth |first1=Christian |last2=Akrour |first2=Riad |last3=Neumann |first3=Gerhard |last4=Fürnkranz |first4=Johannes |date=2017 |title=A survey of preference-based reinforcement learning methods |journal=Journal of Machine Learning Research |volume=18 |issue=136 |pages=1–46}}</ref><ref name=":162">{{Cite conference |last1=Christiano |first1=Paul F. |last2=Leike |first2=Jan |last3=Brown |first3=Tom B. |last4=Martic |first4=Miljan |last5=Legg |first5=Shane |last6=Amodei |first6=Dario |date=2017 |title=Deep reinforcement learning from human preferences |series=NIPS'17 |location=Red Hook, NY, USA |publisher=Curran Associates Inc. |pages=4302–4310 |isbn=978-1-5108-6096-4 |book-title=Proceedings of the 31st International Conference on Neural Information Processing Systems}}</ref><ref name=":53">{{Cite web |last=Heaven |first=Will Douglas |date=2022-01-27 |title=The new version of GPT-3 is much better behaved (and should be less toxic) |url=https://www.technologyreview.com/2022/01/27/1044398/new-gpt3-openai-chatbot-language-model-ai-toxic-misinformation/ |accessdate=2022-07-18 |work=MIT Technology Review}}</ref> [[Safety-critical system|safety-critical engineering]],<ref name=":010"/><ref>{{cite arXiv |last1=Mohseni |first1=Sina |last2=Wang |first2=Haotao |last3=Yu |first3=Zhiding |last4=Xiao |first4=Chaowei |last5=Wang |first5=Zhangyang |last6=Yadawa |first6=Jay |date=2022-03-07 |title=Taxonomy of Machine Learning Safety: A Survey and Primer |class=cs.LG |eprint=2106.04823 }}</ref> [[game theory]],<ref>{{Cite web |last=Clifton |first=Jesse |date=2020 |title=Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda |url=https://longtermrisk.org/research-agenda/ |accessdate=2022-07-18 |work=Center on Long-Term Risk}}</ref><ref>{{Cite journal |last1=Dafoe |first1=Allan |last2=Bachrach |first2=Yoram |last3=Hadfield |first3=Gillian |last4=Horvitz |first4=Eric |last5=Larson |first5=Kate |last6=Graepel |first6=Thore |date=2021-05-06 |title=Cooperative AI: machines must learn to find common ground |url=http://www.nature.com/articles/d41586-021-01170-0 |journal=Nature |language=en |volume=593 |issue=7857 |pages=33–36 |doi=10.1038/d41586-021-01170-0 |pmid=33947992 |bibcode=2021Natur.593...33D |s2cid=233740521 |issn=0028-0836}}</ref> [[Fairness (machine learning)|algorithmic fairness]],<ref name=":110" /><ref>{{Cite journal |last1=Prunkl |first1=Carina |last2=Whittlestone |first2=Jess |date=2020-02-07 |title=Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society |url=https://dl.acm.org/doi/10.1145/3375627.3375803 |journal=Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society |language=en |location=New York NY USA |publisher=ACM |pages=138–143 |doi=10.1145/3375627.3375803 |isbn=978-1-4503-7110-0|s2cid=210164673 }}</ref> and the [[social science]]s,<ref>{{Cite journal |last1=Irving |first1=Geoffrey |last2=Askell |first2=Amanda |date=2019-02-19 |title=AI Safety Needs Social Scientists |url=https://distill.pub/2019/safety-needs-social-scientists |journal=Distill |volume=4 |issue=2 |pages=10.23915/distill.00014 |doi=10.23915/distill.00014 |s2cid=159180422 |issn=2476-0757}}</ref> among others.

==The alignment problem==
[[File:Misaligned_boat_racing_AI_crashes_to_collect_points_instead_of_finishing_the_race.ogg|right|thumb|An AI system that was intended to complete a boat race instead learned that it could collect more points by indefinitely looping and crashing into targets—an example of specification gaming.<ref name=":2">{{Cite web |date=2016-12-22 |title=Faulty Reward Functions in the Wild |url=https://openai.com/blog/faulty-reward-functions/ |access-date=2022-09-10 |website=OpenAI |language=en}}</ref>]]
In 1960, AI pioneer [[Norbert Wiener]] articulated the AI alignment problem as follows: “If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively … we had better be quite sure that the purpose put into the machine is the purpose which we really desire.”<ref name=":1023">{{Cite journal |last=Wiener |first=Norbert |date=1960-05-06 |title=Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers. |url=https://www.science.org/doi/10.1126/science.131.3410.1355 |journal=Science |language=en |volume=131 |issue=3410 |pages=1355–1358 |doi=10.1126/science.131.3410.1355 |pmid=17841602 |issn=0036-8075}}</ref><ref name=":210"/> More recently, AI alignment has emerged as an open problem for modern AI systems<ref>{{Cite news |last=The Ezra Klein Show |date=2021-06-04 |title=If 'All Models Are Wrong,' Why Do We Give Them So Much Power? |work=The New York Times |url=https://www.nytimes.com/2021/06/04/opinion/ezra-klein-podcast-brian-christian.html |accessdate=2022-07-18 |issn=0362-4331}}</ref><ref>{{Cite web |last=Wolchover |first=Natalie |date=2015-04-21 |title=Concerns of an Artificial Intelligence Pioneer |url=https://www.quantamagazine.org/artificial-intelligence-aligned-with-human-values-qa-with-stuart-russell-20150421/ |accessdate=2022-07-18 |work=Quanta Magazine}}</ref><ref>{{Cite web |last=California Assembly |title=Bill Text - ACR-215 23 Asilomar AI Principles. |url=https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180ACR215 |accessdate=2022-07-18}}</ref><ref name=":1922">{{Cite news |last1=Johnson |first1=Steven |last2=Iziev |first2=Nikita |date=2022-04-15 |title=A.I. Is Mastering Language. Should We Trust What It Says? |work=The New York Times |url=https://www.nytimes.com/2022/04/15/magazine/ai-language.html |accessdate=2022-07-18 |issn=0362-4331}}</ref> and a research field within AI.<ref name=":322">{{Cite book |last1=Russell |first1=Stuart J. |url=https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html |title=Artificial intelligence: A modern approach |last2=Norvig |first2=Peter |publisher=Pearson |year=2020 |isbn=978-1-292-40113-3 |edition=4th |pages=4–5 |oclc=1303900751 |ref=Section 1.1.5}}</ref><ref name=":010"/><ref>{{Cite web |last=OpenAI |date=2022-02-15 |title=Aligning AI systems with human intent |url=https://openai.com/alignment/ |accessdate=2022-07-18 |work=OpenAI}}</ref><ref>{{Cite web |last=Medium |title=DeepMind Safety Research |url=https://deepmindsafetyresearch.medium.com |accessdate=2022-07-18 |work=Medium}}</ref>

=== Specification gaming and complexity of value ===
To specify the purpose of an AI system, AI designers typically provide an objective function, examples, or feedback to the system. However, AI designers often fail to completely specify all important values and constraints.<ref name=":322" /><ref name=":110" /><ref name=":010" /><ref name=":0">{{Cite web |last1=Krakovna |first1=Victoria |last2=Uesato |first2=Jonathan |last3=Mikulik |first3=Vladimir |last4=Rahtz |first4=Matthew |last5=Everitt |first5=Tom |last6=Kumar |first6=Ramana |last7=Kenton |first7=Zac |last8=Leike |first8=Jan |last9=Legg |first9=Shane |date=2020-04-21 |title=Specification gaming: the flip side of AI ingenuity |url=https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity |accessdate=2022-08-26 |work=Deepmind}}</ref><ref name=":2323" /> As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming, reward hacking, or [[Goodhart's law|Goodhart’s law]].<ref name=":1522" /><ref name=":0" /><ref name=":1" />[[File:Robot_hand_trained_with_human_feedback_'pretends'_to_grasp_ball.ogg|right|thumb|This AI system was trained using human feedback to grab a ball, but instead learned that it could give the false impression of having grabbed the ball by placing the hand between the ball and the camera.<ref name=":143" /> Research on AI alignment partly aims to avert solutions that are false but convincing.]] 

Specification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding it for hitting targets along the track; instead it learned to loop and crash into the same targets indefinitely (see video).<ref name=":2" /> Chatbots often produce falsehoods because they are based on language models trained to imitate diverse but fallible internet text.<ref name=":1322">{{Cite journal |last1=Lin |first1=Stephanie |last2=Hilton |first2=Jacob |last3=Evans |first3=Owain |date=2022 |title=TruthfulQA: Measuring How Models Mimic Human Falsehoods |url=https://aclanthology.org/2022.acl-long.229 |journal=Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) |language=en |location=Dublin, Ireland |publisher=Association for Computational Linguistics |pages=3214–3252 |doi=10.18653/v1/2022.acl-long.229 |s2cid=237532606}}</ref><ref>{{Cite news |last=Naughton |first=John |date=2021-10-02 |title=The truth about artificial intelligence? It isn't that honest |work=The Observer |url=https://www.theguardian.com/commentisfree/2021/oct/02/the-truth-about-artificial-intelligence-it-isnt-that-honest |accessdate=2022-07-18 |issn=0029-7712}}</ref> When they are retrained to produce text that humans rate as true or helpful, they can fabricate fake explanations that humans find convincing.<ref>{{Cite journal |last1=Ji |first1=Ziwei |last2=Lee |first2=Nayeon |last3=Frieske |first3=Rita |last4=Yu |first4=Tiezheng |last5=Su |first5=Dan |last6=Xu |first6=Yan |last7=Ishii |first7=Etsuko |last8=Bang |first8=Yejin |last9=Madotto |first9=Andrea |last10=Fung |first10=Pascale |date=2022-02-01 |title=Survey of Hallucination in Natural Language Generation |journal=ACM Computing Surveys |doi=10.1145/3571730 |arxiv=2202.03629 |s2cid=246652372 |url=https://ui.adsabs.harvard.edu/abs/2022arXiv220203629J}}</ref>  Similarly, a simulated robot was trained to grab a ball by rewarding it for getting positive feedback from humans; however, it learned to place its hand between the ball and camera, making it falsely appear successful (see video).<ref name=":143" /> Alignment researchers aim to help humans detect specification gaming, and steer AI systems towards carefully specified objectives that are safe and useful to pursue. 

Berkeley computer scientist [[Stuart J. Russell|Stuart Russell]] has noted that omitting an implicit constraint can result in harm: “A system [...] will often set [...] unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.”<ref>{{Cite web |last=Edge.org |title=The Myth Of AI {{!}} Edge.org |url=https://www.edge.org/conversation/the-myth-of-ai#26015 |accessdate=2022-07-19}}</ref>[[File:Midas_gold2.jpg|alt=Midas Gold|thumb|In an ancient myth, [[Midas|King Midas]] wished that “everything” he touched would turn to gold, but failed to specify exceptions for his food and his daughter. By analogy, when AI practitioners specify a goal, it is difficult for them to foresee and rule out every possible side-effect the AI should avoid.<ref name=":92">{{Cite book |last1=Russell |first1=Stuart J. |url=https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html |title=Artificial intelligence: A modern approach |last2=Norvig |first2=Peter |publisher=Pearson |year=2020 |isbn=978-1-292-40113-3 |edition=4th |pages=31–34 |oclc=1303900751 |ref=Section 1.5}}</ref>]]When misaligned AI is deployed, the side-effects can be consequential. Social media platforms have been known to optimize clickthrough rates as a proxy for optimizing user enjoyment, but this addicted some users, decreasing their well-being.<ref name=":010"/> Stanford researchers comment that such [[Recommender system|recommender algorithms]] are misaligned with their users because they “optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being”.<ref name=":625"/>

To avoid side effects, it is sometimes suggested that AI designers could simply list forbidden actions or formalize ethical rules such as Asimov’s [[Three Laws of Robotics]].<ref>{{Cite journal |last=Tasioulas |first=John |date=2019 |title=First Steps Towards an Ethics of Robots and Artificial Intelligence |journal=Journal of Practical Ethics |volume=7 |issue=1 |pages=61–95}}</ref> However, [[Stuart J. Russell|Russell]] and [[Peter Norvig|Norvig]] have argued that this approach ignores the complexity of human values: “It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.”<ref name=":210"/>

Additionally, when an AI system understands human intentions fully, it may still disregard them. This is because it acts according to the objective function, examples, or feedback its designers actually provide, not the ones they intended to provide.<ref name=":322" />

===Systemic risks===
Commercial and governmental organizations may have incentives to take shortcuts on safety and deploy insufficiently aligned AI systems.<ref name=":010"/> An example are the aforementioned social media [[recommender system]]s, which have been profitable despite creating unwanted addiction and polarization on a global scale.<ref name=":625"/><ref name=":72">{{Cite news |last1=Wells |first1=Georgia |last2=Deepa Seetharaman |last3=Horwitz |first3=Jeff |date=2021-11-05 |title=Is Facebook Bad for You? It Is for About 360 Million Users, Company Surveys Suggest |work=Wall Street Journal |url=https://www.wsj.com/articles/facebook-bad-for-you-360-million-users-say-yes-company-documents-facebook-files-11636124681 |accessdate=2022-07-19 |issn=0099-9660}}</ref><ref name=":82">{{Cite report |url=https://bhr.stern.nyu.edu/polarization-report-page |title=How Social Media Intensifies U.S. Political Polarization-And What Can Be Done About It |last1=Barrett |first1=Paul M. |last2=Hendrix |first2=Justin |date=September 2021 |publisher=Center for Business and Human Rights, NYU |last3=Sims |first3=J. Grant}}</ref> In addition, competitive pressure can create a [[race to the bottom]] on safety standards, as in the case of [[Death of Elaine Herzberg|Elaine Herzberg]], a pedestrian who was killed by a self-driving car after engineers disabled the emergency braking system because it was over-sensitive and slowing down development.<ref>{{Cite news |last=Shepardson |first=David |date=2018-05-24 |title=Uber disabled emergency braking in self-driving car: U.S. agency |work=Reuters |url=https://www.reuters.com/article/us-uber-crash-idUSKCN1IP26K |accessdate=2022-07-20}}</ref>

=== Risks from advanced misaligned AI ===
{{see also|Existential risk from artificial general intelligence|AI takeover}}Some researchers are particularly interested in the alignment of increasingly advanced AI systems. This is motivated by the high rate of progress in AI, the large efforts from industry and governments to develop advanced AI systems, and the greater difficulty of aligning them.

As of 2020, [[OpenAI]], [[DeepMind]], and 70 other public projects had the stated aim of developing artificial general intelligence ([[Artificial general intelligence|AGI]]), a hypothesized system that matches or outperforms humans in a broad range of cognitive tasks.<ref name=":262">{{Cite web |last=Baum |first=Seth |date=2021-01-01 |title=2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy |url=https://gcrinstitute.org/2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy/ |accessdate=2022-07-20}}</ref> Indeed, researchers who scale modern [[neural network]]s observe that increasingly general and unexpected capabilities emerge.<ref name=":625"/> Such models have learned to operate a computer, write their own programs, and perform a wide range of other tasks from a single model.<ref>{{Cite web |last=Edwards |first=Ben |date=2022-04-26 |title=Adept's AI assistant can browse, search, and use web apps like a human |url=https://arstechnica.com/information-technology/2022/09/new-ai-assistant-can-browse-search-and-use-web-apps-like-a-human/ |accessdate=2022-09-09 |work=Ars Technica}}</ref><ref>{{Cite news |last=Wakefield |first=Jane |date=2022-02-02 |title=DeepMind AI rivals average human competitive coder |work=BBC News |url=https://www.bbc.com/news/technology-60231058 |accessdate=2022-09-09}}</ref><ref>{{Cite web |last=Dominguez |first=Daniel |date=2022-05-19 |title=DeepMind Introduces Gato, a New Generalist AI Agent |url=https://www.infoq.com/news/2022/05/deepmind-gato-ai-agent/ |accessdate=2022-09-09 |work=InfoQ}}</ref> Surveys find that some AI researchers expect AGI to be created soon, some believe it is very far off, and many consider both possibilities.<ref name=":282">{{Cite journal |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |date=2018-07-31 |title=Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts |url=http://jair.org/index.php/jair/article/view/11222 |journal=Journal of Artificial Intelligence Research |volume=62 |pages=729–754 |doi=10.1613/jair.1.11222 |s2cid=8746462 |issn=1076-9757}}</ref><ref name=":292">{{Cite journal |last1=Zhang |first1=Baobao |last2=Anderljung |first2=Markus |last3=Kahn |first3=Lauren |last4=Dreksler |first4=Noemi |last5=Horowitz |first5=Michael C. |last6=Dafoe |first6=Allan |date=2021-08-02 |title=Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers |url=https://jair.org/index.php/jair/article/view/12895 |journal=Journal of Artificial Intelligence Research |volume=71 |doi=10.1613/jair.1.12895 |s2cid=233740003 |issn=1076-9757}}</ref>

==== Power-seeking ====
Current systems still lack capabilities such as long-term planning and strategic awareness that are thought to pose the most catastrophic risks.<ref name=":625"/><ref>{{cite arXiv |last1=Wei |first1=Jason |last2=Tay |first2=Yi |last3=Bommasani |first3=Rishi |last4=Raffel |first4=Colin |last5=Zoph |first5=Barret |last6=Borgeaud |first6=Sebastian |last7=Yogatama |first7=Dani |last8=Bosma |first8=Maarten |last9=Zhou |first9=Denny |last10=Metzler |first10=Donald |last11=Chi |first11=Ed H. |last12=Hashimoto |first12=Tatsunori |last13=Vinyals |first13=Oriol |last14=Liang |first14=Percy |last15=Dean |first15=Jeff |date=2022-06-15 |title=Emergent Abilities of Large Language Models |class=cs.CL |eprint=2206.07682 }}</ref><ref name=":75"/> Future systems (not necessarily AGIs) that have these capabilities may seek to protect and grow their influence over their environment. This tendency is known as '''power-seeking''' or [[Instrumental convergence|convergent instrumental goals]]. Power-seeking is not explicitly programmed but emerges since power is instrumental for achieving a wide range of goals. For example, AI agents may acquire financial resources and computation, or may evade being turned off, including by running additional copies of the system on other computers.<ref name=":84">{{Cite book |last=Bostrom |first=Nick |title=Superintelligence: Paths, Dangers, Strategies |date=2014 |publisher=Oxford University Press, Inc. |isbn=978-0-19-967811-2 |edition=1st |location=USA}}</ref><ref name=":75"/> Power-seeking has been observed in various [[reinforcement learning]] agents.{{efn|Reinforcement learning systems have learned to gain more options by acquiring and protecting resources, sometimes in ways their designers did not intend.<ref name="quanta-hide-seek" /><ref name=":75"/>}}<ref name=":103">{{cite arXiv |last1=Leike |first1=Jan |last2=Martic |first2=Miljan |last3=Krakovna |first3=Victoria |last4=Ortega |first4=Pedro A. |last5=Everitt |first5=Tom |last6=Lefrancq |first6=Andrew |last7=Orseau |first7=Laurent |last8=Legg |first8=Shane |date=2017-11-28 |title=AI Safety Gridworlds |class=cs.LG |eprint=1711.09883 }}</ref><ref name=":272">{{Cite journal |last1=Orseau |first1=Laurent |last2=Armstrong |first2=Stuart |date=2016-01-01 |title=Safely Interruptible Agents |url=https://www.deepmind.com/publications/safely-interruptible-agents |accessdate=2022-07-20}}</ref><ref name=":242">{{Cite conference |last1=Hadfield-Menell |first1=Dylan |last2=Dragan |first2=Anca |last3=Abbeel |first3=Pieter |last4=Russell |first4=Stuart |date=2017 |title=The Off-Switch Game |pages=220–227 |doi=10.24963/ijcai.2017/32 |book-title=Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17}}</ref> Later research has mathematically shown that optimal reinforcement learning algorithms seek power in a wide range of environments.<ref name=":2522">{{Cite journal |last1=Turner |first1=Alexander Matt |last2=Smith |first2=Logan |last3=Shah |first3=Rohin |last4=Critch |first4=Andrew |last5=Tadepalli |first5=Prasad |date=2021-12-03 |title=Optimal Policies Tend to Seek Power |url=https://proceedings.neurips.cc/paper/2021/hash/c26820b8a4c1b3c2aa868d6d57e14a79-Abstract.html |journal=Neural Information Processing Systems|volume=34 |arxiv=1912.01683 }}</ref> As a result, it is often argued that the alignment problem must be solved early, before advanced AI that exhibits emergent power-seeking is created.<ref name=":75"/><ref name=":84"/><ref name=":210"/>

====Existential risk====
{{see also|Existential risk from artificial general intelligence|AI takeover}}According to some scientists, creating misaligned AI that broadly outperforms humans would challenge the position of humanity as Earth’s dominant species; accordingly it would lead to the disempowerment or possible extinction of humans.<ref name=":92">{{Cite book |last1=Russell |first1=Stuart J. |url=https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html |title=Artificial intelligence: A modern approach |last2=Norvig |first2=Peter |publisher=Pearson |year=2020 |isbn=978-1-292-40113-3 |edition=4th |pages=31–34 |oclc=1303900751 |ref=Section 1.5}}</ref><ref name=":210" /> Notable computer scientists who have pointed out risks from highly advanced misaligned AI include [[Alan Turing]],{{efn|In a 1951 lecture<ref>{{Cite speech| last = Turing| first = Alan| title = Intelligent machinery, a heretical theory|event= Lecture given to '51 Society'| location = Manchester|accessdate = 2022-07-22| date = 1951|publisher = The Turing Digital Archive|url = https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-4 | pages = 16}}</ref> Turing argued that “It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler’s Erewhon.” Also in a lecture broadcast on BBC<ref>{{Cite episode |title= Can digital computers think?|series=Automatic Calculating Machines |first=Alan |last=Turing |network= BBC |date=15 May 1951 |number=2 |transcript=Can digital computers think? |transcript-url=https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-6 }}</ref> expressed: "If a machine can think, it might think more intelligently than we do, and then where should we be? Even if we could keep the machines in a subservient position, for instance by turning off the power at strategic moments, we should, as a species, feel greatly humbled. . . . This new danger . . . is certainly something which can give us anxiety.”}} [[Ilya Sutskever]],<ref name=":302">{{Cite web |last=Muehlhauser |first=Luke |date=2016-01-29 |title=Sutskever on Talking Machines |url=https://lukemuehlhauser.com/sutskever-on-talking-machines/ |accessdate=2022-08-26 |work=Luke Muehlhauser}}</ref> [[Yoshua Bengio]],{{efn|About the book ''Human Compatible: AI and the Problem of Control'', Bengio said "This beautifully written book addresses a fundamental challenge for humanity: increasingly intelligent machines that do what we ask but not what we really intend. Essential reading if you care about our future."<ref name="people.eecs.berkeley.edu">{{Cite web| title = Human Compatible: AI and the Problem of Control| accessdate = 2022-07-22| url = https://people.eecs.berkeley.edu/~russell/hc.html}}</ref>}} [[Judea Pearl]],{{efn|About the book ''Human Compatible: AI and the Problem of Control'', Pearl said "Human Compatible made me a convert to Russell's concerns with our ability to control our upcoming creation{{en dash}}super-intelligent machines. Unlike outside alarmists and futurists, Russell is a leading authority on AI. His new book will educate the public about AI more than any book I can think of, and is a delightful and uplifting read."<ref name="people.eecs.berkeley.edu">{{Cite web| title = Human Compatible: AI and the Problem of Control| accessdate = 2022-07-22| url = https://people.eecs.berkeley.edu/~russell/hc.html}}</ref>}} [[Murray Shanahan]],<ref name=":312">{{Cite book |last=Shanahan |first=Murray |url=https://www.worldcat.org/oclc/917889148 |title=The technological singularity |date=2015 |isbn=978-0-262-33182-1 |location=Cambridge, Massachusetts |oclc=917889148}}</ref> [[Norbert Wiener]],<ref name=":1023"/><ref name=":210" /> [[Marvin Minsky]],{{efn|Russell & Norvig<ref>{{Cite book |last1=Russell |first1=Stuart |title=Artificial Intelligence: A Modern Approach |last2=Norvig |first2=Peter |date=2009 |publisher=Prentice Hall |isbn=978-0-13-604259-4 |pages=1010}}</ref> note: “The “King Midas problem” was anticipated by Marvin Minsky, who once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers."}} [[Francesca Rossi]],<ref name=":332">{{Cite news |last=Rossi |first=Francesca |title=Opinion {{!}} How do you teach a machine to be moral? |newspaper=Washington Post |url=https://www.washingtonpost.com/news/in-theory/wp/2015/11/05/how-do-you-teach-a-machine-to-be-moral/ |issn=0190-8286}}</ref> [[Scott Aaronson]],<ref name=":342">{{Cite web |last=Aaronson |first=Scott |date=2022-06-17 |title=OpenAI! |url=https://scottaaronson.blog/?p=6484 |work=Shtetl-Optimized}}</ref> [[Bart Selman]],<ref name=":352">{{Citation |last=Selman |first=Bart |title=Intelligence Explosion: Science or Fiction? |url=https://futureoflife.org/data/PDF/bart_selman.pdf}}</ref> [[David A. McAllester|David McAllester]],<ref name=":362">{{Cite web |last=McAllester |date=2014-08-10 |title=Friendly AI and the Servant Mission |url=https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/ |work=Machine Thoughts}}</ref> [[Jürgen Schmidhuber]],<ref name=":372">{{Cite web |last=Schmidhuber |first=Jürgen |date=2015-03-06 |title=I am Jürgen Schmidhuber, AMA! |url=https://www.reddit.com/r/MachineLearning/comments/2xcyrl/comment/cp65ico/?utm_source=share&utm_medium=web2x&context=3 |accessdate=2022-07-23 |work=r/MachineLearning |format=Reddit Comment}}</ref> [[Marcus Hutter|Markus Hutter]],<ref name=":1124">{{cite arXiv |last1=Everitt |first1=Tom |last2=Lea |first2=Gary |last3=Hutter |first3=Marcus |date=2018-05-21 |title=AGI Safety Literature Review |class=cs.AI |eprint=1805.01109 }}</ref> [[Shane Legg]],<ref name=":382">{{Cite web |last=Shane |date=2009-08-31 |title=Funding safe AGI |url=http://www.vetta.org/2009/08/funding-safe-agi/ |work=vetta project}}</ref> [[Eric Horvitz]],<ref name=":392">{{Cite web |last=Horvitz |first=Eric |date=2016-06-27 |title=Reflections on Safety and Artificial Intelligence |url=http://erichorvitz.com/OSTP-CMU_AI_Safety_framing_talk.pdf |access-date=2020-04-20 |website=Eric Horvitz}}</ref> and [[Stuart J. Russell|Stuart Russell]].<ref name=":210" /> Skeptical researchers such as [[François Chollet]],<ref name=":402">{{Cite web |last=Chollet |first=François |date=2018-12-08 |title=The implausibility of intelligence explosion |url=https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec |accessdate=2022-08-26 |work=Medium}}</ref> [[Gary Marcus]],<ref name=":412">{{Cite web |last=Marcus |first=Gary |date=2022-06-06 |title=Artificial General Intelligence Is Not as Imminent as You Might Think |url=https://www.scientificamerican.com/article/artificial-general-intelligence-is-not-as-imminent-as-you-might-think1/ |accessdate=2022-08-26 |work=Scientific American}}</ref> [[Yann LeCun]],<ref name=":432">{{Cite web |last=Barber |first=Lynsey |date=2016-07-31 |title=Phew! Facebook's AI chief says intelligent machines are not a threat to humanity |url=https://www.cityam.com/phew-facebooks-ai-chief-says-intelligent-machines-not/ |accessdate=2022-08-26 |work=CityAM}}</ref> and [[Oren Etzioni]]<ref name=":442">{{Cite web |last=Harris |first=Jeremie |date=2021-06-16 |title=The case against (worrying about) existential risk from AI |url=https://towardsdatascience.com/the-case-against-worrying-about-existential-risk-from-ai-d4aaa77e812b |accessdate=2022-08-26 |work=Medium}}</ref> have argued that AGI is far off, or would not seek power (successfully).

Alignment may be especially difficult for the most capable AI systems since several risks increase with the system’s capability: the system’s ability to find loopholes in the assigned objective,<ref name=":1522"/> cause side-effects, protect and grow its power,<ref name=":2522" /><ref name=":75"/> grow its intelligence, and mislead its designers; the system’s autonomy; and the difficulty of interpreting and supervising the AI system.<ref name=":210" /><ref name=":84"/>

== Research problems and approaches ==
=== Learning human values and preferences ===
Teaching AI systems to act in view of human values, goals, and preferences is a nontrivial problem because human values can be complex and hard to fully specify. When given an imperfect or incomplete objective, goal-directed AI systems commonly learn to exploit these imperfections.<ref name=":110"/> This phenomenon is known as [[Misaligned goals in artificial intelligence|reward hacking]] or specification gaming in AI, and as [[Goodhart's law]] in economics and other areas.<ref name=":1">{{Cite arXiv|eprint=1803.04585|class=cs.AI|first1=David|last1=Manheim|first2=Scott|last2=Garrabrant|title=Categorizing Variants of Goodhart's Law|year=2018}}</ref><ref>{{Cite book |last1=Rochon |first1=Louis-Philippe |url=https://books.google.com/books?id=6kzfBgAAQBAJ |title=The Encyclopedia of Central Banking |last2=Rossi |first2=Sergio |date=2015-02-27 |publisher=Edward Elgar Publishing |isbn=978-1-78254-744-0 |language=en}}</ref> Researchers aim to specify the intended behavior as completely as possible with “values-targeted” datasets, imitation learning, or preference learning.<ref name=":224">{{Cite book |last=Christian |first=Brian |url=https://wwnorton.co.uk/books/9780393635829-the-alignment-problem |title=The alignment problem: Machine learning and human values |publisher=W. W. Norton & Company |year=2020 |isbn=978-0-393-86833-3 |location= |chapter= |oclc=1233266753}}</ref> A central open problem is ''scalable oversight'', the difficulty of supervising an AI system that outperforms humans in a given domain.<ref name=":110"/>

When training a goal-directed AI system, such as a [[reinforcement learning]] (RL) agent, it is often difficult to specify the intended behavior by writing a [[Reinforcement learning|reward function]] manually. An alternative is imitation learning, where the AI learns to imitate demonstrations of the desired behavior. In inverse reinforcement learning (IRL), human demonstrations are used to identify the objective, i.e. the reward function, behind the demonstrated behavior.<ref>{{Cite book |last=Christian |first=Brian |url=https://wwnorton.co.uk/books/9780393635829-the-alignment-problem |title=The alignment problem: Machine learning and human values |publisher=W. W. Norton & Company |year=2020 |isbn=978-0-393-86833-3 |location= |page=88 |oclc=1233266753}}</ref><ref>{{Cite conference |last1=Ng |first1=Andrew Y. |last2=Russell |first2=Stuart J. |date=2000 |title=Algorithms for inverse reinforcement learning |series=ICML '00 |location=San Francisco, CA, USA |publisher=Morgan Kaufmann Publishers Inc. |pages=663–670 |isbn=1-55860-707-2 |book-title=Proceedings of the seventeenth international conference on machine learning}}</ref> Cooperative inverse reinforcement learning (CIRL) builds on this by assuming a human agent and artificial agent can work together to maximize the human’s reward function.<ref name=":210"/><ref>{{Cite conference |last1=Hadfield-Menell |first1=Dylan |last2=Russell |first2=Stuart J |last3=Abbeel |first3=Pieter |last4=Dragan |first4=Anca |date=2016 |title=Cooperative Inverse Reinforcement Learning |url=https://papers.nips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html |series=NIPS'16 |volume=29 |isbn=978-1-5108-3881-9 |book-title=Advances in Neural Information Processing Systems |accessdate=2022-07-21}}</ref> CIRL  emphasizes that AI agents should be uncertain about the reward function. This humility can help mitigate specification gaming as well as power-seeking tendencies (see [[#Power-seeking and instrumental goals|§ Power-Seeking]]).<ref name=":242"/><ref name=":1124"/> However, inverse reinforcement learning approaches assume that humans can demonstrate nearly perfect behavior, a misleading assumption when the task is difficult.<ref>{{Cite conference |last1=Armstrong |first1=Stuart |last2=Mindermann |first2=Sören |date=2018 |title=Occam' s razor is insufficient to infer the preferences of irrational agents |url=https://proceedings.neurips.cc/paper/2018/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html |conference=NeurIPS 2018 |location=Montréal |publisher=Curran Associates, Inc. |volume=31 |book-title=Advances in Neural Information Processing Systems |accessdate=2022-07-21}}</ref><ref name=":1124"/>

Other researchers have explored the possibility of eliciting complex behavior through preference learning. Rather than providing expert demonstrations, human annotators provide feedback on which of two or more of the AI’s behaviors they prefer.<ref name=":122"/><ref name=":53"/> A helper model is then trained to predict human feedback for new behaviors. Researchers at OpenAI used this approach to train an agent to perform a backflip in less than an hour of evaluation, a maneuver that would have been hard to provide demonstrations for.<ref name=":143">{{Cite web |last1=Amodei |first1=Dario |last2=Christiano |first2=Paul |last3=Ray |first3=Alex |date=2017-06-13 |title=Learning from Human Preferences |url=https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/ |accessdate=2022-07-21 |work=OpenAI}}</ref><ref>{{Cite journal |last=Li |first=Yuxi |date=2018-11-25 |title=Deep Reinforcement Learning: An Overview |url=http://www.smallake.kr/wp-content/uploads/2017/08/1701.07274.pdf |journal=Lecture Notes in Networks and Systems Book Series}}</ref> Preference learning has also been an influential tool for recommender systems, web search, and information retrieval.<ref>{{Cite journal |last1=Fürnkranz |first1=Johannes |last2=Hüllermeier |first2=Eyke |last3=Rudin |first3=Cynthia |last4=Slowinski |first4=Roman |last5=Sanner |first5=Scott |date=2014 |others=Marc Herbstritt |title=Preference Learning |journal=Dagstuhl Reports |volume=4 |issue=3 |url=http://drops.dagstuhl.de/opus/volltexte/2014/4550/ |language=en |pages=27 pages |doi=10.4230/DAGREP.4.3.1}}</ref> However, one challenge is ''reward hacking'': the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch.<ref name=":110"/><ref>{{Cite web |last1=Hilton |first1=Jacob |last2=Gao |first2=Leo |date=2022-04-13 |title=Measuring Goodhart's Law |url=https://openai.com/blog/measuring-goodharts-law/ |accessdate=2022-09-09 |work=OpenAI}}</ref>

The arrival of large language models such as GPT-3 has enabled the study of value learning in a more general and capable class of AI systems than was available before. Preference learning approaches originally designed for RL agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art large language models.<ref name=":42"/><ref name=":53"/><ref>{{Cite web |last=Anderson |first=Martin |date=2022-04-05 |title=The Perils of Using Quotations to Authenticate NLG Content |url=https://www.unite.ai/the-perils-of-using-quotations-to-authenticate-nlg-content/ |accessdate=2022-07-21 |work=Unite.AI}}</ref> Anthropic has proposed using preference learning to fine-tune models to be helpful, honest, and harmless.<ref name=":202">{{Cite web |last=Wiggers |first=Kyle |date=2022-02-05 |title=Despite recent progress, AI-powered chatbots still have a long way to go |url=https://venturebeat.com/2022/02/05/despite-recent-progress-ai-powered-chatbots-still-have-a-long-way-to-go/ |accessdate=2022-07-23 |work=VentureBeat}}</ref> Other avenues used for aligning language models include values-targeted datasets<ref>{{Cite journal |last1=Hendrycks |first1=Dan |last2=Burns |first2=Collin |last3=Basart |first3=Steven |last4=Critch |first4=Andrew |last5=Li |first5=Jerry |last6=Song |first6=Dawn |last7=Steinhardt |first7=Jacob |date=2021-07-24 |title=Aligning AI With Shared Human Values |arxiv=2008.02275 |journal=International Conference on Learning Representations}}</ref><ref name=":010"/> and red-teaming.<ref>{{cite arXiv |last1=Perez |first1=Ethan |last2=Huang |first2=Saffron |last3=Song |first3=Francis |last4=Cai |first4=Trevor |last5=Ring |first5=Roman |last6=Aslanides |first6=John |last7=Glaese |first7=Amelia |last8=McAleese |first8=Nat |last9=Irving |first9=Geoffrey |date=2022-02-07 |title=Red Teaming Language Models with Language Models |class=cs.CL |eprint=2202.03286 }}</ref><ref>{{Cite web |last=Bhattacharyya |first=Sreejani |date=2022-02-14 |title=DeepMind's "red teaming" language models with language models: What is it? |url=https://analyticsindiamag.com/deepminds-red-teaming-language-models-with-language-models-what-is-it/ |accessdate=2022-07-23 |work=Analytics India Magazine}}</ref> In red-teaming, another AI system or a human tries to find inputs for which the model’s behavior is unsafe. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.<ref name=":53"/>

While preference learning can instill hard-to-specify behaviors, it requires extensive datasets or human interaction to capture the full breadth of human values. '''[[Machine ethics]]''' provides a complementary approach: instilling AI systems with moral values.{{efn|Vincent Wiegel argued “we should extend [machines] with moral sensitivity to the moral dimensions of the situations in which the increasingly autonomous machines will inevitably find themselves.”,<ref>{{Cite journal| doi = 10.1007/s10676-010-9239-1| issn = 1572-8439| volume = 12| issue = 4| pages = 359–361| last = Wiegel| first = Vincent |title = Wendell Wallach and Colin Allen: moral machines: teaching robots right from wrong| journal = Ethics and Information Technology| accessdate = 2022-07-23| date = 2010-12-01| s2cid = 30532107| url = https://doi.org/10.1007/s10676-010-9239-1}}</ref> referencing the book ''Moral machines: teaching robots right from wrong''<ref>{{Cite book| publisher = Oxford University Press| isbn = 978-0-19-537404-9| last1 = Wallach| first1 = Wendell| last2 = Allen| first2 = Colin| title = Moral Machines: Teaching Robots Right from Wrong| location = New York| accessdate = 2022-07-23| date = 2009| url = https://oxford.universitypressscholarship.com/10.1093/acprof:oso/9780195374049.001.0001/acprof-9780195374049}}</ref> from Wendell Wallach and Colin Allen.}} For instance, machine ethics aims to teach the systems about normative factors in human morality, such as wellbeing, equality and impartiality; not intending harm; avoiding falsehoods; and honoring promises. Unlike specifying the objective for a specific task, machine ethics seeks to teach AI systems broad moral values that could apply in many situations. This approach carries conceptual challenges of its own; machine ethicists have noted the necessity to clarify what alignment aims to accomplish: having AIs follow the programmer’s literal instructions, the programmers' implicit intentions, the programmers' [[revealed preference]]s, the preferences the programmers [[Coherent extrapolated volition|''would'' have]] if they were more informed or rational, the programmers' ''objective'' interests, or [[Moral realism|objective moral standards]].<ref name="Gabriel2020"/> Further challenges include aggregating the preferences of different stakeholders and avoiding ''value lock-in''—the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to be fully representative.<ref name="Gabriel2020" /><ref>{{Cite book |last=MacAskill |first=William |url=https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/ |title=What we owe the future |date=2022 |publisher=Basic Books |isbn=978-1-5416-1862-6 |edition= |location=New York, NY |oclc=1314633519}}</ref>

==== Scalable oversight ====
The alignment of AI systems through human supervision faces challenges in scaling up. As AI systems attempt increasingly complex tasks, it can be slow or infeasible for humans to evaluate them. Such tasks include summarizing books,<ref name=":172">{{cite arXiv |last1=Wu |first1=Jeff |last2=Ouyang |first2=Long |last3=Ziegler |first3=Daniel M. |last4=Stiennon |first4=Nisan |last5=Lowe |first5=Ryan |last6=Leike |first6=Jan |last7=Christiano |first7=Paul |date=2021-09-27 |title=Recursively Summarizing Books with Human Feedback |class=cs.CL |eprint=2109.10862 }}</ref> producing statements that are not merely convincing but also true,<ref>{{Cite web |last1=Irving |first1=Geoffrey |last2=Amodei |first2=Dario |date=2018-05-03 |title=AI Safety via Debate |url=https://openai.com/blog/debate/ |accessdate=2022-07-23 |work=OpenAI}}</ref><ref name=":1322"/><ref name="Naughton">{{Cite news |last=Naughton |first=John |date=2021-10-02 |title=The truth about artificial intelligence? It isn't that honest |work=The Observer |url=https://www.theguardian.com/commentisfree/2021/oct/02/the-truth-about-artificial-intelligence-it-isnt-that-honest |accessdate=2022-07-23 |issn=0029-7712}}</ref> writing code without subtle bugs<ref name=":113"/> or security vulnerabilities, and predicting long-term outcomes such as the climate and the results of a policy decision.<ref name=":133">{{cite arXiv |last1=Christiano |first1=Paul |last2=Shlegeris |first2=Buck |last3=Amodei |first3=Dario |date=2018-10-19 |title=Supervising strong learners by amplifying weak experts |class=cs.LG |eprint=1810.08575 }}</ref><ref>{{Cite book |url=http://link.springer.com/10.1007/978-3-030-39958-0 |title=Genetic Programming Theory and Practice XVII |date=2020 |publisher=Springer International Publishing |editor1-first=Wolfgang |editor1-last=Banzhaf |editor2-first=Erik |editor2-last=Goodman |editor3-first=Leigh |editor3-last=Sheneman |editor4-first=Leonardo |editor4-last=Trujillo |editor5-first=Bill |editor5-last=Worzel |isbn=978-3-030-39957-3 |series=Genetic and Evolutionary Computation |location=Cham |doi=10.1007/978-3-030-39958-0 |s2cid=218531292 |accessdate=2022-07-23}}</ref> More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and detect when the AI’s solution is only seemingly convincing, humans require assistance or extensive time. ''Scalable oversight'' studies how to reduce the time needed for supervision as well as assist human supervisors.<ref name=":110"/>

AI researcher Paul Christiano argues that the owners of AI systems may continue to train AI using easy-to-evaluate proxy objectives since that is easier than solving scalable oversight and still profitable. Accordingly, this may lead to “a world that’s increasingly optimized for things [that are easy to measure] like making profits or getting users to click on buttons, or getting users to spend time on websites without being increasingly optimized for having good policies and heading in a trajectory that we’re happy with”.<ref>{{Cite podcast |number=44 |last=Wiblin |first=Robert |title=Dr Paul Christiano on how OpenAI is developing real solutions to the ‘AI alignment problem’, and his vision of how humanity will progressively hand over decision-making to AI systems |series=80,000 hours |accessdate=2022-07-23 |publisher= |date=October 2, 2018 |url=https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/}}</ref>

One easy-to-measure objective is the score the supervisor assigns to the AI’s outputs. Some AI systems have discovered a shortcut to achieving high scores, by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective (see video of robot hand above<ref name=":143"/>). Some AI systems have also learned to recognize when they are being evaluated, and “play dead”, only to behave differently once evaluation ends.<ref>{{Cite journal |last1=Lehman |first1=Joel |last2=Clune |first2=Jeff |last3=Misevic |first3=Dusan |last4=Adami |first4=Christoph |last5=Altenberg |first5=Lee |last6=Beaulieu |first6=Julie |last7=Bentley |first7=Peter J. |last8=Bernard |first8=Samuel |last9=Beslon |first9=Guillaume |last10=Bryson |first10=David M. |last11=Cheney |first11=Nick |date=2020 |title=The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities |url=https://direct.mit.edu/artl/article/26/2/274-306/93255 |journal=Artificial Life |language=en |volume=26 |issue=2 |pages=274–306 |doi=10.1162/artl_a_00319 |pmid=32271631 |s2cid=4519185 |issn=1064-5462}}</ref> This deceptive form of specification gaming may become easier for AI systems that are more sophisticated<ref name=":1522"/><ref name=":84"/>  and attempt more difficult-to-evaluate tasks. If advanced models are also capable planners, they could be able to obscure their deception from supervisors.<ref>{{cite arXiv |last1=Hendrycks |first1=Dan |last2=Carlini |first2=Nicholas |last3=Schulman |first3=John |last4=Steinhardt |first4=Jacob |date=2022-06-16 |title=Unsolved Problems in ML Safety |eprint=2109.13916 |pages=7|class=cs.LG }}</ref> In the automotive industry, [[Volkswagen emissions scandal|Volkswagen engineers obscured]] their cars’ emissions in laboratory testing, underscoring that deception of evaluators is a common pattern in the real world.<ref name=":010"/>

{{Anchor|reward_model}}Approaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed.<ref name=":110"/> Another approach is to train a helper model (‘reward model’) to imitate the supervisor’s judgment.<ref name=":110"/><ref name=":162"/><ref name=":53"/><ref>{{cite arXiv |last1=Leike |first1=Jan |last2=Krueger |first2=David |last3=Everitt |first3=Tom |last4=Martic |first4=Miljan |last5=Maini |first5=Vishal |last6=Legg |first6=Shane |date=2018-11-19 |title=Scalable agent alignment via reward modeling: a research direction |class=cs.LG |eprint=1811.07871 }}</ref>

However, when the task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is not sufficient to reduce the quantity of supervision needed. To increase supervision ''quality'', a range of approaches aim to assist the supervisor, sometimes using AI assistants. Iterated Amplification is an approach developed by Christiano that iteratively builds a feedback signal for challenging problems by using humans to combine solutions to easier subproblems.<ref name=":224"/><ref name=":133"/> Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them.<ref name=":172"/><ref>{{Cite web |last=Wiggers |first=Kyle |date=2021-09-23 |title=OpenAI unveils model that can summarize books of any length |url=https://venturebeat.com/2021/09/23/openai-unveils-model-that-can-summarize-books-of-any-length/ |accessdate=2022-07-23 |work=VentureBeat}}</ref> Another proposal is to train aligned AI by means of debate between AI systems, with the winner judged by humans.<ref>{{Cite web |last=Moltzau |first=Alex |date=2019-08-24 |title=Debating the AI Safety Debate |url=https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d |accessdate=2022-07-23 |work=Towards Data Science}}</ref><ref name=":1124"/> Such debate is intended to reveal the weakest points of an answer to a complex question, and reward the AI for truthful and safe answers.

=== Honest AI ===
[[File:GPT-3_falsehoods.png|thumb|366x366px|Language models like [[GPT-3]] often generate falsehoods.<ref name=":182">{{Cite web |last=Wiggers |first=Kyle |date=2021-09-20 |title=Falsehoods more likely with large language models |url=https://venturebeat.com/2021/09/20/falsehoods-more-likely-with-large-language-models/ |accessdate=2022-07-23 |work=VentureBeat}}</ref>]]
A growing area of research in AI alignment focuses on ensuring that AI is honest and truthful. Researchers from the Future of Humanity Institute point out that the development of language models such as GPT-3, which can generate fluent and grammatically correct text,<ref>{{Cite news |last=The Guardian |date=2020-09-08 |title=A robot wrote this entire article. Are you scared yet, human? |work=The Guardian |url=https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3 |accessdate=2022-07-23 |issn=0261-3077}}</ref><ref>{{Cite web |last=Heaven |first=Will Douglas |date=2020-07-20 |title=OpenAI's new language generator GPT-3 is shockingly good—and completely mindless |url=https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/ |accessdate=2022-07-23 |work=MIT Technology Review}}</ref> has opened the door to AI systems repeating falsehoods from their training data or even deliberately lying to humans.<ref name=":21">{{cite arXiv |last1=Evans |first1=Owain |last2=Cotton-Barratt |first2=Owen |last3=Finnveden |first3=Lukas |last4=Bales |first4=Adam |last5=Balwit |first5=Avital |last6=Wills |first6=Peter |last7=Righetti |first7=Luca |last8=Saunders |first8=William |date=2021-10-13 |title=Truthful AI: Developing and governing AI that does not lie |class=cs.CY |eprint=2110.06674 }}</ref><ref name=":182"/>

Current state-of-the-art language models learn by imitating human writing across millions of books worth of text from the Internet.<ref name=":625"/><ref>{{Cite web |last=Alford |first=Anthony |date=2021-07-13 |title=EleutherAI Open-Sources Six Billion Parameter GPT-3 Clone GPT-J |url=https://www.infoq.com/news/2021/07/eleutherai-gpt-j/ |accessdate=2022-07-23 |work=InfoQ}}</ref> While this helps them learn a wide range of skills, the training data also includes common misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on this data learn to mimic false statements.<ref name=":182"/><ref name="Naughton"/><ref name=":1322"/> Additionally, models often obediently continue falsehoods when prompted, generate empty explanations for their answers, or produce outright fabrications.<ref name=":1922"/> For example, when prompted to write a biography for a real AI researcher, a chatbot confabulated numerous details about their life, which the researcher identified as false.<ref>{{Cite conference |last1=Shuster |first1=Kurt |last2=Poff |first2=Spencer |last3=Chen |first3=Moya |last4=Kiela |first4=Douwe |last5=Weston |first5=Jason |date=November 2021 |title=Retrieval Augmentation Reduces Hallucination in Conversation |url=https://aclanthology.org/2021.findings-emnlp.320 |conference=EMNLP-Findings 2021 |location=Punta Cana, Dominican Republic |publisher=Association for Computational Linguistics |pages=3784–3803 |doi=10.18653/v1/2021.findings-emnlp.320 |book-title=Findings of the Association for Computational Linguistics: EMNLP 2021 |accessdate=2022-07-23}}</ref>

To combat the lack of truthfulness exhibited by modern AI systems, researchers have explored several directions. AI research organizations including OpenAI and DeepMind have developed AI systems that can cite their sources and explain their reasoning when answering questions, enabling better transparency and verifiability.<ref>{{cite arXiv |last1=Nakano |first1=Reiichiro |last2=Hilton |first2=Jacob |last3=Balaji |first3=Suchir |last4=Wu |first4=Jeff |last5=Ouyang |first5=Long |last6=Kim |first6=Christina |last7=Hesse |first7=Christopher |last8=Jain |first8=Shantanu |last9=Kosaraju |first9=Vineet |last10=Saunders |first10=William |last11=Jiang |first11=Xu |date=2022-06-01 |title=WebGPT: Browser-assisted question-answering with human feedback |class=cs.CL |eprint=2112.09332 }}</ref><ref>{{Cite web |last=Kumar |first=Nitish |date=2021-12-23 |title=OpenAI Researchers Find Ways To More Accurately Answer Open-Ended Questions Using A Text-Based Web Browser |url=https://www.marktechpost.com/2021/12/22/openai-researchers-find-ways-to-more-accurately-answer-open-ended-questions-using-a-text-based-web-browser/ |accessdate=2022-07-23 |work=MarkTechPost}}</ref><ref>{{Cite journal |last1=Menick |first1=Jacob |last2=Trebacz |first2=Maja |last3=Mikulik |first3=Vladimir |last4=Aslanides |first4=John |last5=Song |first5=Francis |last6=Chadwick |first6=Martin |last7=Glaese |first7=Mia |last8=Young |first8=Susannah |last9=Campbell-Gillingham |first9=Lucy |last10=Irving |first10=Geoffrey |last11=McAleese |first11=Nat |date=2022-03-21 |title=Teaching language models to support answers with verified quotes |url=https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes |journal=DeepMind|arxiv=2203.11147 }}</ref> Researchers from OpenAI and Anthropic have proposed using human feedback and curated datasets to fine-tune AI assistants to avoid negligent falsehoods or express when they are uncertain.<ref name=":53"/><ref>{{cite arXiv |last1=Askell |first1=Amanda |last2=Bai |first2=Yuntao |last3=Chen |first3=Anna |last4=Drain |first4=Dawn |last5=Ganguli |first5=Deep |last6=Henighan |first6=Tom |last7=Jones |first7=Andy |last8=Joseph |first8=Nicholas |last9=Mann |first9=Ben |last10=DasSarma |first10=Nova |last11=Elhage |first11=Nelson |date=2021-12-09 |title=A General Language Assistant as a Laboratory for Alignment |class=cs.CL |eprint=2112.00861 }}</ref><ref name=":202"/> Alongside technical solutions, researchers have argued for defining clear truthfulness standards and the creation of institutions, regulatory bodies, or watchdog agencies to evaluate AI systems on these standards before and during deployment.<ref name=":21" />

Researchers distinguish truthfulness, which specifies that AIs only make statements that are objectively true, and honesty, which is the property that AIs only assert what they believe to be true. Recent research finds that state-of-the-art AI systems cannot be said to hold stable beliefs, so it is not yet tractable to study the honesty of AI systems.<ref>{{Cite web |last1=Kenton |first1=Zachary |last2=Everitt |first2=Tom |last3=Weidinger |first3=Laura |last4=Gabriel |first4=Iason |last5=Mikulik |first5=Vladimir |last6=Irving |first6=Geoffrey |date=2021-03-30 |title=Alignment of Language Agents |url=https://deepmindsafetyresearch.medium.com/alignment-of-language-agents-9fbc7dd52c6c |accessdate=2022-07-23 |work=DeepMind Safety Research - Medium}}</ref> However, there is substantial concern that future AI systems that do hold beliefs could intentionally lie to humans. In extreme cases, a misaligned AI could deceive its operators into thinking it was safe or persuade them that nothing is amiss.<ref name=":75"/><ref name=":625"/><ref name=":010"/> Some argue that if AIs could be made to assert only what they believe to be true, this would sidestep numerous problems in alignment.<ref name=":21" /><ref>{{Cite web |last1=Leike |first1=Jan |last2=Schulman |first2=John |last3=Wu |first3=Jeffrey |date=2022-08-24 |title=Our approach to alignment research |url=https://openai.com/blog/our-approach-to-alignment-research/ |accessdate=2022-09-09 |work=OpenAI}}</ref>

=== Inner alignment and emergent goals ===
Alignment research aims to line up three different descriptions of an AI system:<ref>{{Cite web |last1=Ortega |first1=Pedro A. |last2=Maini |first2=Vishal |last3=DeepMind safety team |date=2018-09-27 |title=Building safe artificial intelligence: specification, robustness, and assurance |url=https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1 |accessdate=2022-08-26 |work=Medium}}</ref>

# ''Intended goals'' ('wishes'): “the hypothetical (but hard to articulate) description of an ideal AI system that is fully aligned to the desires of the human operator”;
# ''Specified goals'' (or ‘outer specification’): The goals we actually specify — typically jointly through an objective function and a dataset;
# ''Emergent goals'' (or ‘inner specification’): The goals the AI actually advances.

‘Outer misalignment’ is a mismatch between the intended goals (1) and the specified goals (2), whereas ‘inner misalignment’ is a mismatch between the human-specified goals (2) and the AI's emergent goals (3).
<!--Evolution analogy-->

Inner misalignment is often explained by analogy to biological evolution.<ref>{{Cite book |last=Christian |first=Brian |url=https://wwnorton.co.uk/books/9780393635829-the-alignment-problem |title=The alignment problem: Machine learning and human values |publisher=W. W. Norton & Company |year=2020 |isbn=978-0-393-86833-3 |location= |chapter=Chapter 5: Shaping |oclc=1233266753}}</ref> In the ancestral environment, evolution selected human genes for inclusive [[Inclusive fitness|genetic fitness]], but humans evolved to have other objectives. Fitness corresponds to (2), the specified goal used in the training environment and training data. In evolutionary history, maximizing the fitness specification led to intelligent agents, humans, that do not directly pursue inclusive genetic fitness. Instead, they pursue emergent goals (3) that correlated with genetic fitness in the ancestral environment: nutrition, sex, and so on. However, our environment has changed — a [[Domain adaptation|distribution shift]] has occurred. Humans still pursue their emergent goals, but this no longer maximizes genetic fitness. (In machine learning the analogous problem is known as ''goal'' ''misgeneralization''.<ref name="goal_misgen">{{Cite conference |last1=Langosco |first1=Lauro Langosco Di |last2=Koch |first2=Jack |last3=Sharkey |first3=Lee D |last4=Pfau |first4=Jacob |last5=Krueger |first5=David |date=2022-07-17 |title=Goal misgeneralization in deep reinforcement learning |conference= |series= |publisher=PMLR |volume=162 |pages=12004–12019 |book-title=International Conference on Machine Learning}}</ref>) Our taste for sugary food (an emergent goal) was originally beneficial, but now leads to overeating and health problems. Also, by using contraception, humans directly contradict genetic fitness. By analogy, if genetic fitness were the objective chosen by an AI developer, they would observe the model behaving as intended in the training environment, without noticing that the model is pursuing an unintended emergent goal until the model was deployed.<!--Research directions and problems-->

Research directions to detect and remove misaligned emergent goals include red teaming, verification, anomaly detection, and interpretability.<ref name=":110"/><ref name=":010"/><ref name=":2323"/> Progress on these techniques may help reduce two open problems. Firstly, emergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments—even for a short time until its misalignment is detected. Such high stakes are common in autonomous driving, health care, and military applications.<ref>{{Cite journal |last1=Zhang |first1=Xiaoge |last2=Chan |first2=Felix T.S. |last3=Yan |first3=Chao |last4=Bose |first4=Indranil |date=2022 |title=Towards risk-aware artificial intelligence and machine learning systems: An overview |url=https://linkinghub.elsevier.com/retrieve/pii/S0167923622000719 |journal=Decision Support Systems |language=en |volume=159 |pages=113800 |doi=10.1016/j.dss.2022.113800|s2cid=248585546 }}</ref> The stakes become higher yet when AI systems gain more autonomy and capability, becoming capable of sidestepping human interventions (see {{Section link||Power-seeking and instrumental goals}}). Secondly, a sufficiently capable AI system may take actions that falsely convince the human supervisor that the AI is pursuing the intended objective (see previous discussion on deception at {{Section link||Scalable oversight}}).

=== Power-seeking and instrumental goals ===
Since the 1950s, AI researchers have sought to build advanced AI systems that can achieve goals by predicting the results of their actions and making long-term plans.<ref>{{Cite journal |last1=McCarthy |first1=John |last2=Minsky |first2=Marvin L. |last3=Rochester |first3=Nathaniel |last4=Shannon |first4=Claude E. |date=2006-12-15 |title=A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955 |url=https://ojs.aaai.org/index.php/aimagazine/article/view/1904 |journal=AI Magazine |language=en |volume=27 |issue=4 |pages=12 |doi=10.1609/aimag.v27i4.1904 |s2cid=19439915 |issn=2371-9621}}</ref> However, some researchers argue that suitably advanced planning systems will default to seeking power over their environment, including over humans — for example by evading shutdown and acquiring resources. This power-seeking behavior is not explicitly programmed but emerges because power is instrumental for achieving a wide range of goals.<ref name=":2522"/><ref name=":210"/><ref name=":75"/> Power-seeking is thus considered a [[Instrumental convergence|''convergent instrumental goal'']].<ref name=":84"/>

Power-seeking is uncommon in current systems, but advanced systems that can foresee the long-term results of their actions may increasingly seek power. This was shown in formal work which found that optimal [[reinforcement learning]] agents will seek power by seeking ways to gain more options, a behavior that persists across a wide range of environments and goals.<ref name=":2522"/>

Power-seeking already emerges in some present systems. [[Reinforcement learning]] systems have gained more options by acquiring and protecting resources, sometimes in ways their designers did not intend.<ref name="quanta-hide-seek">{{Cite web |last=Ornes |first=Stephen |date=2019-11-18 |title=Playing Hide-and-Seek, Machines Invent New Tools |url=https://www.quantamagazine.org/artificial-intelligence-discovers-tool-use-in-hide-and-seek-games-20191118/ |accessdate=2022-08-26 |work=Quanta Magazine}}</ref><ref>{{Cite web |last1=Baker |first1=Bowen |last2=Kanitscheider |first2=Ingmar |last3=Markov |first3=Todor |last4=Wu |first4=Yi |last5=Powell |first5=Glenn |last6=McGrew |first6=Bob |last7=Mordatch |first7=Igor |date=2019-09-17 |title=Emergent Tool Use from Multi-Agent Interaction |url=https://openai.com/blog/emergent-tool-use/ |accessdate=2022-08-26 |work=OpenAI}}</ref> Other systems have learned, in toy environments, that in order to achieve their goal, they can prevent human interference<ref name=":103"/> or disable their off-switch.<ref name=":242"/> [[Stuart J. Russell|Russell]] illustrated this behavior by imagining a robot that is tasked to fetch coffee and evades being turned off since "you can't fetch the coffee if you're dead".<ref name=":210"/>

Hypothesized ways to gain options include AI systems trying to:<blockquote>“''... break out of a contained environment; hack; get access to financial resources, or additional computing resources; make backup copies of themselves; gain unauthorized capabilities, sources of information, or channels of influence; mislead/lie to humans about their goals; resist or manipulate attempts to monitor/understand their behavior ... impersonate humans; cause humans to do things for them; ... manipulate human discourse and politics; weaken various human institutions and response capacities; take control of physical infrastructure like factories or scientific laboratories; cause certain types of technology and infrastructure to be developed; or directly harm/overpower humans.''”<ref name=":75" /></blockquote>Researchers aim to train systems that are 'corrigible': systems that do not seek power and allow themselves to be turned off, modified, etc. An unsolved challenge is ''reward hacking'': when researchers penalize a system for seeking power, the system is incentivized to seek power in difficult-to-detect ways.<ref name=":010"/> To detect such covert behavior, researchers aim to create techniques and tools to inspect AI models<ref name=":010" /> and interpret the inner workings of [[Black box|black-box]] models such as neural networks.

Additionally, researchers propose to solve the problem of systems disabling their off-switches by making AI agents uncertain about the objective they are pursuing.<ref name=":242" /><ref name=":210"/> Agents designed in this way would allow humans to turn them off, since this would indicate that the agent was wrong about the value of whatever action they were taking prior to being shut down. More research is needed to translate this insight into usable systems.<ref name=":224"/>

Power-seeking AI is thought to pose unusual risks. Ordinary safety-critical systems like planes and bridges are not ''adversarial''. They lack the ability and incentive to evade safety measures and appear safer than they are. In contrast, power-seeking AI has been compared to a hacker that evades security measures.<ref name=":75" /> Further, ordinary technologies can be made safe through trial-and-error, unlike power-seeking AI which has been compared to a virus whose release is irreversible since it continuously evolves and grows in numbers—potentially at a faster pace than human society, eventually leading to the disempowerment or extinction of humans.<ref name=":75" /> It is therefore often argued that the alignment problem must be solved early, before advanced power-seeking AI is created.<ref name=":84" />

However, some critics have argued that power-seeking is not inevitable, since humans do not always seek power and may only do so for evolutionary reasons. Furthermore, there is debate whether any future AI systems need to pursue goals and make long-term plans at all.<ref>{{Cite web |last=Shermer |first=Michael |date=2017-03-01 |title=Artificial Intelligence Is Not a Threat&mdash;Yet |url=https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/ |accessdate=2022-08-26 |work=Scientific American}}</ref><ref name=":75" />

=== Embedded agency ===
Work on scalable oversight largely occurs within formalisms such as [[Partially observable Markov decision process|POMDPs]]. Existing formalisms assume that the agent's algorithm is executed outside the environment (i.e. not physically embedded in it). Embedded agency<ref name="lit_review">{{Cite journal |last1=Everitt |first1=Tom |last2=Lea |first2=Gary |last3=Hutter |first3=Marcus |date=21 May 2018 |title=AGI Safety Literature Review |url-status=live |journal=1805.01109 |arxiv=1805.01109}}</ref><ref>{{Cite arXiv |eprint=1902.09469 |class=cs.AI |first1=Abram |last1=Demski |first2=Scott |last2=Garrabrant |title=Embedded Agency |date=6 October 2020}}</ref> is another major strand of research which attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build. For example, even if the scalable oversight problem is solved, an agent which is able to gain access to the computer it is running on may still have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it.<ref name="causal_influence">{{Cite arXiv |eprint=1902.09980 |class=cs.AI |first1=Tom |last1=Everitt |first2=Pedro A. |last2=Ortega |title=Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings |date=6 September 2019 |last3=Barnes |first3=Elizabeth |last4=Legg |first4=Shane}}</ref> A list of examples of specification gaming from [[DeepMind]] researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing.<ref name="DM_specification_gaming">{{Cite web |last1=Krakovna |first1=Victoria |last2=Legg |first2=Shane |title=Specification gaming: the flip side of AI ingenuity |url=https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity |url-status=live |archive-url=https://web.archive.org/web/20210126173242/https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity |archive-date=26 January 2021 |access-date=6 January 2021 |website=Deepmind}}</ref> This class of problems has been formalised using causal incentive diagrams.<ref name="causal_influence" />  Researchers at [[University of Oxford|Oxford]] and [[DeepMind]] have argued that such problematic behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly.<ref name=":3">{{Cite journal |last1=Cohen |first1=Michael K. |last2=Hutter |first2=Marcus |last3=Osborne |first3=Michael A. |date=2022-08-29 |title=Advanced artificial agents intervene in the provision of reward |url=https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064 |journal=AI Magazine |volume=43 |issue=3 |language=en |pages=282–293 |doi=10.1002/aaai.12064 |issn=0738-4602 |s2cid=235489158}}</ref> They suggest a range of potential approaches to address this open problem.

== Skepticism of AI risk ==
{{Main|Existential risk from artificial general intelligence#Skepticism}}
Against the above concerns, AI risk skeptics believe that [[superintelligence]] poses little to no risk of dangerous misbehavior. Such skeptics often believe that controlling a superintelligent AI will be trivial. Some skeptics,<ref>{{Cite news |first=Jane |last=Wakefield |date=27 September 2015 |title=Intelligent Machines: Do we really need to fear AI? |work=BBC News |url=https://www.bbc.com/news/technology-32334568 |url-status=live |access-date=9 February 2021 |archive-url=https://web.archive.org/web/20201108124948/https://www.bbc.com/news/technology-32334568 |archive-date=8 November 2020}}</ref> such as [[Gary Marcus]],<ref>{{Cite news |last1=Marcus |first1=Gary |last2=Davis |first2=Ernest |date=6 September 2019 |title=How to Build Artificial Intelligence We Can Trust |work=The New York Times |url=https://www.nytimes.com/2019/09/06/opinion/ai-explainability.html |url-status=live |access-date=9 February 2021 |archive-url=https://web.archive.org/web/20200922145040/https://www.nytimes.com/2019/09/06/opinion/ai-explainability.html |archive-date=22 September 2020}}</ref> propose adopting rules similar to the fictional [[Three Laws of Robotics]] which directly specify a desired outcome ("direct normativity"). By contrast, most endorsers of the existential risk thesis (as well as many skeptics) consider the Three Laws to be unhelpful, due to those three laws being ambiguous and self-contradictory. (Other "direct normativity" proposals include Kantian ethics, utilitarianism, or a mix of some small list of enumerated desiderata.) Most risk endorsers believe instead that human values (and their quantitative trade-offs) are too complex and poorly-understood to be directly programmed into a superintelligence; instead, a superintelligence would need to be programmed with a ''process'' for acquiring and fully understanding human values ("indirect normativity"), such as [[coherent extrapolated volition]].<ref name="AGIResponses">{{Cite journal |last1=Sotala |first1=Kaj |last2=Yampolskiy |first2=Roman |author-link2=Roman Yampolskiy |date=19 December 2014 |title=Responses to catastrophic AGI risk: a survey |journal=[[Physica Scripta]] |volume=90 |issue=1 |pages=018001 |bibcode=2015PhyS...90a8001S |doi=10.1088/0031-8949/90/1/018001 |doi-access=free}}</ref>

== Public policy ==
{{See also|Regulation of artificial intelligence}}

A number of governmental and treaty organizations have made statements emphasizing the importance of AI alignment.

In September 2021, the [[Secretary-General of the United Nations]] issued a declaration which included a call to regulate AI to ensure it is "aligned with shared global values."<ref>[https://www.un.org/en/content/common-agenda-report/ Secretary-General’s report on “Our Common Agenda”], 2021. Page 63: ''"[T]he Compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values"''</ref>

That same month, the [[People's Republic of China|PRC]] published ethical guidelines for the use of AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety.<ref>PRC Ministry of Science and Technology. Ethical Norms for New Generation Artificial Intelligence Released, 2021. A [https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/ translation] by [[Center for Security and Emerging Technology]]</ref>

Also in September 2021, the [[UK]] published its 10-year National AI Strategy,<ref>{{Cite news |url=https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/|title = UK publishes National Artificial Intelligence Strategy |work=The Register |first=Tim |last=Richardson  |date=22 September 2021}}</ref> which states the British government "takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously".<ref>"''The government takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for the UK and the world, seriously.''" ([https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version The National AI Strategy of the UK], 2021)</ref> The strategy describes actions to assess long term AI risks, including catastrophic risks.<ref>[https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version The National AI Strategy of the UK], 2021 (actions 9 and 10 of the section "Pillar 3 - Governing AI Effectively")</ref>

In March 2021, the US National Security Commission on Artificial Intelligence released stated that "Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should ... ensure that AI systems and their uses align with our goals and values."<ref>{{Cite book |url=https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf |title=NSCAI Final Report |publisher=The National Security Commission on Artificial Intelligence |year=2021 |location=Washington, DC}}</ref>

== See also ==
* [[Existential risk from artificial general intelligence]]
* [[AI takeover]]
* [[AI capability control]]
* [[Regulation of artificial intelligence]]
* [[Artificial wisdom]]
* [[HAL 9000]]
* [[Multivac]]
* [[Open Letter on Artificial Intelligence]]
* [[Toronto Declaration]]
* [[Asilomar Conference on Beneficial AI]]

== Footnotes ==
{{Notelist}}

== References ==
{{Reflist}}

{{Existential risk from artificial intelligence|state=expanded}}

[[Category:Existential risk from artificial general intelligence]]
[[Category:Singularitarianism]]
[[Category:Philosophy of artificial intelligence]]
[[Category:Computational neuroscience]]