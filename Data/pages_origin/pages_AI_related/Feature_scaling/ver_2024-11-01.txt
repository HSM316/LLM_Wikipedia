{{Short description|Method used to normalize the range of independent variables}}
{{Machine learning bar}}

'''Feature scaling''' is a method used to normalize the range of independent variables or features of data. In [[data processing]], it is also known as [[data normalization]] and is generally performed during the [[data preprocessing]] step.

==Motivation==
Since the range of values of raw data varies widely, in some [[machine learning]] algorithms, objective functions will not work properly without [[Normalization (statistics)|normalization]]. For example, many [[Statistical classification|classifiers]] calculate the distance between two points by the [[Euclidean distance]]. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.

Another reason why feature scaling is applied is that [[gradient descent]] converges much faster with feature scaling than without it.<ref>{{cite arXiv|last=Ioffe|first=Sergey|author2=Christian Szegedy|year=2015|title=Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift|eprint=1502.03167|class=cs.LG}}</ref>

It's also important to apply feature scaling if [[Regularization (mathematics)|regularization]] is used as part of the loss function (so that coefficients are penalized appropriately).

Empirically, feature scaling can improve the convergence speed of [[stochastic gradient descent]]. In support vector machines,<ref>{{cite journal |last=Juszczak |first=P. |author2=D. M. J. Tax |author3=R. P. W. Dui |year=2002 |title=Feature scaling in support vector data descriptions |journal=Proc. 8th Annu. Conf. Adv. School Comput. Imaging |pages=25–30 |citeseerx=10.1.1.100.2524}}</ref> it can reduce the time to find support vectors. Feature scaling is also often used in applications involving distances and similarities between data points, such as clustering and similarity search. As an example, the [[K-means clustering]] algorithm is sensitive to feature scales.

==Methods==

===Rescaling (min-max normalization)===
Also known as min-max scaling or min-max normalization, rescaling is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data. The general formula for a min-max of [0, 1] is given as:<ref>{{cite web |title=Min Max normalization |url=https://ml-concepts.com/2021/10/08/min-max-normalization/ |website=ml-concepts.com |access-date=2022-12-14 |archive-date=2023-04-05 |archive-url=https://web.archive.org/web/20230405085506/https://ml-concepts.com/2021/10/08/min-max-normalization/ |url-status=dead }}</ref>

: <math>x' = \frac{x - \text{min}(x)}{\text{max}(x)-\text{min}(x)}</math>

where <math>x</math> is an original value, <math>x'</math> is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights).

To rescale a range between an arbitrary set of values [a, b], the formula becomes:

: <math>x' = a + \frac{(x - \text{min}(x))(b-a)}{\text{max}(x)-\text{min}(x)}</math>

where <math>a,b</math> are the min-max values.

===Mean normalization===

: <math>x' = \frac{x - \bar{x}}{\text{max}(x)-\text{min}(x)}</math>

where <math>x</math> is an original value, <math>x'</math> is the normalized value, <math>\bar{x}=\text{average}(x)</math> is the mean of that feature vector. There is another form of the means normalization which divides by the standard deviation which is also called standardization.

===Standardization (Z-score Normalization)===
{{see also|Standard score}}
[[File:The_effect_of_z-score_normalization_on_k-means_clustering.svg|thumb|277x277px|The effect of z-score normalization on k-means clustering. 4 gaussian clusters of points are generated, then squashed along the ''y''-axis, and a <math>k = 4</math> clustering was computed. Without normalization, the clusters were arranged along the ''x''-axis, since it is the axis with most of variation. After normalization, the clusters are recovered as expected.]]
In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple [[dimensions]]. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., [[support vector machine]]s, [[logistic regression]], and [[artificial neural network]]s).<ref name=":0">{{Cite book|title = Data Science from Scratch|last = Grus|first = Joel|publisher = O'Reilly|year = 2015|isbn = 978-1-491-90142-7|location = Sebastopol, CA|pages = 99, 100}}</ref><ref>{{Cite book |last1=Hastie |first1=Trevor |url=https://books.google.com/books?id=eBSgoAEACAAJ |title=The Elements of Statistical Learning: Data Mining, Inference, and Prediction |last2=Tibshirani |first2=Robert |last3=Friedman |first3=Jerome H. |date=2009 |publisher=Springer |isbn=978-0-387-84884-6 |language=en}}</ref> The general method of calculation is to determine the distribution [[mean]] and [[standard deviation]] for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.

: <math>x' = \frac{x - \bar{x}}{\sigma}</math>

Where <math>x</math> is the original feature vector, <math>\bar{x}=\text{average}(x)</math> is the mean of that feature vector, and <math>\sigma</math> is its standard deviation.

=== Robust Scaling ===
'''Robust scaling''', also known as standardization using [[median]] and [[interquartile range]] (IQR), is designed to be [[Robust statistics|robust]] to [[Outlier|outliers]]. It scales features using the median and IQR as reference points instead of the mean and standard deviation:<math display="block">
x' = \frac{x - Q_2(x)}{Q_3(x) - Q_1(x)}
</math>where <math>Q_1(x), Q_2(x), Q_3(x)</math> are the three quartiles (25th, 50th, 75th percentile) of the feature.

=== Unit vector normalization ===
Unit vector normalization regards each individual data point as a vector, and divide each by its [[Norm (mathematics)|vector norm]], to obtain <math>
x' = x/\|x\|
</math>. Any vector norm can be used, but the most common ones are the [[Lp space|L1 norm and the L2 norm]].

For example, if <math>
x = (v_1, v_2, v_3)
</math>, then its Lp-normalized version is:<math display="block">
\left(\frac{v_1}{(|v_1|^p + |v_2|^p + |v_3|^p)^{1/p}}, 
\frac{v_2}{(|v_1|^p + |v_2|^p + |v_3|^p)^{1/p}}, 
\frac{v_3}{(|v_1|^p + |v_2|^p + |v_3|^p)^{1/p}}\right)
</math>

==See also==
* [[Normalization (machine learning)]]
* [[Normalization (statistics)]]
* [[Standard score]]
* [[fMLLR]], Feature space Maximum Likelihood Linear Regression

==References==
{{reflist}}

== Further reading ==
* {{cite book |first1=Jiawei |last1=Han |first2=Micheline |last2=Kamber |first3=Jian |last3=Pei |title=Data Mining: Concepts and Techniques |publisher=Elsevier |year=2011 |chapter=Data Transformation and Data Discretization |pages=111–118 |isbn=9780123814807 |chapter-url=https://books.google.com/books?id=pQws07tdpjoC&pg=PA111 }}

== External links ==
*[http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning&video=03.1-LinearRegressionII-FeatureScaling&speed=100/ Lecture by Andrew Ng on feature scaling]

[[Category:Machine learning]]
[[Category:Statistical data transformation]]