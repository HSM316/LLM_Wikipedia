{{short description|Line of Nvidia produced servers and workstations}}

'''Nvidia DGX''' is a line of [[Nvidia]] produced servers and workstations which specialize in using [[GPGPU]] to accelerate [[deep learning]] applications.

== DGX-1 ==
DGX-1 servers feature 8 [[GPU]]s based on the [[Pascal (microarchitecture)|Pascal]] or [[Volta (microarchitecture)|Volta]] [[daughter card]]s<ref>{{cite web|title=nvidia dgx-1|url=https://images.nvidia.com/content/technologies/deep-learning/pdf/61681-DB2-Launch-Datasheet-Deep-Learning-Letter-WEB.pdf}}</ref> with [[HBM 2]] memory, connected by an [[NVLink]] [[mesh network]].<ref>{{cite web|title=inside pascal|url=https://devblogs.nvidia.com/parallelforall/inside-pascal/|quote=Eight GPU hybrid cube mesh architecture with NVLink}}</ref>

The product line is intended to bridge the gap between GPUs and [[AI accelerator (computer hardware)|AI accelerator]]s in that the device has specific features specializing it for deep learning workloads.<ref>{{cite web|title=deep learning supercomputer|url=https://www.engadget.com/2016/04/05/nvidia-dgx-1-deep-learning-supercomputer/}}</ref> 
The initial Pascal based DGX-1 delivered 170 [[teraflop]]s of [[FP16|half precision]] processing,<ref>{{cite web|title=DGX-1 deep learning system|url=http://images.nvidia.com/content/technologies/deep-learning/pdf/Datasheet-DGX1.pdf|quote=NVIDIA DGX-1 Delivers 75X Faster Training...Note: Caffe benchmark with AlexNet, training 1.28M images with 90 epochs}}</ref> while the Volta-based upgrade increased this to 960 [[teraflop]]s.<ref>{{cite web|title=DGX Server|url=https://www.nvidia.com/en-us/data-center/dgx-server/|website=DGX Server|publisher=Nvidia|accessdate=7 September 2017}}</ref>

== DGX-2 ==
The successor of the Nvidia DGX-1 is the Nvidia DGX-2, which uses 16 32GB V100 (second generation) cards in a single unit.  This increases performance of up to 2 Petaflops with 512GB of shared memory for tackling larger problems and uses NVSwitch to speed up internal communication.

Additionally, there is a higher performance version of the DGX-2, the DGX-2H with a notable difference being the replacement of the Dual Intel Xeon Platinum  8168's @ 2.7  GHz with Dual Intel Xeon Platinum  8174's @ 3.1  GHz<ref>https://docs.nvidia.com/dgx/pdf/dgx2-user-guide.pdf</ref>

== DGX A100 ==
Announced and released on May 14, 2020 was the 3rd generation of DGX server, including 8 [[Ampere (microarchitecture)|Ampere]]-based A100 accelerators.<ref name=anand-A100>{{cite news|url=https://www.anandtech.com/show/15801/nvidia-announces-ampere-architecture-and-a100-products|title=NVIDIA Ampere Unleashed: NVIDIA Announces New GPU Architecture, A100 GPU, and Accelerator|author=Ryan Smith|date=May 14, 2020|publisher=AnandTech}}</ref> Also included is 15TB of [[PCI express|PCIe]] gen 4 [[NVMe]] storage,<ref name=verge-A100>{{cite news|url=https://www.theverge.com/2020/5/14/21258419/nvidia-ampere-gpu-ai-data-centers-specs-a100-dgx-supercomputer|title=Nvidiaâ€™s first Ampere GPU is designed for data centers and AI, not your PC|author1=Tom Warren|author2=James Vincent|date=May 14, 2020|publisher=The Verge}}</ref> two 64-core AMD [[Epyc#Second_generation_Epyc_(Rome)|Rome]] 7742 CPUs, 1 TB of RAM, and [[Mellanox]]-powered HDR InfiniBand interconnect. The initial price for the DGX A100 was $199,000.<ref name=anand-A100 />

== Accelerators ==
Comparison of accelerators used in DGX:<ref name=anand-A100 />
{{Scrolling table/top |first=Accelerator
|A100
|V100
|P100
}}
{{Scrolling table/mid}}
|-
! Architecture !! FP32 CUDA Cores !! Boost Clock !! Memory Clock !! Memory Bus Width !! Memory Bandwidth !! VRAM !! Single Precision !! Double Precision !! INT8 Tensor !! FP16 Tensor !! FP32 Tensor !! Interconnect !! GPU !! GPU Die Size !! Transistor Count !! TDP !! Manufacturing Process
|-
| Ampere || 6912 || ~1410MHz || 2.4Gbps HBM2 || 5120-bit || 1.6TB/sec || 40GB || 19.5 TFLOPs || 9.7 TFLOPs || 624 TFLOPs || 312 TFLOPs || 156 TFLOPs || 600GB/sec || A100 || 826mm2 || 54.2B || 400W || TSMC 7N
|-
| Volta || 5120 || 1530MHz || 1.75Gbps HBM2 || 4096-bit || 900GB/sec || 16GB/32GB || 15.7 TFLOPs || 7.8 TFLOPs || N/A || 125 TFLOPs || N/A || 300GB/sec || GV100 || 815mm2 || 21.1B || 300W/350W || TSMC 12nm FFN
|-
| Pascal || 3584 || 1480MHz || 1.4Gbps HBM2 || 4096-bit || 720GB/sec || 16GB || 10.6 TFLOPs || 5.3 TFLOPs || N/A || N/A || N/A || 160GB/sec || GP100 || 610mm2 || 15.3B || 300W || TSMC 16nm FinFET
{{Scrolling table/end}}

==See also==
*[[Deep Learning Super Sampling]]

== References ==
{{reflist}}

{{Nvidia}}

[[Category:AI accelerators]]
[[Category:GPGPU]]
[[Category:Nvidia products]]
[[Category:Parallel computing]]

{{computer-stub}}