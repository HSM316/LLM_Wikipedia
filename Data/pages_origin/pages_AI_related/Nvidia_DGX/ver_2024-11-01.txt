{{Short description|Line of Nvidia produced servers and workstations}}
{{Use dmy dates|date=November 2023}}
{{ad|date=January 2024}}
{{Infobox information appliance
| name         = DGX
| title        = DGX
| image        = NetApp ONTAP AI.jpg
| caption      = A rack containing five DGX-1 supercomputers
| manufacturer = [[Nvidia]]
| releasedate  = {{Start date|2016}}
}}
The '''Nvidia DGX''' (Deep GPU Xceleration) represents a series of [[Server (computing)|servers]] and [[Workstation|workstations]] designed by [[Nvidia]], primarily geared towards enhancing [[deep learning]] applications through the use of [[general-purpose computing on graphics processing units]] (GPGPU). These systems typically come in a rackmount format featuring high-performance [[x86]] server CPUs on the motherboard.

The core feature of a DGX system is its inclusion of 4 to 8 [[Nvidia Tesla]] GPU modules, which are housed on an independent system board. These GPUs can be connected either via a version of the [[SXM (socket)|SXM]] socket or a [[PCIe]] x16 slot, facilitating flexible integration within the system architecture. To manage the substantial thermal output, DGX units are equipped with heatsinks and fans designed to maintain optimal operating temperatures.

This framework makes DGX units suitable for computational tasks associated with artificial intelligence and machine learning models.{{says who?|date=July 2024}}

== Models ==
=== Pascal - Volta ===
==== DGX-1 ====
DGX-1 servers feature 8 [[GPU]]s based on the [[Pascal (microarchitecture)|Pascal]] or [[Volta (microarchitecture)|Volta]] [[daughter card]]s<ref>{{Cite web|url=https://images.nvidia.com/content/technologies/deep-learning/pdf/61681-DB2-Launch-Datasheet-Deep-Learning-Letter-WEB.pdf|title=nvidia dgx-1|accessdate=15 November 2023}}</ref> with 128&nbsp;GB of total [[HBM 2|HBM2]] memory, connected by an [[NVLink]] [[mesh network]].<ref>{{cite web|title=inside pascal|date=5 April 2016|url=https://devblogs.nvidia.com/parallelforall/inside-pascal/|quote=Eight GPU hybrid cube mesh architecture with NVLink}}</ref> The DGX-1 was announced on the 6th of April in 2016.<ref>{{Cite web|url=https://www.anandtech.com/show/10229/nvidia-announces-dgx1-server|title = NVIDIA Unveils the DGX-1 HPC Server: 8 Teslas, 3U, Q2 2016}}</ref> All models are based on a dual socket configuration of Intel Xeon E5 CPUs, and are equipped with the following features.

* 512&nbsp;GB of [[DDR4 SDRAM|DDR4-2133]]
* Dual 10&nbsp;Gb networking
* 4 x 1.92&nbsp;TB [[Solid-state drive|SSDs]]
* 3200W of combined power supply capability
* 3U Rackmount Chassis

The product line is intended to bridge the gap between GPUs and [[AI accelerator (computer hardware)|AI accelerator]]s using specific features for deep learning workloads.<ref>{{cite web|title=deep learning supercomputer|date=5 April 2016 |url=https://www.engadget.com/2016/04/05/nvidia-dgx-1-deep-learning-supercomputer/}}</ref> 
The initial Pascal based DGX-1 delivered 170 [[teraflop]]s of [[FP16|half precision]] processing,<ref>{{cite web|title=DGX-1 deep learning system|url=http://images.nvidia.com/content/technologies/deep-learning/pdf/Datasheet-DGX1.pdf|quote=NVIDIA DGX-1 Delivers 75X Faster Training...Note: Caffe benchmark with AlexNet, training 1.28M images with 90 epochs}}</ref> while the Volta-based upgrade increased this to 960 [[teraflop]]s.<ref>{{cite web|title=DGX Server|url=https://www.nvidia.com/en-us/data-center/dgx-server/|website=DGX Server|publisher=Nvidia|access-date=7 September 2017}}</ref>

The DGX-1 was first available in only the Pascal based configuration, with the first generation SXM socket. The later revision of the DGX-1 offered support for first generation Volta cards via the SXM-2 socket. Nvidia offered upgrade kits that allowed users with a Pascal based DGX-1 to upgrade to a Volta based DGX-1.<ref>[https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf Volta architecture whitepaper] nvidia.com</ref><ref>[https://images.nvidia.com/content/technologies/deep-learning/pdf/DGX-1-UserGuide.pdf Use Guide] nvidia.com</ref>

* The [[Pascal (microarchitecture)|Pascal]] base DGX-1 has two variants, one with a 16 core [[Xeon|Intel Xeon]] E5-2698 V3, and one with a 20 core E5-2698 V4. Pricing for the variant equipped with an E5-2698 V4 is unavailable, the Pascal based DGX-1 with an E5-2698 V3 was priced at launch at $129,000<ref name=":0">{{Cite web |last=Oh |first=Nate |title=NVIDIA Ships First Volta-based DGX Systems |url=https://www.anandtech.com/show/11824/nvidia-ships-first-volta-dgx-systems |access-date=2022-03-24 |website=www.anandtech.com}}</ref>
* The [[Volta (microarchitecture)|Volta]] based DGX-1 is equipped with an E5-2698 V4 and was priced at launch at $149,000.<ref name=":0" />

==== DGX Station ====
Designed as a [[turnkey]] deskside AI supercomputer, the DGX Station is a [[Computer tower|tower]] computer that can function completely independently without typical datacenter infrastructure such as cooling, redundant power, or [[19-inch rack|19 inch racks]].

The DGX station was first available with the following specifications.<ref>{{Cite web |title=CompecTA {{!}} NVIDIA DGX Station Deep Learning System |url=https://www.compecta.com/dgxstation.html |access-date=2022-03-24 |website=www.compecta.com}}</ref>

* Four [[Volta (microarchitecture)|Volta]]-based [[Nvidia Tesla|Tesla]] V100 accelerators, each with 16&nbsp;GB of [[High Bandwidth Memory|HBM2]] memory
* 480 TFLOPS FP16
* Single [[Xeon|Intel Xeon]] E5-2698 v4<ref>{{Cite web |title=Intel® Xeon® Processor E5-2698 v4 (50M Cache, 2.20 GHz) - Product Specifications |url=https://www.intel.com/content/www/us/en/products/sku/91753/intel-xeon-processor-e52698-v4-50m-cache-2-20-ghz/specifications.html |access-date=2023-08-19 |website=Intel |language=en}}</ref>
* 256&nbsp;GB DDR4
* 4x 1.92&nbsp;TB SSDs
* Dual 10&nbsp;Gb Ethernet

The DGX station is [[Water cooling|water-cooled]] to better manage the heat of almost 1500W of total system components, this allows it to keep a noise range under 35&nbsp;dB under load.<ref>[https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-station/dgx-station-data-science-supercomputer-datasheet-v4.pdf Supercomputer datasheet] nvidia.com</ref> This, among other features, made this system a compelling purchase for customers without the infrastructure to run [[19-inch rack|rackmount]] DGX systems, which can be loud, output a lot of heat, and take up a large area. This was Nvidia's first venture into bringing [[High-performance computing|high performance computing]] deskside, which has since remained a prominent marketing strategy for Nvidia.<ref name="nvidia.com">{{Cite web|url=https://www.nvidia.com/en-us/data-center/dgx-platform/|title=NVIDIA DGX Platform|website=NVIDIA|access-date=15 November 2023}}</ref>

==== DGX-2 ====
The successor of the Nvidia DGX-1 is the Nvidia DGX-2, which uses sixteen [[Volta (microarchitecture)|Volta]]-based V100 32&nbsp;GB (second generation) cards in a single unit. It was announced on 27 March in 2018.<ref>{{Cite web|url=https://hexus.net/tech/news/systems/116672-nvidia-launches-dgx-2-two-petaflops-power/|title = Nvidia launches the DGX-2 with two petaFLOPS of power|date = 28 March 2018}}</ref> The DGX-2 delivers 2 Petaflops with 512&nbsp;GB of shared memory for tackling massive datasets and uses [[NVLink|NVSwitch]] for high-bandwidth internal communication. DGX-2 has a total of 512&nbsp;GB of [[High Bandwidth Memory|HBM2]] memory, a total of 1.5&nbsp;TB of [[DDR4 SDRAM|DDR4]]. Also present are eight 100&nbsp;Gb/sec [[InfiniBand]] cards and 30.72&nbsp;TB of SSD storage,<ref>{{Cite web |title=NVIDIA DGX -2 for Complex AI Challenges |url=https://www.nvidia.com/en-us/data-center/dgx-2/ |access-date=2022-03-24 |website=NVIDIA |language=en-us}}</ref> all enclosed within a massive 10U rackmount chassis and drawing up to 10&nbsp;kW under maximum load.<ref name=":1">{{Cite web |last=Cutress |first=Ian |title=NVIDIA's DGX-2: Sixteen Tesla V100s, 30 TB of NVMe, only $400K |url=https://www.anandtech.com/show/12587/nvidias-dgx2-sixteen-v100-gpus-30-tb-of-nvme-only-400k |access-date=2022-04-28 |website=www.anandtech.com}}</ref> The initial price for the DGX-2 was $399,000.<ref>{{Cite web |title=The NVIDIA DGX-2 is the world's first 2-petaflop single server supercomputer |url=https://www.hardwarezone.com.sg/tech-news-nvidia-dgx-2-world-s-first-2-petaflop-single-server-supercomputer |access-date=2022-03-24 |website=www.hardwarezone.com.sg |language=en}}</ref>

The DGX-2 differs from other DGX models in that it contains two separate GPU daughterboards, each with eight GPUs. These boards are connected by an NVSwitch system that allows for full bandwidth communication across all GPUs in the system, without additional latency between boards.<ref name=":1" />

A higher performance variant of the DGX-2, the DGX-2H, was offered as well. The DGX-2H replaced the DGX-2's dual Intel Xeon Platinum 8168's with upgraded dual Intel Xeon Platinum 8174's. This upgrade does not increase core count per system, as both CPUs are 24 cores, nor does it enable any new functions of the system, but it does increase the base frequency of the CPUs from 2.7&nbsp;GHz to 3.1&nbsp;GHz.<ref>[https://docs.nvidia.com/dgx/pdf/dgx2-user-guide.pdf DGX2 User Guide] nvidia.com</ref><ref>{{Cite web |title=Product Specifications |url=https://www.intel.com/content/www/us/en/products/sku/120504/intel-xeon-platinum-8168-processor-33m-cache-2-70-ghz.html |access-date=2022-04-28 |website=www.intel.com}}</ref><ref>{{Cite web |title=Product Specifications |url=https://www.intel.com/content/www/us/en/products/sku/136874/intel-xeon-platinum-8174-processor-33m-cache-3-10-ghz.html |access-date=2022-04-28 |website=www.intel.com}}</ref>

=== Ampere ===
==== DGX A100 Server ====
Announced and released on May 14, 2020. The DGX A100 was the 3rd generation of DGX server, including 8 [[Ampere (microarchitecture)|Ampere]]-based A100 accelerators.<ref name=anand-A100>{{cite news|url=https://www.anandtech.com/show/15801/nvidia-announces-ampere-architecture-and-a100-products|title=NVIDIA Ampere Unleashed: NVIDIA Announces New GPU Architecture, A100 GPU, and Accelerator|author=Ryan Smith|date=May 14, 2020|publisher=AnandTech}}</ref> Also included is 15&nbsp;TB of [[PCI express|PCIe]] gen 4 [[NVMe]] storage,<ref name=verge-A100>{{cite news|url=https://www.theverge.com/2020/5/14/21258419/nvidia-ampere-gpu-ai-data-centers-specs-a100-dgx-supercomputer|title=Nvidia's first Ampere GPU is designed for data centers and AI, not your PC|author1=Tom Warren|author2=James Vincent|date=May 14, 2020|publisher=The Verge}}</ref> 1&nbsp;TB of RAM, and eight [[Mellanox]]-powered 200&nbsp;GB/s HDR [[InfiniBand]] ConnectX-6 [[Network interface controller|NICs]]. The DGX A100 is in a much smaller enclosure than its predecessor, the DGX-2, taking up only 6 Rack units.<ref>{{Cite web |title=Boston Labs welcomes the DGX A100 to our remote testing portfolio! |url=https://www.boston.co.uk/blog/2020/09/15/boston-labs-welcomes-the-dgx-a100.aspx |access-date=2022-03-24 |website=www.boston.co.uk}}</ref>

The DGX A100 also moved to a 64 core [[Epyc|AMD EPYC]] 7742 CPU, the first DGX server to not be built with an [[Xeon|Intel Xeon]] CPU. The initial price for the DGX A100 Server was $199,000.<ref name="anand-A100" />

==== DGX Station A100 ====
As the successor to the original DGX Station, the DGX Station A100, aims to fill the same niche as the DGX station in being a quiet, efficient, turnkey [[Computer cluster|cluster-in-a-box]] solution that can be purchased, leased, or rented by smaller companies or individuals who want to utilize machine learning. It follows many of the design choices of the original DGX station, such as the tower orientation, single socket CPU [[Motherboard|mainboard]], a new refrigerant-based cooling system, and a reduced number of accelerators compared to the corresponding rackmount DGX A100 of the same generation.<ref name="nvidia.com"/> The price for the DGX Station A100 320G is $149,000 and $99,000 for the 160G model, Nvidia also offers Station rental at ~$9000 USD per month through partners in the US (rentacomputer.com) and Europe (iRent IT Systems) to help reduce the costs of implementing these systems at a small scale.<ref>{{Cite web |author1=Mayank Sharma |date=2021-04-13 |title=Nvidia will let you rent its mini supercomputers |url=https://www.techradar.com/news/nvidia-will-let-you-rent-its-mini-supercomputers |access-date=2022-03-31 |website=TechRadar |language=en}}</ref><ref>{{Cite web |author1=Jarred Walton |date=2021-04-12 |title=Nvidia Refreshes Expensive, Powerful DGX Station 320G and DGX Superpod |url=https://www.tomshardware.com/news/nvidia-dgx-station-320g |access-date=2022-04-28 |website=Tom's Hardware |language=en}}</ref>

The DGX Station A100 comes with two different configurations of the built in A100.

* Four [[Ampere (microarchitecture)|Ampere]]-based A100 accelerators, configured with 40&nbsp;GB (HBM) or 80&nbsp;GB (HBM2e) memory,<br />thus giving a total of 160&nbsp;GB or 320&nbsp;GB resulting either in DGX Station A100 variants 160G or 320G.
* 2.5 PFLOPS FP16
* Single 64 Core [[Epyc|AMD EPYC]] 7742
* 512&nbsp;GB [[DDR4 SDRAM|DDR4]]
* 1 x 1.92&nbsp;TB [[NVM Express|NVMe]] OS drive
* 1 x 7.68&nbsp;TB U.2 NVMe Drive
* Dual port 10&nbsp;Gb Ethernet
* Single port 1&nbsp;Gb BMC port

=== Hopper ===
==== DGX H100 Server ====
Announced March 22, 2022<ref>{{Cite web |last=Newsroom |first=NVIDIA |title=NVIDIA Announces DGX H100 Systems – World's Most Advanced Enterprise AI Infrastructure |url=http://nvidianews.nvidia.com/news/nvidia-announces-dgx-h100-systems-worlds-most-advanced-enterprise-ai-infrastructure |access-date=2022-03-24 |website=NVIDIA Newsroom Newsroom |language=en-us}}</ref> and planned for release in Q3 2022,<ref>{{Cite web |last=Albert |date=2022-03-24 |title=NVIDIA H100: Overview, Specs, & Release Date {{!}} SeiMaxim |url=https://www.seimaxim.com/kb/gpu/nvidia-h100-overview-release-date |access-date=2022-08-22 |website=www.seimaxim.com |language=en-US}}</ref> The DGX H100 is the 4th generation of DGX servers, built with 8 [[Hopper (microarchitecture)|Hopper]]-based H100 accelerators, for a total of 32 PFLOPs of FP8 AI compute and 640&nbsp;GB of HBM3 Memory, an upgrade over the DGX A100s 640GB HBM2 memory. This upgrade also increases [[Video random access memory|VRAM]] bandwidth to 3&nbsp;TB/s.<ref>{{Cite web |last=Walton |first=Jarred |date=2022-03-22 |title=Nvidia Reveals Hopper H100 GPU With 80 Billion Transistors |url=https://www.tomshardware.com/news/nvidia-hopper-h100-gpu-revealed-gtc-2022 |access-date=2022-03-24 |website=Tom's Hardware |language=en}}</ref> The DGX H100 increases the [[19-inch rack|rackmount]] size to 8U to accommodate the 700W TDP of each H100 SXM card. The DGX H100 also has two 1.92&nbsp;TB SSDs for [[Operating system|Operating System]] storage, and 30.72&nbsp;TB of [[Solid-state drive|Solid state storage]] for application data.

One more notable addition is the presence of two Nvidia [[Nvidia BlueField|Bluefield]] 3 [[Data processing unit|DPUs]],<ref>{{Cite web |last=Newsroom |first=NVIDIA |title=NVIDIA Announces DGX H100 Systems – World's Most Advanced Enterprise AI Infrastructure |url=http://nvidianews.nvidia.com/news/nvidia-announces-dgx-h100-systems-worlds-most-advanced-enterprise-ai-infrastructure |access-date=2022-04-19 |website=NVIDIA Newsroom Newsroom |language=en-us}}</ref> and the upgrade to 400&nbsp;Gb/s [[InfiniBand]] via [[Mellanox Technologies|Mellanox]] ConnectX-7 [[Network interface controller|NICs]], double the bandwidth of the DGX A100. The DGX H100 uses new 'Cedar Fever' cards, each with four ConnectX-7 400&nbsp;GB/s controllers, and two cards per system. This gives the DGX H100 3.2&nbsp;Tb/s of fabric bandwidth across Infiniband.<ref>{{Cite web |last=servethehome |date=2022-04-14 |title=NVIDIA Cedar Fever 1.6Tbps Modules Used in the DGX H100 |url=https://www.servethehome.com/nvidia-cedar-fever-1-6tbps-modules-used-in-the-dgx-h100/ |access-date=2022-04-19 |website=ServeTheHome |language=en-US}}</ref>

The DGX H100 has two [[Xeon]] Platinum 8480C Scalable CPUs (Codenamed [[Sapphire Rapids (microprocessor)|Sapphire Rapids]])<ref>{{Cite web |title=NVIDIA DGX H100 Datasheet |url=https://resources.nvidia.com/en-us-dgx-systems/ai-enterprise-dgx |access-date=2023-08-02 |website=www.nvidia.com |language=en}}</ref> and 2 Terabytes of [[Random-access memory|System Memory]].<ref>{{Cite web |title=NVIDIA DGX H100 |url=https://www.nvidia.com/en-us/data-center/dgx-h100/ |access-date=2022-03-24 |website=NVIDIA |language=en-us}}</ref>

The DGX H100 was priced at £379,000 or ~$482,000 USD at release.<ref>{{Citation |title=Every NVIDIA DGX benchmarked & power efficiency & value compared, including the latest DGX H100. |url=https://www.youtube.com/watch?v=ktNZLLZnjbk |language=en |access-date=2023-03-01}}</ref>

==== DGX GH200 ====
Announced May 2023, the DGX GH200 connects 32 Nvidia Hopper Superchips into a singular superchip, that consists totally of 256 H100 GPUs, 32 Grace Neoverse V2 72-core CPUs, 32 OSFT single-port ConnectX-7 VPI of with 400&nbsp;Gb/s InfiniBand and 16 dual-port [[Nvidia BlueField|BlueField-3]] VPI with 200&nbsp;Gb/s of [[Mellanox Technologies|Mellanox]] [https://resources.nvidia.com/en-us-dgx-gh200/nvidia-dgx-gh200-datasheet-web-us] [https://resources.nvidia.com/en-us-dgx-gh200/nvidia-grace-hopper-superchip-datasheet] . Nvidia DGX GH200 is designed to handle terabyte-class models for massive recommender systems, generative AI, and graph analytics, offering 19.5&nbsp;TB of shared memory with linear scalability for giant AI models.<ref>{{Cite web |title=NVIDIA DGX GH200 |url=https://www.nvidia.com/en-us/data-center/dgx-gh200/ |access-date=2022-03-24 |website=NVIDIA|language=en-us}}</ref>

==== DGX Helios ====
Announced May 2023, the DGX Helios supercomputer features 4 DGX GH200 systems. Each is interconnected with Nvidia Quantum-2 [[InfiniBand]] networking to supercharge data throughput for training large AI models. Helios includes 1,024 H100 GPUs.

=== Blackwell ===
==== DGX GB200 ====
Announced March 2024,&nbsp;GB200 NVL72 connects 36 Grace Neoverse V2 72-core CPUs and 72 B100 GPUs in a rack-scale design. The GB200 NVL72 is a liquid-cooled, rack-scale solution that boasts a 72-GPU NVLink domain that acts as a single massive GPU [https://www.nvidia.com/en-eu/data-center/gb200-nvl72/]. Nvidia DGX GB200 offers 13.5&nbsp;TB HBM3e of shared memory with linear scalability for giant AI models, less than its predecessor DGX GH200.

==== DGX SuperPod ====
The DGX Superpod is a high performance turnkey [[supercomputer]] solution provided by Nvidia using DGX hardware.<ref>{{Cite web|url=https://resources.nvidia.com/en-us-dgx-systems/dgx-ai-4|title=NVIDIA SuperPOD Datasheet|website=NVIDIA|accessdate=15 November 2023}}</ref> This system combines DGX compute nodes with fast storage and high bandwidth [[Computer network|networking]] to provide a solution to high demand machine learning workloads. The [[Selene (supercomputer)|Selene Supercomputer]], at the [[Argonne National Laboratory]], is one example of a DGX SuperPod based system.

Selene, built from 280 DGX A100 nodes, ranked 5th on the [[TOP500|Top500]] list for most powerful supercomputers at the time of its completion, and has continued to remain high in performance. This same integration is available to any customer with minimal effort on their behalf, and the new [[Hopper (microarchitecture)|Hopper]] based SuperPod can scale to 32 DGX H100 nodes, for a total of 256 H100 GPUs and 64 [[x86]] CPUs. This gives the complete SuperPod 20&nbsp;TB of HBM3 memory, 70.4&nbsp;TB/s of bisection bandwidth, and up to 1 [[FLOPS|ExaFLOP]] of [[Floating-point arithmetic|FP8]] AI compute.<ref>{{Cite web |author1=Jarred Walton |date=2022-03-22 |title=Nvidia Reveals Hopper H100 GPU With 80 Billion Transistors |url=https://www.tomshardware.com/news/nvidia-hopper-h100-gpu-revealed-gtc-2022 |access-date=2022-03-24 |website=Tom's Hardware |language=en}}</ref> These SuperPods can then be further joined to create larger supercomputers.

[[Eos (supercomputer)|Eos supercomputer]], designed, built, and operated by Nvidia,<ref>{{Cite web |last=Vincent |first=James |date=2022-03-22 |title=Nvidia reveals H100 GPU for AI and teases 'world's fastest AI supercomputer' |url=https://www.theverge.com/2022/3/22/22989182/nvidia-ai-hopper-architecture-h100-gpu-eos-supercomputer |access-date=2022-05-16 |website=[[The Verge]] |language=en}}</ref><ref>{{Cite web |last=Mellor |first=Chris |date=2022-03-31 |title=Nvidia Eos AI supercomputer will need a monster storage system |url=https://blocksandfiles.com/2022/03/31/nvidia-eos-ai-supercomputer-storage/ |access-date=2022-05-21 |website=Blocks and Files |language=en-GB}}</ref><ref>{{Cite web |last=Comment |first=Sebastian Moss |title=Nvidia announces Eos, "world's fastest AI supercomputer" |url=https://www.datacenterdynamics.com/en/news/nvidia-announces-eos-supercomputer-worlds-fastest-ai-supercomputer/ |access-date=2022-05-21 |website=Data Center Dynamics |language=en}}</ref> was constructed of 18 H100 based SuperPods, totaling 576 DGX H100 systems, 500 Quantum-2 [[InfiniBand]] switches, and 360 [[NVLink]] Switches, that allow Eos to deliver 18 EFLOPs of FP8 compute, and 9 EFLOPs of FP16 compute, making Eos the 5th fastest AI supercomputer in the world, according to TOP500 (November 2023 edition).

As Nvidia does not produce any storage devices or systems, Nvidia SuperPods rely on partners to provide high performance storage. Current storage partners for Nvidia Superpods are [[Dell EMC]], [[DataDirect_Networks|DDN]], [[Hewlett Packard Enterprise|HPE]], [[IBM]], [[NetApp]], Pavilion Data, and [[VAST Data]].<ref>{{Cite web |last=Mellor |first=Chris |date=2022-03-31 |title=Nvidia Eos AI supercomputer will need a monster storage system |url=https://blocksandfiles.com/2022/03/31/nvidia-eos-ai-supercomputer-storage/ |access-date=2022-04-29 |website=Blocks and Files |language=en-GB}}</ref>

== Accelerators ==
{{NvidiaDgxAccelerators}}

==See also==
*[[Deep Learning Super Sampling]]
*[[Nvidia Tesla]]
*[[Supercomputer]]
*[https://hpc.fau.de/systems-services/systems-documentation-instructions/clusters/alex-cluster/#a100 Page on high performance computing with 4x and 8x A100 per computer node], also showing switch topology dumps

== References ==
{{reflist}}

{{Nvidia}}

[[Category:AI accelerators]]
[[Category:GPGPU]]
[[Category:Nvidia products]]
[[Category:Parallel computing]]