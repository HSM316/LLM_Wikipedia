{{short description|Deep learning artificial intelligence research team at Google}}
{{Infobox project
| name = Google Brain
| logo = 
| website = {{URL|https://ai.google/brain-team/}}
| commercial = Yes
| type = [[Artificial intelligence]] and [[machine learning]]
| location = [[Mountain View, California|Mountain View]], California
}}
'''Google Brain''' is a [[deep learning]] [[artificial intelligence]] research team under the umbrella of [[Google AI]], a research division at Google dedicated to artificial intelligence. Formed in 2011, Google Brain combines open-ended machine learning research with information systems and large-scale computing resources.<ref>{{Cite web|date=2020-02-06|title=What is Google Brain?|url=https://www.geeksforgeeks.org/what-is-google-brain/|access-date=2021-04-09|website=GeeksforGeeks|language=en-us}}</ref> The team has created tools such as [[TensorFlow]], which allow for neural networks to be used by the public, with multiple internal AI research projects.<ref name=":2" /> The team aims to create research opportunities in [[machine learning]] and [[natural language processing]].<ref name=":2" />

==History==
The Google Brain project began in 2011 as a part-time research collaboration between Google fellow [[Jeff Dean (computer scientist)|Jeff Dean]], Google Researcher Greg Corrado, and [[Stanford University]] professor [[Andrew Ng]].<ref name=nyt-cats /> Ng had been interested in using [[deep learning]] techniques to crack the problem of [[artificial intelligence]] since 2006, and in 2011 began collaborating with Dean and Corrado to build a large-scale deep learning software system, [[DistBelief]],<ref>{{cite web|author1=Jeffrey Dean|title=Large Scale Distributed Deep Networks|url=http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf|date=December 2012|access-date=25 October 2015|display-authors=etal}}</ref> on top of Google's cloud computing infrastructure. Google Brain started as a [[Google X]] project and became so successful that it was graduated back to Google: [[Astro Teller]] has said that Google Brain paid for the entire cost of [[X Development|Google X]].<ref>{{cite web|author1=Conor Dougherty|title=Astro Teller, Google's 'Captain of Moonshots,' on Making Profits at Google X|url=http://bits.blogs.nytimes.com/2015/02/16/googles-captain-of-moonshots-on-making-profits-at-google-x/|date = 16 February 2015|access-date=25 October 2015}}</ref>

In June 2012, the ''[[New York Times]]'' reported that a cluster of 16,000 [[Central processing unit|processors]] in 1,000 [[Computer|computers]] dedicated to mimicking some aspects of [[Brain activity|human brain activity]] had successfully trained itself to recognize a [[cat]] based on 10 million digital images taken from [[YouTube]] videos.<ref name=nyt-cats>{{cite news|url=https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html?pagewanted=all|title = How Many Computers to Identify a Cat? 16,000|last = Markoff|first = John|author-link = John Markoff|date = June 25, 2012|access-date = February 11, 2014|newspaper = The New York Times}}</ref> The story was also covered by [[National Public Radio]].<ref name=npr-cats>{{cite web|url=https://www.npr.org/2012/06/26/155792609/a-massive-google-network-learns-to-identify|title = A Massive Google Network Learns To Identify — Cats|publisher = [[National Public Radio]]|date = June 26, 2012|access-date = February 11, 2014}}</ref>

In March 2013, Google hired [[Geoffrey Hinton]], a leading researcher in the [[deep learning]] field, and acquired the company DNNResearch Inc. headed by [[Geoffrey Hinton|Hinton]]. [[Geoffrey Hinton|Hinton]] said that he would be dividing his future time between his university research and his work at Google.<ref>{{cite press release|title=U of T neural networks start-up acquired by Google|date=12 March 2013|location=Toronto, ON|url=http://media.utoronto.ca/media-releases/u-of-t-neural-networks-start-up-acquired-by-google/|access-date=13 March 2013}}</ref>

==Team and location==
Google Brain was initially established by Google Fellow [[Jeff Dean (computer scientist)|Jeff Dean]] and visiting Stanford professor [[Andrew Ng]]. In 2014, the team included Jeff Dean, Quoc Le, [[Ilya Sutskever]], [[Alex Krizhevsky]], [[Samy Bengio]] and Vincent Vanhoucke. In 2017, team members include Anelia Angelova, Samy Bengio, Greg Corrado, George Dahl, Michael Isard, Anjuli Kannan, Hugo Larochelle, Chris Olah, Salih Edneer, Benoit Steiner, Vincent Vanhoucke, Vijay Vasudevan and [[Fernanda Viegas]].<ref name=":16" /> [[Chris Lattner]], who created [[Apple Inc.|Apple]]'s programming language [[Swift (programming language)|Swift]] and then ran [[Tesla, Inc.|Tesla]]'s autonomy team for six months joined Google Brain's team in August 2017.<ref>{{cite news|last1=Etherington|first1=Darrell|date=Aug 14, 2017|title=Swift creator Chris Lattner joins Google Brain after Tesla Autopilot stint|work=TechCrunch|url=https://techcrunch.com/2017/08/14/swift-creator-chris-lattner-joins-google-brain-after-tesla-autopilot-stint/|access-date=11 October 2017}}</ref> Lattner left the team in January 2020 and joined [[SiFive]].<ref>{{Cite web|date=2020-01-27|title=Former Google and Tesla Engineer Chris Lattner to Lead SiFive Platform Engineering Team|url=https://www.businesswire.com/news/home/20200127005141/en/Former-Google-and-Tesla-Engineer-Chris-Lattner-to-Lead-SiFive-Platform-Engineering-Team|access-date=2021-04-09|website=www.businesswire.com|language=en}}</ref>

In 2021, Google Brain is led by [[Jeff Dean]], [[Geoffrey Hinton]] and [[Zoubin Ghahramani]]. Other members include Katherine Heller, Pi-Chuan Chang, Ian Simon, Jean-Philippe Vert, Nevena Lazic, Anelia Angelova, Lukasz Kaiser, Carrie Jun Cai, Eric Breck, Ruoming Pang, Carlos Riquelme, Hugo Larochelle, David Ha.<ref name=":16">{{Cite web|title=Brain Team – Google Research|url=https://research.google/teams/brain/|access-date=2021-04-08|website=Google Research|language=en}}</ref> [[Samy Bengio]] left the team in April 2021<ref name=":11">{{Cite news|last=Dave|first=Jeffrey Dastin, Paresh|date=2021-04-07|title=Google AI scientist Bengio resigns after colleagues' firings: email|language=en|work=Reuters|url=https://www.reuters.com/article/us-alphabet-google-research-bengio-idUSKBN2BT2JT|access-date=2021-04-08}}</ref> with [[Zoubin Ghahramani]] taking on his responsibilities.

Google Research includes Google Brain and is based in [[Mountain View, California]]. It also has satellite groups in [[Accra]], [[Amsterdam]], [[Atlanta]],  [[Beijing]], [[Berlin]], [[Cambridge, Massachusetts|Cambridge (Massachusetts)]], [[Israel]], [[Los Angeles]], [[London]], [[Montreal]], [[Munich]], [[New York City]], [[Paris]], [[Pittsburgh]], [[Princeton, New Jersey|Princeton]], [[San Francisco]], [[Seattle]], [[Tokyo]], [[Toronto]], and [[Zurich]].<ref>{{Cite web|title=Build for Everyone - Google Careers|url=https://careers.google.com/|access-date=2021-04-08|website=careers.google.com|language=en}}</ref>

==Projects==
===Artificial-intelligence-devised encryption system===
In October 2016, Google Brain designed an experiment to determine that [[Neural network|neural networks]] are capable of learning secure [[Symmetric-key algorithm|symmetric encryption]].<ref name=":15">{{Cite journal|last1=Zhu|first1=Y.|last2=Vargas|first2=D. V.|last3=Sakurai|first3=K.|date=November 2018|title=Neural Cryptography Based on the Topology Evolving Neural Networks|url=https://ieeexplore.ieee.org/document/8590945|journal=2018 Sixth International Symposium on Computing and Networking Workshops (CANDARW)|pages=472–478|doi=10.1109/CANDARW.2018.00091|isbn=978-1-5386-9184-7|s2cid=57192497}}</ref> In this experiment, three [[Neural network|neural networks]] were created: Alice, Bob and Eve.<ref name=":13">{{cite journal|last1=Abadi|first1=Martín|last2=Andersen|first2=David G.|date=2016|title=Learning to Protect Communications with Adversarial Neural Cryptography|arxiv=1610.06918|bibcode=2016arXiv161006918A}}</ref> Adhering to the idea of a [[generative adversarial network]] (GAN), the goal of the experiment was for Alice to send an encrypted message to Bob that Bob could decrypt, but the adversary, Eve, could not.<ref name=":13" /> Alice and Bob maintained an advantage over Eve, in that they shared a [[Key (cryptography)|key]] used for [[encryption]] and [[decryption]].<ref name=":15" /> In doing so, Google Brain demonstrated the capability of [[Neural network|neural networks]] to learn secure [[encryption]].<ref name=":15" />

===Image enhancement===
In February 2017, Google Brain determined a [[probabilistic method]] for converting pictures with 8x8 [[Image resolution|resolution]] to a resolution of 32x32.<ref>{{cite journal|last1=Dahl|first1=Ryan|last2=Norouzi|first2=Mohammad|last3=Shlens|first3=Jonathon|date=2017|title=Pixel Recursive Super Resolution|arxiv=1702.00783|bibcode=2017arXiv170200783D}}</ref><ref name=":14">{{cite web|date=2017-02-07|title=Google Brain super-resolution image tech makes "zoom, enhance!" real|url=https://arstechnica.com/information-technology/2017/02/google-brain-super-resolution-zoom-enhance/|access-date=2017-05-15|website=arstechnica.co.uk}}</ref> The method built upon an already existing probabilistic model called pixelCNN to generate pixel translations.<ref>{{Citation|last1=Bulat|first1=Adrian|title=To Learn Image Super-Resolution, Use a GAN to Learn How to Do Image Degradation First|date=2018|url=http://dx.doi.org/10.1007/978-3-030-01231-1_12|work=Computer Vision – ECCV 2018|pages=187–202|place=Cham|publisher=Springer International Publishing|isbn=978-3-030-01230-4|access-date=2021-04-09|last2=Yang|first2=Jing|last3=Tzimiropoulos|first3=Georgios|doi=10.1007/978-3-030-01231-1_12|arxiv=1807.11458|s2cid=51882734}}</ref><ref>{{Cite journal|last1=Oord|first1=Aaron Van|last2=Kalchbrenner|first2=Nal|last3=Kavukcuoglu|first3=Koray|date=2016-06-11|title=Pixel Recurrent Neural Networks|url=http://proceedings.mlr.press/v48/oord16.html|journal=International Conference on Machine Learning|language=en|publisher=PMLR|pages=1747–1756|arxiv=1601.06759}}</ref>

The proposed software utilizes two [[Neural network|neural networks]] to make approximations for the [[pixel]] makeup of translated images.<ref name=":14" /><ref>{{cite web|title=Google uses AI to sharpen low-res images|url=https://www.engadget.com/2017/02/07/google-ai-image-enhancement/|access-date=2017-05-15|website=engadget.com}}</ref> The first network, known as the “conditioning network,” downsizes [[Image resolution|high-resolution]] images to 8x8 and attempts to create mappings from the original 8x8 image to these higher-resolution ones.<ref name=":14" /> The other network, known as the “prior network,” uses the mappings from the previous network to add more detail to the original image.<ref name=":14" /> The resulting translated image is not the same image in higher resolution, but rather a 32x32 resolution estimation based on other existing high-resolution images.<ref name=":14" /> Google Brain's results indicate the possibility for neural networks to enhance images.<ref>{{cite web|title=Google just made 'zoom and enhance' a reality -- kinda|url=https://www.cnet.com/news/google-just-made-zoom-and-enhance-a-reality/|access-date=2017-05-15|website=cnet.com}}</ref>

=== Google Translate ===
The Google Brain team contributed to the [[Google Translate]] project by employing a new deep learning system that combines artificial neural networks with vast databases of [[Multilingualism|multilingual]] texts.<ref name=":0">{{Cite journal|last=Castelvecchi|first=Davide|title=Deep learning boosts Google Translate tool|url=http://www.nature.com/news/deep-learning-boosts-google-translate-tool-1.20696|journal=Nature News|year=2016|language=en|doi=10.1038/nature.2016.20696|s2cid=64308242}}</ref> In September 2016, [[Google Neural Machine Translation]] ([[Google Neural Machine Translation|GNMT]]) was launched, an end-to-end learning framework, able to learn from a large number of examples.<ref name=":0" /> Previously, Google Translate's Phrase-Based Machine Translation (PBMT) approach would statistically analyze word by word and try to match corresponding words in other languages without considering the surrounding phrases in the sentence.<ref name=":1">{{Cite news|last=Lewis-Kraus|first=Gideon|date=2016-12-14|title=The Great A.I. Awakening|language=en-US|work=The New York Times|url=https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html|access-date=2021-04-08|issn=0362-4331}}</ref> But rather than choosing a replacement for each individual word in the desired language, [[Google Neural Machine Translation|GNMT]] evaluates word segments in the context of the rest of the sentence to choose more accurate replacements.<ref name=":2">{{Cite journal|last1=Helms|first1=Mallory|last2=Ault|first2=Shaun V.|last3=Mao|first3=Guifen|last4=Wang|first4=Jin|date=2018-03-09|title=An Overview of Google Brain and Its Applications|url=https://doi.org/10.1145/3206157.3206175|journal=Proceedings of the 2018 International Conference on Big Data and Education|series=ICBDE '18|location=Honolulu, HI, USA|publisher=Association for Computing Machinery|pages=72–75|doi=10.1145/3206157.3206175|isbn=978-1-4503-6358-7|s2cid=44107806}}</ref> Compared to older PBMT models, the [[Google Neural Machine Translation|GNMT]] model scored a 24% improvement in similarity to human translation, with a 60% reduction in errors.<ref name=":2" /><ref name=":0" /> The GNMT has also shown significant improvement for notoriously difficult translations, like [[Chinese language|Chinese]] to [[English language|English]].<ref name=":0" />

While the introduction of the GNMT has increased the quality of Google Translate's translations for the pilot languages, it was very difficult to create such improvements for all of its 103 languages. Addressing this problem, the Google Brain Team was able to develop a [[Multilingualism|Multilingual]] [[Google Neural Machine Translation|GNMT]] system, which extended the previous one by enabling translations between multiple languages. Furthermore, it allows for Zero-Shot Translations, which are translations between two languages that the system has never explicitly seen before.<ref>{{Cite journal|last1=Johnson|first1=Melvin|last2=Schuster|first2=Mike|last3=Le|first3=Quoc V.|last4=Krikun|first4=Maxim|last5=Wu|first5=Yonghui|last6=Chen|first6=Zhifeng|last7=Thorat|first7=Nikhil|last8=Viégas|first8=Fernanda|last9=Wattenberg|first9=Martin|last10=Corrado|first10=Greg|last11=Hughes|first11=Macduff|date=2017-10-01|title=Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation|journal=Transactions of the Association for Computational Linguistics|volume=5|pages=339–351|doi=10.1162/tacl_a_00065|issn=2307-387X|doi-access=free}}</ref> Google announced that Google Translate can now also translate without transcribing, using neural networks. This means that it is possible to translate speech in one language directly into text in another language, without first transcribing it to text. According to the Researchers at Google Brain, this intermediate step can be avoided using neural networks. In order for the system to learn this, they exposed it to many hours of Spanish audio together with the corresponding English text. The different layers of neural networks, replicating the human brain, were able to link the corresponding parts and subsequently manipulate the audio waveform until it was transformed to English text.<ref>{{cite web|last1=Reynolds|first1=Matt|title=Google uses neural networks to translate without transcribing|url=https://www.newscientist.com/article/2126738-google-uses-neural-networks-to-translate-without-transcribing/|website=New Scientist|access-date=15 May 2017}}</ref> Another drawback of the GNMT model is that it causes the time of translation to increase exponentially with the number of words in the sentence.<ref name=":2" /> This caused the Google Brain Team to add 2000 more processors to ensure the new translation process would still be fast and reliable.<ref name=":1" />

=== Robotics ===
Aiming to improve traditional robotics control algorithms where new skills of a robot need to be [[Programming language|hand-programmed]], robotics researchers at Google Brain are developing [[machine learning]] techniques to allow robots to learn new skills on their own.<ref>{{Cite news|last1=Metz|first1=Cade|last2=Dawson|first2=Brian|last3=Felling|first3=Meg|date=2019-03-26|title=Inside Google's Rebooted Robotics Program|language=en-US|work=The New York Times|url=https://www.nytimes.com/2019/03/26/technology/google-robotics-lab.html|access-date=2021-04-08|issn=0362-4331}}</ref> They also attempt to develop ways for information sharing between robots so that robots can learn from each other during their learning process, also known as [[cloud robotics]].<ref name=":3">{{Cite web|date=2018-10-24|title=Google Cloud Robotics Platform coming to developers in 2019|url=https://www.therobotreport.com/google-cloud-robotics-platform/|access-date=2021-04-08|website=The Robot Report|language=en-US}}</ref> As a result, Google has launched the Google Cloud Robotics Platform for developers in 2019, an effort to combine [[robotics]], [[Artificial intelligence|AI]], and the [[Cloud computing|cloud]] to enable efficient robotic automation through cloud-connected collaborative robots.<ref name=":3" />

Robotics research at Google Brain has focused mostly on improving and applying deep learning algorithms to enable robots to complete tasks by learning from experience, simulation, human demonstrations, and/or visual representations.<ref name=":4">{{Cite journal|last1=Zeng|first1=A.|last2=Song|first2=S.|last3=Lee|first3=J.|last4=Rodriguez|first4=A.|last5=Funkhouser|first5=T.|date=August 2020|title=TossingBot: Learning to Throw Arbitrary Objects With Residual Physics|journal=IEEE Transactions on Robotics|volume=36|issue=4|pages=1307–1319|doi=10.1109/TRO.2020.2988642|issn=1941-0468|doi-access=free}}</ref><ref>{{Cite journal|last1=Gu|first1=S.|last2=Holly|first2=E.|last3=Lillicrap|first3=T.|last4=Levine|first4=S.|date=May 2017|title=Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates|url=https://ieeexplore.ieee.org/document/7989385|journal=2017 IEEE International Conference on Robotics and Automation (ICRA)|pages=3389–3396|doi=10.1109/ICRA.2017.7989385|arxiv=1610.00633|isbn=978-1-5090-4633-1|s2cid=18389147}}</ref><ref name=":5">{{Cite journal|last1=Sermanet|first1=P.|last2=Lynch|first2=C.|last3=Chebotar|first3=Y.|last4=Hsu|first4=J.|last5=Jang|first5=E.|last6=Schaal|first6=S.|last7=Levine|first7=S.|last8=Brain|first8=G.|date=May 2018|title=Time-Contrastive Networks: Self-Supervised Learning from Video|url=https://ieeexplore.ieee.org/document/8462891|journal=2018 IEEE International Conference on Robotics and Automation (ICRA)|pages=1134–1141|doi=10.1109/ICRA.2018.8462891|arxiv=1704.06888|isbn=978-1-5386-3081-5|s2cid=3997350}}</ref><ref name=":6">{{Cite journal|last1=Tanwani|first1=A. K.|last2=Sermanet|first2=P.|last3=Yan|first3=A.|last4=Anand|first4=R.|last5=Phielipp|first5=M.|last6=Goldberg|first6=K.|date=May 2020|title=Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos|url=https://ieeexplore.ieee.org/document/9197324|journal=2020 IEEE International Conference on Robotics and Automation (ICRA)|pages=2174–2181|doi=10.1109/ICRA40945.2020.9197324|arxiv=2006.00545|isbn=978-1-7281-7395-5|s2cid=219176734}}</ref> For example, Google Brain researchers showed that robots can learn to pick and throw rigid objects into selected boxes by experimenting in an environment without being pre-programmed to do so.<ref name=":4" /> In another research, researchers trained robots to learn behaviors such as pouring liquid from a cup; robots learned from videos of human demonstrations recorded from multiple viewpoints.<ref name=":5" />

Google Brain researchers have collaborated with other companies and academic institutions on robotics research. In 2016, the Google Brain Team collaborated with researchers at [[X (company)|X]] in a research on learning hand-eye coordination for robotic grasping.<ref name=":7">{{Cite journal|last1=Levine|first1=Sergey|last2=Pastor|first2=Peter|last3=Krizhevsky|first3=Alex|last4=Ibarz|first4=Julian|last5=Quillen|first5=Deirdre|date=2018-04-01|title=Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection|journal=The International Journal of Robotics Research|language=en|volume=37|issue=4–5|pages=421–436|doi=10.1177/0278364917710318|issn=0278-3649|doi-access=free}}</ref> Their method allowed real-time robot control for grasping novel objects with self-correction.<ref name=":7" /> In 2020, researchers from Google Brain, Intel AI Lab, and UC Berkeley created an AI model for robots to learn surgery-related tasks such as suturing from training with surgery videos.<ref name=":6" />

=== Interactive Speaker Recognition with Reinforcement Learning ===
In 2020, Google Brain Team and [[University of Lille]] presented a model for automatic speaker recognition which they called Interactive Speaker Recognition. The ISR module recognizes a speaker from a given list of speakers only by requesting a few user specific words.<ref name=":12">{{Cite journal|last1=Seurin|first1=Mathieu|last2=Strub|first2=Florian|last3=Preux|first3=Philippe|last4=Pietquin|first4=Olivier|date=2020-10-25|title=A Machine of Few Words: Interactive Speaker Recognition with Reinforcement Learning|url=http://dx.doi.org/10.21437/interspeech.2020-2892|journal=Interspeech 2020|pages=4323–4327|location=ISCA|publisher=ISCA|doi=10.21437/interspeech.2020-2892|arxiv=2008.03127|s2cid=221083446}}</ref> The model can be altered to choose speech segments in the context of [[Speech synthesis|Text-To-Speech]] Training.<ref name=":12" /> It can also prevent malicious voice generators to protect the data.<ref name=":12" />

=== TensorFlow ===
{{Main|TensorFlow}}
[[TensorFlow]] is an open source software library powered by Google Brain that allows anyone to utilize machine learning by providing the tools to train one's own neural network.<ref name=":2" /> The tool has been used by farmers to reduce the amount of manual labor required to sort their yield, by training it with a data set of human-sorted images.<ref name=":2" />

=== Magenta ===
Magenta is a project that uses Google Brain to create new information in the form of art and music rather than classify and sort existing data.<ref name=":2" /> [[TensorFlow]] was updated with a suite of tools for users to guide the [[neural network]] to create images and music.<ref name=":2" /> However, the team from [[Valdosta State University]] found that the [[Artificial intelligence|AI]] struggles to perfectly replicate human intention in [[Art|artistry]], similar to the issues faced in [[translation]].<ref name=":2" />

=== Medical Applications ===
The image sorting capabilities of Google Brain have been used to help detect certain medical conditions by seeking out patterns that human doctors may not notice to provide an earlier diagnosis.<ref name=":2" /> During screening for breast cancer, this method was found to have one quarter the false positive rate of human pathologists, who require more time to look over each photo and cannot spend their entire focus on this one task.<ref name=":2" /> Due to the neural network's very specific training for a single task, it cannot identify other afflictions present in a photo that a human could easily spot.<ref name=":2" />

=== Other Google Products ===
The Google Brain projects’ technology is currently used in various other Google products such as the [[Android (operating system)|Android Operating System]]’s [[Speech recognition|speech recognition system]], photo search for [[Google Photos]], [https://blog.google/products/gmail/save-time-with-smart-reply-in-gmail/ smart reply] in [[Gmail]], and video recommendations in [[YouTube]].<ref>{{Cite news|title=How Google Retooled Android With Help From Your Brain|language=en-us|work=Wired|url=https://www.wired.com/2013/02/android-neural-network/|access-date=2021-04-08|issn=1059-1028}}</ref><ref>{{Cite web|title=Google Open-Sources The Machine Learning Tech Behind Google Photos Search, Smart Reply And More|url=https://social.techcrunch.com/2015/11/09/google-open-sources-the-machine-learning-tech-behind-google-photos-search-smart-reply-and-more/|access-date=2021-04-08|website=TechCrunch|language=en-US}}</ref><ref>{{cite web|date=May 18, 2015|title=This Is Google's Plan to Save YouTube|url=http://time.com/3882422/google-youtube/|work=[[Time (magazine)|Time]]}}</ref>

==Reception==
Google Brain has received coverage in ''[[Wired Magazine]]'',<ref name=wired-kurzweil>{{cite magazine|url=https://www.wired.com/business/2013/04/kurzweil-google-ai/|title = How Ray Kurzweil Will Help Google Make the Ultimate AI Brain|magazine = Wired|last = Levy|first = Steven|author-link = Steven Levy|date = April 25, 2013|access-date = February 11, 2014}}</ref><ref name="wired-2014">{{cite magazine|url=https://www.wired.com/business/2014/01/google-buying-way-making-brain-irrelevant/|title=Google's Grand Plan to Make Your Brain Irrelevant|magazine=Wired|last=Wohlsen|first=Marcus|date=January 27, 2014|access-date=February 11, 2014}}</ref><ref name="wired-ng">{{cite magazine|last=Hernandez|first=Daniela|date=May 7, 2013|title=The Man Behind the Google Brain: Andrew Ng and the Quest for the New AI|url=https://www.wired.com/wiredenterprise/2013/05/neuro-artificial-intelligence/all/|magazine=Wired|access-date=February 11, 2014}}</ref> [[National Public Radio]],<ref name=npr-cats/> and [[Big Think]].<ref name=":8">{{cite web|url=http://bigthink.com/big-think-tv/ray-kurzweil-and-the-brains-behind-the-google-brain|title = Ray Kurzweil and the Brains Behind the Google Brain|publisher = [[Big Think]]|date = December 8, 2013|access-date = February 11, 2014}}</ref> These articles have contained interviews with key team members Ray Kurzweil and Andrew Ng, and focus on explanations of the project’s goals and applications.<ref name="wired-kurzweil" /><ref name="npr-cats" /><ref name=":8" />

=== Controversy ===
In December 2020, AI ethicist [[Timnit Gebru]] left Google.<ref name=":9">{{Cite web|title=We read the paper that forced Timnit Gebru out of Google. Here's what it says.|url=https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/|access-date=2021-04-08|website=MIT Technology Review|language=en}}</ref> While the exact nature of her quitting or being fired is disputed, the cause of the departure was her refusal to retract a paper entitled “On the Dangers of Stochastic Parrots: Can Language Models be Too Big?”<ref name=":9" /> This paper explored potential risks of the growth of AI such as Google Brain, including environmental impact, biases in training data, and the ability to deceive the public.<ref name=":9" /><ref>{{Cite journal|last1=Bender|first1=Emily M.|last2=Gebru|first2=Timnit|last3=McMillan-Major|first3=Angelina|last4=Shmitchell|first4=Shmargaret|date=2021-03-03|title=On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜|journal=Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency|language=en|location=Virtual Event Canada|publisher=ACM|pages=610–623|doi=10.1145/3442188.3445922|isbn=978-1-4503-8309-7|doi-access=free}}</ref> The request to retract the paper was made by Megan Kacholia, vice president of Google Brain.<ref name=":10">{{Cite web|last=Schiffer|first=Zoe|date=2021-02-19|title=Google fires second AI ethics researcher following internal investigation|url=https://www.theverge.com/2021/2/19/22292011/google-second-ethical-ai-researcher-fired|access-date=2021-04-08|website=The Verge|language=en}}</ref> As of April 2021, nearly 7000 current or former Google employees and industry supporters have signed an open letter accusing Google of “research censorship” and condemning Gebru's treatment at the company.<ref>{{Cite web|last=Change|first=Google Walkout For Real|date=2020-12-15|title=Standing with Dr. Timnit Gebru — #ISupportTimnit #BelieveBlackWomen|url=https://googlewalkout.medium.com/standing-with-dr-timnit-gebru-isupporttimnit-believeblackwomen-6dadc300d382|access-date=2021-04-08|website=Medium|language=en}}</ref>

In February 2021, Google fired one of the leaders of the company's AI ethics team, Margaret Mitchell.<ref name=":10" /> The company's statement alleged that Mitchell had broken company policy by using automated tools to find support for Gebru.<ref name=":10" /> In the same month, engineers outside the ethics team began to quit, citing the “wrongful” termination of Gebru as the reason why.<ref>{{Cite news|last=Dave|first=Jeffrey Dastin, Paresh|date=2021-02-04|title=Two Google engineers resign over firing of AI ethics researcher Timnit Gebru|language=en|work=Reuters|url=https://www.reuters.com/article/us-alphabet-resignations-idUSKBN2A4090|access-date=2021-04-08}}</ref> In April 2021, Google Brain co-founder [[Samy Bengio]] announced his resignation from the company.<ref name=":11" /> Despite being Gebru's manager, Bengio was not notified before her termination, and he posted online in support of both her and Mitchell.<ref name=":11" /> While Bengio's announcement focused on personal growth as his reason for leaving, anonymous sources indicated to Reuters that the turmoil within the AI ethics team played a role in his considerations.<ref name=":11" />

==See also==
* [[Artificial intelligence]]
*[[Deep learning|Deep Learning]]
* [[Glossary of artificial intelligence]]
* [[Quantum Artificial Intelligence Lab]] – run by Google in collaboration with NASA and Universities Space Research Association
* [[Noogenesis]]
* [[TensorFlow]]
*[[Timnit Gebru]]
*[[Samy Bengio]]

== References ==
{{Reflist}}

{{-}}
{{Google Inc.}}
{{Differentiable computing}}

[[Category:Applied machine learning]]
[[Category:Google|Brain]]