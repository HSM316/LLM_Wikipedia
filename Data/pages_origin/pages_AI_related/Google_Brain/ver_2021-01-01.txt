{{advert|date=February 2019}}
{{Infobox project
| name = Google Brain
| logo = 
| website = {{URL|https://ai.google/brain-team/}}
| commercial = Yes
| type = [[Artificial intelligence]] and [[machine learning]]
| location = [[Mountain View, California|Mountain View]], California
}}
'''Google Brain''' is a [[deep learning]] [[artificial intelligence]] research team at [[Google]]. Formed in the early 2010s, Google Brain combines open-ended machine learning research with [[Information system|information systems]] and large-scale [[computing]] resources.<ref>{{cite web |title=Brain Team mission – Google AI |url=https://www.inbetweenerz.com/ |website=Google AI |language=en |access-date=2018-06-19 |archive-url=https://web.archive.org/web/20180620051038/https://ai.google/research/teams/brain/our-mission/ |archive-date=2018-06-20 |url-status=dead }}</ref><ref>[https://research.google.com/teams/brain/machine-learning/  Machine Learning Algorithms and Techniques] Research at Google. Retrieved May 18, 2017</ref><ref>{{Cite web|url=https://research.google.com/teams/brain/|title=Research at Google|website=research.google.com|language=en|access-date=2018-02-16}}</ref>

==History==
The so-called "Google Brain" project began in 2011 as a part-time research collaboration between Google Fellow [[Jeff Dean (computer scientist)|Jeff Dean]], Google Researcher Greg Corrado, and [[Stanford University]] professor [[Andrew Ng]].<ref>{{cite web|title=Google's Large Scale Deep Neural Networks Project|url=https://www.inbetweenerz.com/|accessdate=25 October 2015}}</ref><ref name=ng-dean-blog /><ref name=nyt-cats /> Ng had been interested in using [[deep learning]] techniques to crack the problem of [[artificial intelligence]] since 2006, and in 2011 began collaborating with Dean and Corrado to build a large-scale deep learning software system, [[DistBelief]],<ref>{{cite web|author1=Jeffrey Dean|title=Large Scale Distributed Deep Networks|url=http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf|date=December 2012|accessdate=25 October 2015|display-authors=etal}}</ref> on top of Google's cloud computing infrastructure. Google Brain started as a [[Google X]] project and became so successful that it was graduated back to Google: [[Astro Teller]] has said that Google Brain paid for the entire cost of Google X.<ref>{{cite web|author1=Conor Dougherty|title=Astro Teller, Google's 'Captain of Moonshots,' on Making Profits at Google X|url=http://bits.blogs.nytimes.com/2015/02/16/googles-captain-of-moonshots-on-making-profits-at-google-x/|date = 16 February 2015|accessdate=25 October 2015}}</ref>

In June 2012, the ''[[New York Times]]'' reported that a cluster of 16,000 processors in 1,000 computers dedicated to mimicking some aspects of human brain activity had successfully trained itself to recognize a [[cat]] based on 10 million digital images taken from [[YouTube]] videos.<ref name=nyt-cats>{{cite news|url=https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html?pagewanted=all|title = How Many Computers to Identify a Cat? 16,000|last = Markoff|first = John|authorlink = John Markoff|date = June 25, 2012|accessdate = February 11, 2014|publisher = [[New York Times]]|newspaper = The New York Times}}</ref> The story was also covered by [[National Public Radio]]<ref name=npr-cats>{{cite web|url=https://www.npr.org/2012/06/26/155792609/a-massive-google-network-learns-to-identify|title = A Massive Google Network Learns To Identify — Cats|publisher = [[National Public Radio]]|date = June 26, 2012|accessdate = February 11, 2014}}</ref> and [[SmartPlanet]].<ref>{{cite web|url=http://www.smartplanet.com/blog/science-scope/google-brain-simulator-teaches-itself-to-recognize-cats/|title = Google brain simulator teaches itself to recognize cats|last = Shin|first = Laura|date = June 26, 2012|accessdate = February 11, 2014|publisher = [[SmartPlanet]]}}</ref>

In March 2013, Google hired [[Geoffrey Hinton]], a leading researcher in the deep learning field, and acquired the company DNNResearch Inc. headed by Hinton. Hinton said that he would be dividing his future time between his university research and his work at Google.<ref>{{cite press release|title=U of T neural networks start-up acquired by Google|date=12 March 2013|location=Toronto, ON|url=http://media.utoronto.ca/media-releases/u-of-t-neural-networks-start-up-acquired-by-google/|accessdate=13 March 2013}}</ref>

==Projects==
===Artificial-intelligence-devised encryption system===
In October 2016, the Google Brain ran an experiment concerning the encrypting of communications. In it, two sets of [[artificial intelligence|AI's]] devised their own cryptographic algorithms to protect their communications from another AI, which at the same time aimed at evolving its own system to crack the AI-generated encryption. The study proved to be successful, with the two initial AIs being able to learn and further develop their communications from scratch.<ref>{{cite web|url=https://arstechnica.co.uk/information-technology/2016/10/google-ai-neural-network-cryptography/ |title=Google AI invents its own cryptographic algorithm; no one knows how it works |website=arstechnica.co.uk |date= 2016-10-28|accessdate=2017-05-15}}</ref>

In this experiment, three AIs were created: Alice, Bob and Eve. The goal of the experiment was for Alice to send a message to Bob, which would [[decryption|decrypt]] it, while in the meantime Eve would try to intercept the message. In it, the AIs were not given specific instructions on how to [[cryptography|encrypt]] their messages, they were solely given a [[Loss_function|loss function]]. The consequence was that during the experiment, if communications between Alice and Bob were not successful, with Bob misinterpreting Alice's message or Eve intercepting the communications, the following rounds would show an evolution in the cryptography so that Alice and Bob could communicate safely. Indeed, this study allowed for concluding that it is possible for AIs to devise their own encryption system without having any cryptographic algorithms prescribed beforehand, which would {{clarify |date=April 2020 |reason=this claim isn't backed by citation, likely speculative in nature.|text=reveal a breakthrough for message encryption in the future }}.<ref>{{cite journal |first=Martín |last=Abadi |first2=David G. |last2=Andersen |date=2016 |title=Learning to Protect Communications with Adversarial Neural Cryptography |arxiv=1610.06918 |bibcode=2016arXiv161006918A }}</ref>

===Image enhancement===
In February 2017, Google Brain announced an image enhancement system using [[neural networks]] to fill in details in very low resolution pictures. The examples provided would transform pictures with an 8x8 resolution into 32x32 ones.

The software utilizes two different neural networks to generate the images. The first, called a "conditioning network," maps the pixels of the low-resolution picture to a similar high-resolution one, lowering the resolution of the latter to 8×8 and trying to make a match. The second is a "prior network", which analyzes the pixelated image and tries to add details based on a large number of high resolution pictures. Then, upon upscaling of the original 8×8 picture, the system adds pixels based on its knowledge of what the picture should be. Lastly, the outputs from the two networks are combined to create the final image.<ref>{{cite journal |first=Ryan |last=Dahl |first2=Mohammad |last2=Norouzi |first3=Jonathon |last3=Shlens |date=2017 |title=Pixel Recursive Super Resolution |arxiv=1702.00783 |bibcode=2017arXiv170200783D }}</ref>

This represents a breakthrough in the enhancement of low resolution pictures. Despite the fact that the added details are not part of the real image, but only best guesses, the technology has shown impressive results when facing real-world testing. Upon being shown the enhanced picture and the real one, humans were fooled 10% of the time in case of celebrity faces, and 28% in case of bedroom pictures. This compares to previous disappointing results from normal bicubic scaling, which did not fool any human.<ref>{{cite web|url=https://arstechnica.com/information-technology/2017/02/google-brain-super-resolution-zoom-enhance/ |title=Google Brain super-resolution image tech makes "zoom, enhance!" real |website=arstechnica.co.uk |date= 2017-02-07|accessdate=2017-05-15}}</ref><ref>{{cite web|url=https://www.cnet.com/news/google-just-made-zoom-and-enhance-a-reality/ |title=Google just made 'zoom and enhance' a reality -- kinda |website=cnet.com |date= |accessdate=2017-05-15}}</ref><ref>{{cite web|url=https://www.engadget.com/2017/02/07/google-ai-image-enhancement/ |title=Google uses AI to sharpen low-res images |website=engadget.com |date= |accessdate=2017-05-15}}</ref>

=== Google Translate ===
The Google Brain project contributed to [[Google Translate]]. In September 2016, [[Google Neural Machine Translation|Google Neural Machine Translation (GNMT)]] was launched, an end-to-end learning framework, able to learn from a large number of examples. While its introduction has increased the quality of Google Translate's translations for the pilot languages, it was very difficult to create such improvements for all of its 103 languages. Addressing this problem, the Google Brain Team was able to develop a Multilingual GNMT system, which extended the previous one by enabling translations between multiple languages. Furthermore, it allows for Zero-Shot Translations, which are translations between two languages that the system has never explicitly seen before.<ref>{{cite web|last1=Schuster|first1=Mike|last2=Johnson|first2=Melvin|last3=Thorat|first3=Nikhil|title=Zero-Shot Translation with Google's Multilingual Neural Machine Translation System|url=https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html|website=Google Research Blog|accessdate=15 May 2017}}</ref> Google announced that Google Translate can now also translate without transcribing, using neural networks. This means that it is possible to translate speech in one language directly into text in another language, without first transcribing it to text. According to the Researchers at Google Brain, this intermediate step can be avoided using neural networks. In order for the system to learn this, they exposed it to many hours of Spanish audio together with the corresponding English text. The different layers of neural networks, replicating the human brain, were able to link the corresponding parts and subsequently manipulate the audio waveform until it was transformed to English text.<ref>{{cite web|last1=Reynolds|first1=Matt|title=Google uses neural networks to translate without transcribing|url=https://www.newscientist.com/article/2126738-google-uses-neural-networks-to-translate-without-transcribing/|website=New Scientist|accessdate=15 May 2017}}</ref>

=== Robotics ===
Different from the traditional robotics, robotics researched by the Google Brain Team could automatically learn to acquire new skills by machine learning. In 2016, the Google Brain Team collaborated with researchers at Google X to demonstrate how robots could use their experiences to teach themselves more efficiently. Robots made about 800,000 grasping attempts during research.<ref name="auto">{{Cite news|url=https://research.googleblog.com/2017/01/the-google-brain-team-looking-back-on.html|title=The Google Brain team — Looking Back on 2016|work=Research Blog|access-date=2017-12-18|language=en-US}}</ref> Later in 2017, the team explored three approaches for learning new skills: through reinforcement learning, through their own interaction with objects, and through human demonstration.<ref name="auto"/> To build on the goal of the Google Brain Team, they would continue making robots that are able to learn new tasks through learning and practice, as well as deal with compound tasks.

==In Google products==
The project's technology is currently used in the [[Android (operating system)|Android Operating System]]'s speech recognition system,<ref>{{cite web|url=http://googleresearch.blogspot.com/2012/08/speech-recognition-and-deep-learning.html|title=Speech Recognition and Deep Learning|work=Google Research Blog|date=August 6, 2012|accessdate=February 11, 2014}}</ref> photo search for Google+<ref>{{cite web|url=http://googleresearch.blogspot.com/2013/06/improving-photo-search-step-across.html|title=Improving Photo Search: A Step Across the Semantic Gap|work=Google Research Blog|date=June 12, 2013}}</ref> and video recommendations in YouTube.<ref>{{cite web|url=http://time.com/3882422/google-youtube/|title=This Is Google's Plan to Save YouTube|work=[[Time (magazine)|Time]]|date=May 18, 2015}}</ref>

==Team and location==
Google Brain was initially established by Google Fellow [[Jeff Dean (computer scientist)|Jeff Dean]] and visiting Stanford professor [[Andrew Ng]].<ref name=ng-dean-blog>{{cite web|author1=Jeff Dean and Andrew Ng|title=Using large-scale brain simulations for machine learning and A.I.|url=http://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html|website=Official Google Blog|accessdate=26 January 2015|date=26 June 2012}}</ref> In 2014, the team included Jeff Dean, Quoc Le, Ilya Sutskever, Alex Krizhevsky, Samy Bengio and Vincent Vanhoucke. In 2017, team members include Anelia Angelova, Samy Bengio, Greg Corrado, George Dahl, Michael Isard, Anjuli Kannan, Hugo Larochelle, Chris Olah,Salih Edneer ,Vincent Vanhoucke, Vijay Vasudevan and [[Fernanda Viegas]].<ref>Google Brain team website.  Accessed 13.05.2017. https://research.google.com/teams/brain/</ref> [[Chris Lattner]], who created [[Apple Inc.|Apple]]'s programming language [[Swift (programming language)|Swift]] and then ran [[Tesla, Inc.|Tesla]]'s autonomy team for six months joined Google Brain's team in August 2017.<ref>{{cite news|last1=Etherington|first1=Darrell|title=Swift creator Chris Lattner joins Google Brain after Tesla Autopilot stint|url=https://techcrunch.com/2017/08/14/swift-creator-chris-lattner-joins-google-brain-after-tesla-autopilot-stint/|accessdate=11 October 2017|work=TechCrunch|date=Aug 14, 2017}}</ref> Lattner left the team in January 2020 and joined [[SiFive]].<ref>{{Cite web|title=Chris Lattner's Homepage|url=http://nondot.org/sabre/|website=nondot.org|access-date=2020-05-09}}</ref>

Google Brain is based in [[Mountain View, California]] and has satellite groups in [[Accra]], [[Amsterdam]], [[Beijing]], [[Berlin]], [[Cambridge, Massachusetts|Cambridge (Massachusetts)]], [[London]], [[Montreal]], [[New York City]], [[Paris]], [[Pittsburgh]], [[Princeton, New Jersey|Princeton]], [[San Francisco]], [[Tokyo]], [[Toronto]], and [[Zurich]].<ref>{{Cite web|url=https://research.google.com/teams/brain/|title=Research at Google|website=research.google.com|language=en|access-date=2017-08-01}}</ref>

==Reception==
Google Brain has received coverage in ''[[Wired Magazine]]'',<ref name=wired-kurzweil>{{cite journal|url=https://www.wired.com/business/2013/04/kurzweil-google-ai/|title = How Ray Kurzweil Will Help Google Make the Ultimate AI Brain|journal = Wired|last = Levy|first = Steven|authorlink = Steven Levy|date = April 25, 2013|accessdate = February 11, 2014}}</ref><ref name="wired-2014">{{cite journal|url=https://www.wired.com/business/2014/01/google-buying-way-making-brain-irrelevant/|title=Google's Grand Plan to Make Your Brain Irrelevant|journal=Wired|last=Wohlsen|first=Marcus|date=January 27, 2014|accessdate=February 11, 2014}}</ref><ref name=wired-ng/> the ''[[New York Times]]'',<ref name=wired-ng>{{cite journal|url=https://www.wired.com/wiredenterprise/2013/05/neuro-artificial-intelligence/all/|title = The Man Behind the Google Brain: Andrew Ng and the Quest for the New AI|journal = Wired|last = Hernandez|first = Daniela|date = May 7, 2013|accessdate = February 11, 2014}}</ref> [[Technology Review]],<ref name=technologyreview-kurzweil>{{cite magazine|url=http://www.technologyreview.com/featuredstory/513696/deep-learning/|title = Deep Learning: With massive amounts of computational power, machines can now recognize objects and translate speech in real time. Artificial intelligence is finally getting smart.|last = Hof|first = Robert|date = April 23, 2013|accessdate = February 11, 2014|magazine = [[Technology Review]]}}</ref><ref name="technologyreview-deepmind">{{cite magazine|url=http://www.technologyreview.com/news/524026/is-google-cornering-the-market-on-deep-learning/|title=Is Google Cornering the Market on Deep Learning? A cutting-edge corner of science is being wooed by Silicon Valley, to the dismay of some academics.|last=Regalado|first=Antonio|date=January 29, 2014|magazine=[[Technology Review]]|accessdate=February 11, 2014}}</ref> [[National Public Radio]],<ref name=npr-cats/> and [[Big Think]].<ref>{{cite web|url=http://bigthink.com/big-think-tv/ray-kurzweil-and-the-brains-behind-the-google-brain|title = Ray Kurzweil and the Brains Behind the Google Brain|publisher = [[Big Think]]|date = December 8, 2013|accessdate = February 11, 2014}}</ref>

==See also==
* [[Artificial intelligence]]
* [[Glossary of artificial intelligence]]
* [[Quantum Artificial Intelligence Lab]] – run by Google in collaboration with NASA and Universities Space Research Association
* [[Noogenesis]]
* [[TensorFlow]]

== References ==
{{Reflist}}

{{-}}
{{Google Inc.}}
{{Differentiable computing}}

[[Category:Applied machine learning]]
[[Category:Google|Brain]]