{{Cleanup-reorganize|date=December 2012}}
'''Explanation-based learning''' ('''EBL''') is a form of [[machine learning]] that exploits a very strong, or even perfect, [[Domain (software engineering)|domain]] theory (i.e. a formal theory of an application domain akin to a [[domain model]] in [[ontology engineering]], not to be confused with Scott's [[domain theory]]) in order to make generalizations or form concepts from training examples.<ref>{{cite journal|title=Special issue on explanation in case-based reasoning |journal=Artificial Intelligence Review|date=October 2005|volume=24|issue=2}}</ref> It is also linked with [[Encoding (memory)]] to help with [[Learning]]. <ref>{{Cite journal|last1=Calin-Jageman|first1=Robert J.|last2=Horn Ratner|first2=Hilary|date=2005-12-01|title=The Role of Encoding in the Self-Explanation Effect|url=https://doi.org/10.1207/s1532690xci2304_4|journal=Cognition and Instruction|volume=23|issue=4|pages=523–543|doi=10.1207/s1532690xci2304_4| s2cid=145410154 |issn=0737-0008}}</ref>

== Details==
An example of EBL using a perfect domain theory is a program that learns to play [[chess]] through example. A specific chess position that contains an important feature such as "Forced loss of black queen in two moves" includes many irrelevant features, such as the specific scattering of pawns on the board. EBL can take a single training example and determine what are the relevant features in order to form a generalization.<ref>Black-queen example from {{cite book | last = Mitchell | first = Tom | title = Machine Learning | publisher = McGraw-Hill | year = 1997 | pages = [https://archive.org/details/machinelearning00mitc_087/page/n319 308]–309 | url =https://archive.org/details/machinelearning00mitc_087| url-access = limited | isbn = 0-07-042807-7 }}</ref>

A domain theory is ''perfect'' or ''complete'' if it contains, in principle, all information needed to decide any question about the domain. For example, the domain theory for chess is simply the rules of chess. Knowing the rules, in principle, it is possible to deduce the best move in any situation. However, actually making such a deduction is impossible in practice due to [[combinatoric explosion]]. EBL uses training examples to make searching for deductive consequences of a domain theory efficient in practice.

In essence, an EBL system works by finding a way to deduce each training example from the system's existing database of domain theory. Having a short [[Mathematical proof|proof]] of the training example extends the domain-theory database, enabling the EBL system to find and classify future examples that are similar to the training example very quickly.<ref>{{cite book | last = Mitchell | first = Tom | title = Machine Learning | publisher = McGraw-Hill | year = 1997 | pages = [https://archive.org/details/machinelearning00mitc_087/page/n331 320] | url =https://archive.org/details/machinelearning00mitc_087| url-access = limited | isbn = 0-07-042807-7 | quote = In its pure form, EBL involves reformulating the domain theory to produce general rules that classify examples in a single inference step. }}</ref>
The main drawback of the method—the cost of applying the learned proof macros, as these become numerous—was analyzed by Minton.<ref>{{cite journal | doi = 10.1016/0004-3702(90)90059-9 | last = Minton | first = Steven | journal = Artificial Intelligence | volume=42 | issue = 2–3 | pages = 363–392 | title = Quantitative Results Concerning the Utility Problem in Explanation-Based Learning | year = 1990 }}</ref>
 
=== Basic formulation===
EBL software takes four inputs:

* a hypothesis space (the set of all possible conclusions)
* a domain theory (axioms about a domain of interest)
* training examples (specific facts that rule out some possible hypothesis)
* operationality criteria (criteria for determining which features in the domain are efficiently recognizable, e.g. which features are directly detectable using sensors)<ref>{{cite journal|title=Defining operationality for explanation-based learning|journal=Artificial Intelligence|year=1988|first=Richard|last=Keller|volume=35|issue=2|pages=227–241|url=http://www.aaai.org/Papers/AAAI/1987/AAAI87-086.pdf|access-date=2009-02-22 | quote = Current Operationality Defn.: A concept description is ''operational'' if it can be used efficiently to recognize instances of the concept it denotes|doi=10.1016/0004-3702(88)90013-6}} After stating the common definition, the paper actually argues against it in favor of more-refined criteria.</ref>

== Application ==
An especially good application domain for an EBL is natural language processing (NLP). Here a rich domain theory, i.e., a natural language grammar—although neither perfect nor complete, is tuned to a particular application or particular language usage, using a [[treebank]] (training examples). Rayner pioneered this work.<ref>{{cite news | last = Rayner | first = Manny | title = Applying Explanation-Based Generalization to Natural Language Processing | location = Procs. International Conference on Fifth Generation Computing, Kyoto | pages = 1267–1274 | year = 1988 }}</ref> The first successful industrial application was to a commercial NL interface to relational databases.<ref>{{cite news | last = Samuelsson | first = Christer |author2=Manny Rayner | title = Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System | location = Procs. 12th International Joint Conference on Artificial Intelligence, Sydney | pages = 609–615 | year = 1991 }}</ref> The method has been successfully applied to several large-scale natural language parsing systems,<ref>{{cite book | last = Samuelsson | first = Christer | title =  Fast Natural-Language Parsing Using Explanation-Based Learning | publisher =  Doctoral Dissertation, Royal Institute of Technology |  location = Stockholm | year = 1994 }}</ref> where the utility problem was solved by omitting the original grammar (domain theory) and using specialized LR-parsing techniques, resulting in huge speed-ups, at a cost in coverage, but with a gain in disambiguation.
EBL-like techniques have also been applied to surface generation, the converse of parsing.<ref>{{cite news | last = Samuelsson | first = Christer | title = Example-Based Optimization of Surface-Generation Tables | location = in R. Mitkov and N. Nicolov (eds.) "Recent Advances in Natural Language Processing," vol. 136 of "Current Issues in Linguistic Theory" | publisher = John Benjamins, Amsterdam | year = 1996}}</ref>

When applying EBL to NLP, the operationality criteria can be hand-crafted,<ref>{{cite news | last = Rayner | first = Manny |author2=David Carter | title = Fast Parsing using Pruning and Grammar Specialization | url = https://archive.org/details/arxiv-cmp-lg9604017 | location = Procs. ACL, Santa Cruz | year = 1996 }}</ref> or can be
inferred from the treebank using either the entropy of its or-nodes<ref>{{cite news | last = Samuelsson | first = Christer | title = Grammar Specialization through Entropy Thresholds | url = https://archive.org/details/arxiv-cmp-lg9405022 | year = 1994 | pages = 188–195 | location = Procs. ACL, Las Cruces }}</ref>
or a target coverage/disambiguation trade-off (= recall/precision trade-off  = f-score).<ref>{{cite news | last = Cancedda | first = Nicola |author2=Christer Samuelsson | title = Corpus-based Grammar Specialization | year = 2000 | location = Procs 4th Computational Natural Language Learning Workshop }}</ref>
EBL can also be used to compile grammar-based language models for speech recognition, from general unification grammars.<ref>{{cite book | last = Rayner | first = Manny |author2=Beth Ann Hockey |author3=Pierrette Bouillon| title = Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler | date = n.d. | isbn = 1-57586-526-2 }}</ref>
Note how the utility problem, first exposed by Minton, was solved by discarding the original grammar/domain theory, and that the quoted articles tend to contain the phrase ''grammar specialization''—quite the opposite of the original term ''explanation-based generalization.'' Perhaps the best name for this technique would be ''data-driven search space reduction.''
Other people who worked on EBL for NLP include Guenther Neumann, Aravind Joshi, Srinivas Bangalore, and Khalil Sima'an.

== See also ==

* [[One-shot learning in computer vision]]
* [[Zero-shot learning]]

== References ==
{{reflist}}

{{DEFAULTSORT:Explanation-Based Learning}}
[[Category:Machine learning]]