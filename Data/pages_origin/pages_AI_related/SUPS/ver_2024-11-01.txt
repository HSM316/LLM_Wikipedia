{{Short description|Measure of neuronal network performance}}
In [[computational neuroscience]], '''SUPS''' (for '''S'''ynaptic '''U'''pdates '''P'''er '''S'''econd) or formerly CUPS ('''C'''onnections '''U'''pdates '''P'''er '''S'''econd) is a measure of a neuronal network performance, useful in fields of [[neuroscience]], [[cognitive science]], [[artificial intelligence]], and [[computer science]].

==Computing==
For a processor or computer designed to simulate a neural network SUPS is measured as the product of simulated neurons <math>N</math> and average connectivity <math>c</math>(synapses) per neuron per second:

<math>SUPS = c \times N</math>

Depending on the type of simulation it is usually equal to the total number of synapses simulated.

In an "asynchronous" dynamic simulation if a neuron spikes at <math>\upsilon</math> Hz, the average rate of synaptic updates provoked by the activity of that neuron is <math>\upsilon cN</math>. In a synchronous simulation with step <math>\Delta t</math> the number of synaptic updates per second would be <math>\frac{cN}{\Delta t}</math>. As <math>\Delta t</math> has to be chosen much smaller than the average interval between two successive afferent spikes, which implies <math>\Delta t < \frac{1}{\upsilon N}</math>, giving an average of synaptic updates equal to <math>\upsilon c N^2</math>. Therefore, spike-driven synaptic dynamics leads to a linear scaling of computational complexity [[Big O notation|O]](N) per neuron, compared with the O(N<sup>2</sup>) in the "synchronous" case.<ref name="Mattia1998">{{cite book
 |author1=Maurizio Mattia |author2=Paolo Del Giudice |title=Icann 98 |year = 1998
 |pages=1045–1050
 |doi = 10.1007/978-1-4471-1599-1_164 |title-link=spiking neuron |chapter=Asynchronous simulation of large networks of spiking neurons and dynamical synapses |series=Perspectives in Neural Computing |isbn=978-3-540-76263-8 |citeseerx=10.1.1.56.272 }}</ref>

==Records==
Developed in the 1980s  Adaptive Solutions' CNAPS-1064 Digital Parallel Processor chip is a full [[Neurochip|neural network (NNW)]]. It was designed as a [[coprocessor]] to a host and has 64 sub-processors arranged in a [[Network topology|1D array]] and operating in a [[SIMD]] mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. At 25&nbsp;MHz it is capable of 1.28&nbsp;[[Multiply–accumulate operation|GMAC]].<ref name="Weems">[ftp://ftp.cs.umass.edu/pub/osl/papers/uPSurvey-TR-95-42.ps.Z ''Real-Time Computing: Implications for General Microprocessors''] Chip Weems, Steve Dropsho</ref>

After the presentation of the RN-100 (12&nbsp;MHz) single neuron chip at Seattle 1991 [[Ricoh]] developed the multi-neuron chip RN-200. It had 16 neurons and 16 synapses per neuron. The chip has on-chip learning ability using a proprietary backdrop algorithm. It came in a 257-pin [[Pin grid array|PGA]] encapsulation and drew 3.0 W at a maximum. It was capable of 3&nbsp;GCPS (1&nbsp;GCPS at 32&nbsp;MHz).
<ref name="Almeida2003">{{cite book
 |author1=L. Almeida |author2=Luis B. Almeida |author3=S. Boverie |year = 2003
 |title = Intelligent Components and Instruments For Control Applications 2003 (SICICA 2003)
 |publisher=Elsevier |url = https://books.google.com/books?id=pDFdub32IdYC |isbn=9780080440101 }}</ref>

In 1991–97, [[Siemens]] developed the MA-16 chip, SYNAPSE-1 and SYNAPSE-3 Neurocomputer. The MA-16 was a fast matrix-matrix multiplier that can be combined to form [[systolic array]]s. It could process 4 patterns of 16 elements each (16-bit), with 16 neuron values (16-bit) at a rate of 800&nbsp;MMAC or 400&nbsp;MCPS at 50&nbsp;MHz. The SYNAPSE3-PC [[Conventional PCI|PCI card]] contained 2 MA-16 with a peak performance of 2560&nbsp;MOPS (1.28&nbsp;GMAC); 7160&nbsp;MOPS (3.58&nbsp;GMAC) when using three boards.<ref name="Lindsey1998">[http://neuralnets.web.cern.ch/NeuralNets/nnwinhephard.html ''Neural Network Hardware''] Clark S. Lindsey, Bruce Denby, Thomas Lindblad, 1998</ref>

In 2013, the [[K computer]] was used to simulate a neural network of 1.73 billion neurons with a total of 10.4 trillion synapses (1% of the human brain). The simulation ran for 40 minutes to simulate 1 s of brain activity at a normal activity level (4.4 on average). The simulation required 1 Petabyte of storage.<ref name="CNET20130805">[http://www.cnet.com/news/fujitsu-supercomputer-simulates-1-second-of-brain-activity/ ''Fujitsu supercomputer simulates 1 second of brain activity''] Tim Hornyak, CNET, August 5, 2013</ref>

== See also ==

* [[FLOP]]
* [[SPECint]]
* [[SPECfp]]
* [[Multiply–accumulate operation]]
* [[Orders of magnitude (computing)]]
* [[SyNAPSE]]

==References==
{{Reflist|30em}}

[[Category:Benchmarks (computing)]]
[[Category:Units of frequency]]
[[Category:Artificial intelligence]]
[[Category:Computational neuroscience]]
[[Category:Neurotechnology]]