{{Short description|Computational method for constructing 3D models}}
[[File:Inverse depth parametrization.svg|thumb|upright=1.5|In inverse depth parametrization, a point is identified by its inverse depth <math>\rho = \frac{1}{\left\Vert \mathbf{p} - \mathbf{c}_0\right\Vert} </math> along the ray, with direction <math>v = (\cos \phi \sin \theta, -\sin \phi, \cos \phi \cos \theta)</math>, from which it was first observed.]]

In [[computer vision]], the '''inverse depth parametrization''' is a [[parametrization (geometry)|parametrization]] used in methods for [[3D reconstruction from multiple images]] such as [[simultaneous localization and mapping]] (SLAM).<ref name="Piniés">Piniés et al. (2007)</ref><ref name="Sunderhauf">Sunderhauf et al. (2007)</ref> Given a point <math>\mathbf{p}</math> in 3D space observed by a [[monocular]] [[pinhole camera]] from multiple views, the inverse depth parametrization of the point's position is a 6D vector that encodes the [[optical centre]] of the camera <math>\mathbf{c}_0</math> when in first observed the point, and the position of the point along the ray passing through <math>\mathbf{p}</math> and <math>\mathbf{c}_0</math>.<ref name="Civiera">Civiera et al. (2008)</ref>

Inverse depth parametrization generally improves [[numerical stability]] and allows to represent points with zero [[parallax]]. Moreover, the error associated to the observation of the point's position can be modelled with a [[Gaussian distribution]] when expressed in inverse depth. This is an important property required to apply methods, such as [[Kalman filter]]s, that assume normality of the measurement error distribution. The major drawback is the larger memory consumption, since the dimensionality of the point's representation is doubled.<ref name="Civiera" />

== Definition ==

Given 3D point <math>\mathbf{p} = (x, y, z)</math> with world coordinates in a [[coordinate system|reference frame]] <math>(e_1, e_2, e_3)</math>, observed from different views, the inverse depth parametrization <math>\mathbf{y}</math> of <math>\mathbf{p}</math> is given by:
:<math> \mathbf{y} = (x_0, y_0, z_0, \theta, \phi, \rho) </math>
where the first five components encode the camera pose in the first observation of the point, being <math>\mathbf{c_0} = (x_0, y_0, z_0)</math> the [[optical centre]], <math>\phi</math> the [[azimuth]], <math>\theta</math> the elevation angle, and <math>\rho = \frac{1}{\left\Vert \mathbf{p} - \mathbf{c}_0\right\Vert}</math> the inverse depth of <math>p</math> at the first observation.<ref name="Civiera" />

== References ==
<references />

== Bibliography ==
* {{cite journal|title=Unified inverse depth parametrization for monocular SLAM|last1=Montiel|first1=JM Martínez|last2=Civera|first2=Javier|last3=Davison|first3=Andrew J|year=2006|publisher=Robotics: Science and Systems}}
* {{cite journal|title=Inverse depth parametrization for monocular SLAM|year=2008|journal=IEEE Transactions on Robotics|volume=24|pages=932–945|issue=5|publisher=IEEE|last1=Civera|first1=Javier|last2=Davison|first2=Andrew J|last3=Montiel|first3=JM Martínez|doi=10.1109/TRO.2008.2003276|s2cid=345360}}
* {{cite journal|title=Inertial aiding of inverse depth SLAM using a monocular camera|last1=Piniés|first1=Pedro|last2=Lupton|first2=Todd|last3=Sukkarieh|first3=Salah|last4=Tardós|first4=Juan D|journal=Proceedings 2007 IEEE International Conference on Robotics and Automation|pages=2797–2802|year=2007|publisher=IEEE|doi=10.1109/ROBOT.2007.363895|isbn=978-1-4244-0602-9|s2cid=10474338}}
* {{cite journal|title=Using the unscented Kalman filter in mono-SLAM with inverse depth parametrization for autonomous airship control|last1=Sunderhauf|first1=Niko|last2=Lange|first2=Sven|last3=Protzel|first3=Peter|journal=2007 IEEE International Workshop on Safety, Security and Rescue Robotics|pages=1–6|year=2007|publisher=IEEE}}

[[Category:Computer vision]]