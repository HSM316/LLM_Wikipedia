{{short description|Set of learning techniques in machine learning}}
{{Machine learning|Problems}}
[[File:Feature Learning Diagram.png|thumb|354x354px|Diagram of the feature learning paradigm in ML for application to downstream tasks, which can be applied to either raw data such as images or text, or to an initial set of [[Feature (machine learning)|features]] of the data. Feature learning is intended to result in faster training or better performance in task-specific settings than if the data was input directly (compare [[transfer learning]]).<ref>Goodfellow, Ian (2016). ''Deep learning''. Yoshua Bengio, Aaron Courville. Cambridge, Massachusetts. pp. 524–534. {{ISBN|0-262-03561-8}}. {{OCLC|955778308}}.</ref>]]
In [[machine learning]] (ML), '''feature learning''' or '''representation learning'''<ref name="pami">{{cite journal |author1=Y. Bengio |author2=A. Courville |author3=P. Vincent |title=Representation Learning: A Review and New Perspectives |journal= IEEE Transactions on Pattern Analysis and Machine Intelligence|year=2013|doi=10.1109/tpami.2013.50 |pmid=23787338 |volume=35 |issue=8 |pages=1798–1828|arxiv=1206.5538 |s2cid=393948 }}</ref> is a set of techniques that allow a system to automatically discover the representations needed for [[Feature (machine learning)|feature]] detection or classification from raw data. This replaces manual [[feature engineering]] and allows a machine to both learn the features and use them to perform  a specific task.

Feature learning is motivated by the fact that ML tasks such as [[Statistical classification|classification]] often require input that is mathematically and computationally convenient to process. However, real-world data, such as image, video, and sensor data, have not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.

Feature learning can be either supervised, unsupervised, or self-supervised:

* In [[Supervised learning|supervised feature learning]], features are learned using labeled input data. Labeled data includes input-label pairs where the input is given to the model, and it must produce the ground truth label as the output.<ref>Stuart J. Russell, Peter Norvig (2010) ''[[Artificial Intelligence: A Modern Approach]], Third Edition'', Prentice Hall {{ISBN|978-0-13-604259-4}}.</ref> This can be leveraged to generate feature representations with the model which result in high label prediction accuracy. Examples include supervised [[Neural network (machine learning)|neural networks]], [[multilayer perceptron]]s, and [[dictionary learning]].
* In [[Unsupervised learning|unsupervised feature learning]], features are learned with unlabeled input data by analyzing the relationship between points in the dataset.<ref>Hinton, Geoffrey; Sejnowski, Terrence (1999). ''Unsupervised Learning: Foundations of Neural Computation''. MIT Press. {{ISBN|978-0-262-58168-4}}.</ref>  Examples include dictionary learning, [[independent component analysis]], [[Matrix decomposition|matrix factorization]],<ref>{{cite conference
|author1=Nathan Srebro |author2=Jason D. M. Rennie |author3=Tommi S. Jaakkola
|title=Maximum-Margin Matrix Factorization
|conference=[[Conference on Neural Information Processing Systems|NIPS]]
|year=2004
}}</ref> and various forms of [[Cluster analysis|clustering]].<ref name="coates2011"/><ref>{{cite conference
|last1 = Csurka |first1 = Gabriella
|last2 = Dance |first2 = Christopher C.
|last3 = Fan |first3 = Lixin
|last4 = Willamowski |first4 = Jutta
|last5 = Bray |first5 = Cédric
|title = Visual categorization with bags of keypoints
|conference = ECCV Workshop on Statistical Learning in Computer Vision
|year = 2004
|url = https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/csurka-eccv-04.pdf
}}</ref><ref name="jurafsky">{{cite book |title=Speech and Language Processing |author1=Daniel Jurafsky|author-link=Daniel Jurafsky|author2=James H. Martin |publisher=Pearson Education International |year=2009 |pages=145–146}}</ref>
* In [[Self-supervised learning|self-supervised feature learning]], features are learned using unlabeled data like unsupervised learning, however input-label pairs are constructed from each data point, enabling learning the structure of the data through supervised methods such as [[gradient descent]].<ref name=":0">{{Cite journal |last1=Ericsson |first1=Linus |last2=Gouk |first2=Henry |last3=Loy |first3=Chen Change |last4=Hospedales |first4=Timothy M. |date=May 2022 |title=Self-Supervised Representation Learning: Introduction, advances, and challenges |url=https://ieeexplore.ieee.org/document/9770283 |journal=IEEE Signal Processing Magazine |volume=39 |issue=3 |pages=42–62 |doi=10.1109/MSP.2021.3134634 |arxiv=2110.09327 |bibcode=2022ISPM...39c..42E |s2cid=239017006 |issn=1558-0792}}</ref> Classical examples include [[word embedding]]s and [[autoencoder]]s.<ref name=":3">{{Cite journal |last1=Mikolov |first1=Tomas |last2=Sutskever |first2=Ilya |last3=Chen |first3=Kai |last4=Corrado |first4=Greg S |last5=Dean |first5=Jeff |date=2013 |title=Distributed Representations of Words and Phrases and their Compositionality |url=https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html |journal=Advances in Neural Information Processing Systems |publisher=Curran Associates, Inc. |volume=26|arxiv=1310.4546 }}</ref><ref name=":1">Goodfellow, Ian (2016). ''Deep learning''. Yoshua Bengio, Aaron Courville. Cambridge, Massachusetts. pp. 499–516. {{ISBN|0-262-03561-8}}. {{OCLC|955778308}}.</ref> Self-supervised learning has since been applied to many modalities through the use of [[Deep learning|deep neural network architectures]] such as [[convolutional neural network]]s and [[Transformer (machine learning model)|transformers]].<ref name=":0" />

== Supervised  ==
Supervised feature learning is learning features from labeled data. The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error). Approaches include:

=== Supervised dictionary learning ===
Dictionary learning develops a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error  (over the input data), together with [[Regularization (mathematics)|''L1'' regularization]] on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights).

Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, this<ref>{{cite journal|last1=Mairal|first1=Julien|last2=Bach|first2=Francis|last3=Ponce|first3=Jean|last4=Sapiro|first4=Guillermo|last5=Zisserman|first5=Andrew|title=Supervised Dictionary Learning|journal=Advances in Neural Information Processing Systems|date=2009}}</ref> supervised dictionary learning technique applies dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an ''L1'' regularization on the representing weights for each data point (to enable sparse representation of data), and an ''L2'' regularization on the parameters of the classifier.

=== Neural networks===
[[Artificial neural networks|Neural networks]] are a family of learning algorithms that use a "network" consisting of multiple layers of inter-connected nodes. It is inspired by the animal nervous system, where the nodes are viewed as neurons and edges are viewed as synapses. Each edge has an associated weight, and the network defines computational rules for passing input data from the network's input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights).

Multilayer [[neural network]]s can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. The most popular network architecture of this type is [[Siamese neural network|Siamese networks]].

== Unsupervised{{anchor|Unsupervised_feature_learning}}  ==
Unsupervised feature learning is learning features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that capture some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of [[semisupervised learning]] where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data.<ref name="liang">{{cite thesis |type=M. Eng. |author=Percy Liang |year=2005 |title=Semi-Supervised Learning for Natural Language |publisher=[[Massachusetts Institute of Technology|MIT]] |url=http://people.csail.mit.edu/pliang/papers/meng-thesis.pdf |pages=44–52}}</ref><ref name="turian"/> Several approaches are introduced in the following.

=== ''K''-means clustering ===
[[K-means clustering|''K''-means clustering]] is an approach for vector quantization. In particular, given a set of ''n'' vectors, ''k''-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally [[NP-hard]], although suboptimal [[greedy algorithm]]s have been developed.

K-means clustering can be used to group an unlabeled set of inputs into ''k'' clusters, and then use the [[centroid]]s of these clusters to produce features. These features can be produced in several ways. The simplest is to add ''k'' binary features to each sample, where each feature ''j'' has value one [[if and only if|iff]] the ''j''th centroid learned by ''k''-means is the closest to the sample under consideration.<ref name="coates2011"/> It is also possible to use the distances to the clusters as features, perhaps after transforming them through a [[radial basis function]] (a technique that has been used to train [[Radial basis function network|RBF network]]s<ref name="schwenker">{{cite journal |last1=Schwenker |first1=Friedhelm |last2=Kestler |first2=Hans A. |last3=Palm |first3=Günther |title=Three learning phases for radial-basis-function networks |journal=Neural Networks |volume=14 |issue=4–5 |pages=439–458 |year=2001 |citeseerx = 10.1.1.109.312 |doi=10.1016/s0893-6080(01)00027-2|pmid=11411631 }}</ref>). Coates and [[Andrew Ng|Ng]] note that certain variants of ''k''-means behave similarly to [[sparse coding]] algorithms.<ref name=Coates2012>{{cite encyclopedia |last1 = Coates |first1 = Adam |last2 = Ng |first2 = Andrew Y. |title=Learning feature representations with k-means |encyclopedia=Neural Networks: Tricks of the Trade |year = 2012 |publisher=Springer |editor=G. Montavon, G. B. Orr and [[Klaus-Robert Müller|K.-R. Müller]]}}</ref>

In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that ''k''-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task.<ref name="coates2011"/> ''K''-means also improves performance in the domain of [[Natural language processing|NLP]], specifically for [[named-entity recognition]];<ref>{{cite conference |title=Phrase clustering for discriminative learning |author1=Dekang Lin |author2=Xiaoyun Wu |conference=Proc. J. Conf. of the ACL and 4th Int'l J. Conf. on Natural Language Processing of the AFNLP |pages=1030–1038 |year=2009 |url=http://wmmks.csie.ncku.edu.tw/ACL-IJCNLP-2009/ACLIJCNLP/pdf/ACLIJCNLP116.pdf |access-date=2013-07-14 |archive-date=2016-03-03 |archive-url=https://web.archive.org/web/20160303224116/http://wmmks.csie.ncku.edu.tw/ACL-IJCNLP-2009/ACLIJCNLP/pdf/ACLIJCNLP116.pdf |url-status=dead }}</ref> there, it competes with [[Brown clustering]], as well as with distributed word representations (also known as neural word embeddings).<ref name="turian">{{cite conference |author1=Joseph Turian |author2=Lev Ratinov |author3=Yoshua Bengio |title=Word representations: a simple and general method for semi-supervised learning |conference=Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics |year=2010 |url=http://www.newdesign.aclweb.org/anthology/P/P10/P10-1040.pdf |access-date=2014-02-22 |archive-url=https://web.archive.org/web/20140226202823/http://www.newdesign.aclweb.org/anthology/P/P10/P10-1040.pdf |archive-date=2014-02-26 |url-status=dead }}</ref>

=== Principal component analysis ===
[[Principal component analysis]] (PCA) is often used for dimension reduction. Given an unlabeled set of ''n'' input data vectors, PCA generates ''p'' (which is much smaller than the dimension of the input data) [[Singular value decomposition|right singular vectors]] corresponding to the ''p'' largest singular values of the data matrix, where the ''k''th row of the data matrix is the ''k''th input data vector shifted by the [[Sample mean and sample covariance|sample mean]] of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the [[eigenvector]]s corresponding to the ''p'' largest eigenvalues of the [[Sample mean and sample covariance|sample covariance matrix]] of the input vectors. These ''p'' singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations.

PCA is a linear feature learning approach since the ''p'' singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with ''p'' iterations. In the ''i''th iteration, the projection of the data matrix on the ''(i-1)''th eigenvector is subtracted, and the ''i''th singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix.

PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case. PCA only relies on orthogonal transformations of the original data, and it exploits only the first- and second-order [[Moment (mathematics)|moments]] of the data, which may not well characterize the data distribution. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues).

=== Local linear embedding ===
[[Nonlinear dimensionality reduction|Local linear embedding]] (LLE) is a nonlinear learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Roweis and Saul (2000).<ref name="RowSau00">{{cite journal|last1=Roweis|first1=Sam T|last2=Saul|first2=Lawrence K|title=Nonlinear Dimensionality Reduction by Locally Linear Embedding|journal=Science |series=New Series|date=2000|volume=290|issue=5500|pages=2323–2326|doi=10.1126/science.290.5500.2323|jstor=3081722|pmid=11125150|bibcode=2000Sci...290.2323R|s2cid=5987139 }}</ref><ref name="SauRow00">{{cite journal|last1=Saul|first1=Lawrence K|last2=Roweis|first2=Sam T|title=An Introduction to Locally Linear Embedding|date=2000|url=http://www.cs.toronto.edu/~roweis/lle/publications.html}}</ref> The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set.

LLE consists of two major steps. The first step is for "neighbor-preserving", where each input data point ''Xi'' is reconstructed as a weighted sum of [[K-nearest neighbors algorithm|''K'' nearest neighbor]] data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between an input point and its reconstruction) under the constraint that the weights associated with each point sum up to one. The second step is for "dimension reduction," by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with fixed data, which can be solved as a [[least squares]] problem. In the second step, lower-dimensional points are optimized with fixed weights, which can be solved via sparse eigenvalue decomposition.

The reconstruction weights obtained in the first step capture the "intrinsic geometric properties" of a neighborhood in the input data.<ref name="SauRow00"/> It is assumed that original data lie on a smooth lower-dimensional [[manifold]], and the "intrinsic geometric properties" captured by the weights of the original data are also expected to be on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying data structure.

=== Independent component analysis ===
[[Independent component analysis]] (ICA) is a technique for forming a data representation using a weighted sum of independent non-Gaussian components.<ref>{{cite journal|last1=Hyvärinen|first1=Aapo|last2=Oja|first2=Erkki|title=Independent Component Analysis: Algorithms and Applications|journal=Neural Networks|date=2000|volume=13|issue=4|pages=411–430|doi= 10.1016/s0893-6080(00)00026-5|pmid=10946390|s2cid=11959218 }}</ref> The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow [[Normal distribution|Gaussian]] distribution.

=== Unsupervised dictionary learning ===

Unsupervised dictionary learning does not utilize data labels and exploits the structure underlying the data for optimizing dictionary elements. An example of unsupervised dictionary learning is [[Sparse dictionary learning|sparse coding]], which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionaries, where the number of dictionary elements is larger than the dimension of the input data.<ref>{{cite journal|last1=Lee|first1=Honglak|last2=Battle|first2=Alexis|last3=Raina|first3=Rajat|last4=Ng|first4=Andrew Y|title=Efficient sparse coding algorithms|journal=Advances in Neural Information Processing Systems|date=2007}}</ref> [[Michal Aharon|Aharon]] et al. proposed algorithm [[K-SVD]] for learning a dictionary of elements that enables sparse representation.<ref>{{cite journal|last1=Aharon|first1=Michal|author1-link=Michal Aharon|last2=Elad|first2=Michael|last3=Bruckstein|first3=Alfred|title=K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation|journal=IEEE Trans. Signal Process.|date=2006|volume=54|issue=11|pages=4311–4322|doi=10.1109/TSP.2006.881199|bibcode=2006ITSP...54.4311A|s2cid=7477309 }}</ref>

== Multilayer/deep architectures ==

The hierarchical architecture of the biological neural system inspires [[deep learning]] architectures for feature learning by stacking multiple layers of learning nodes.<ref>{{cite journal|last1=Bengio|first1=Yoshua|title=Learning Deep Architectures for AI|journal=Foundations and Trends in Machine Learning|date=2009|volume=2|issue=1|pages=1–127|doi=10.1561/2200000006|s2cid=207178999 }}</ref> These architectures are often designed based on the assumption of [[distributed representation]]: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by the previous, lower level as input, and produces new representations as output, which are then fed to higher levels. The input at the bottom layer is raw data, and the output of the final, highest layer is the final low-dimensional feature or representation.

=== Restricted Boltzmann machine ===
[[Restricted Boltzmann machine]]s (RBMs) are often used as a building block for multilayer learning architectures.<ref name="coates2011">{{cite conference
|last1 = Coates
|first1 = Adam
|last2 = Lee
|first2 = Honglak
|last3 = Ng
|first3 = Andrew Y.
|title = An analysis of single-layer networks in unsupervised feature learning
|conference = Int'l Conf. on AI and Statistics (AISTATS)
|year = 2011
|url = http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf
|access-date = 2014-11-24
|archive-url = https://web.archive.org/web/20170813153615/http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf
|archive-date = 2017-08-13
|url-status = dead
}}</ref><ref name = Hinton2006>{{Cite journal | last1 = Hinton | first1 = G. E. | last2 = Salakhutdinov | first2 = R. R. | title = Reducing the Dimensionality of Data with Neural Networks | doi = 10.1126/science.1127647 | journal = Science | volume = 313 | issue = 5786 | pages = 504–507 | year = 2006 | pmid = 16873662 | url = http://www.cs.toronto.edu/~hinton/science.pdf | bibcode = 2006Sci...313..504H | s2cid = 1658773 | access-date = 2015-08-29 | archive-date = 2015-12-23 | archive-url = https://web.archive.org/web/20151223152006/http://www.cs.toronto.edu/~hinton/science.pdf | url-status = dead }}</ref> An RBM can be represented by an undirected [[bipartite graph]] consisting of a group of [[Binary variable|binary]] [[Latent variable|hidden variables]], a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general [[Boltzmann machine]]s with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an [[energy function]], based on which a [[joint distribution]] of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent, conditioned on the visible (hidden) variables.{{Clarify|reason=visible hidden?|date=June 2017}} Such conditional independence facilitates computations.

An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using [[Geoffrey Hinton|Hinton]]'s [[contrastive divergence]] (CD) algorithm.<ref name = Hinton2006/>

In general, training RBMs by solving the maximization problem tends to result in non-sparse representations. Sparse RBM<ref name = Lee2008>{{cite journal|last1=Lee|first1=Honglak|last2=Ekanadham|first2=Chaitanya|last3=Andrew|first3=Ng|title=Sparse deep belief net model for visual area V2|journal=Advances in Neural Information Processing Systems|date=2008}}</ref> was proposed to enable sparse representations. The idea is to add a [[Regularization (mathematics)|regularization]] term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant <math>p</math>. RBMs have also been used to obtain ''disentangled'' representations of data, where interesting features map to separate hidden units.<ref>{{Cite journal |last1=Fernandez-de-Cossio-Diaz |first1=Jorge |last2=Cocco |first2=Simona |last3=Monasson |first3=Rémi |date=2023-04-05 |title=Disentangling Representations in Restricted Boltzmann Machines without Adversaries |url=https://link.aps.org/doi/10.1103/PhysRevX.13.021003 |journal=Physical Review X |volume=13 |issue=2 |pages=021003 |doi=10.1103/PhysRevX.13.021003|arxiv=2206.11600 |bibcode=2023PhRvX..13b1003F }}</ref>

=== Autoencoder ===
An [[autoencoder]] consisting of an encoder and a decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov<ref name = Hinton2006/> where the encoder uses raw data (e.g., image) as input and produces feature or representation as output and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a [[Greedy algorithm|greedy]] layer-by-layer manner: after one layer of feature detectors is learned, they are fed up as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with [[stochastic gradient descent]] methods. Training can be repeated until some stopping criteria are satisfied.

== Self-supervised ==
Self-supervised representation learning is learning features by training on the structure of unlabeled data rather than relying on explicit labels for an [[Gradient descent|information signal]]. This approach has enabled the combined use of deep neural network architectures and larger unlabeled datasets to produce deep feature representations.<ref name=":0" /> Training tasks typically fall under the classes of either contrastive, generative or both.<ref name=":2">{{Cite journal |last1=Liu |first1=Xiao |last2=Zhang |first2=Fanjin |last3=Hou |first3=Zhenyu |last4=Mian |first4=Li |last5=Wang |first5=Zhaoyu |last6=Zhang |first6=Jing |last7=Tang |first7=Jie |date=2021 |title=Self-supervised Learning: Generative or Contrastive |url=https://ieeexplore.ieee.org/document/9462394 |journal=IEEE Transactions on Knowledge and Data Engineering |volume=35 |issue=1 |pages=857–876 |doi=10.1109/TKDE.2021.3090866 |arxiv=2006.08218 |s2cid=219687051 |issn=1558-2191}}</ref> Contrastive representation learning trains representations for associated data pairs, called positive samples, to be aligned, while pairs with no relation, called negative samples, are contrasted. A larger portion of negative samples is typically necessary in order to prevent catastrophic collapse, which is when all inputs are mapped to the same representation.<ref name=":0" /> Generative representation learning tasks the model with producing the correct data to either match a restricted input or reconstruct the full input from a lower dimensional representation.<ref name=":2" />

A common setup for self-supervised representation learning of a certain data type (e.g. text, image, audio, video) is to pretrain the model using large datasets of general context, unlabeled data.<ref name=":1" /> Depending on the context, the result of this is either a set of representations for common data segments (e.g. words) which new data can be broken into, or a neural network able to convert each new data point (e.g. image) into a set of lower dimensional features.<ref name=":0" /> In either case, the output representations can then be used as an initialization in many different problem settings where labeled data may be limited. Specialization of the model to specific tasks is typically done with supervised learning, either by fine-tuning the model / representations with the labels as the signal, or freezing the representations and training an additional model which takes them as an input.<ref name=":1" />

Many self-supervised training schemes have been developed for use in representation learning of various [[Modality (human–computer interaction)|modalities]], often first showing successful application in text or image before being transferred to other data types.<ref name=":0" />

=== Text ===
[[Word2vec]] is a [[word embedding]] technique which learns to represent words through self-supervision over each word and its neighboring words in a sliding window across a large corpus of text.<ref name=":4">{{cite arXiv |last1=Mikolov |first1=Tomas |last2=Chen |first2=Kai |last3=Corrado |first3=Greg |last4=Dean |first4=Jeffrey |date=2013-09-06 |title=Efficient Estimation of Word Representations in Vector Space |class=cs.CL |eprint=1301.3781 }}</ref> The model has two possible training schemes to produce word vector representations, one generative and one contrastive.<ref name=":2" /> The first is word prediction given each of the neighboring words as an input.<ref name=":4" /> The second is training on the representation similarity for neighboring words and representation dissimilarity for random pairs of words.<ref name=":3" /> A limitation of word2vec is that only the pairwise co-occurrence structure of the data is used, and not the ordering or entire set of context words. More recent transformer-based representation learning approaches attempt to solve this with word prediction tasks.<ref name=":0" /> [[Generative pre-trained transformer|GPTs]] pretrain on next word prediction using prior input words as context,<ref>[https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf "Improving Language Understanding by Generative Pre-Training"] (PDF). Retrieved October 10, 2022.</ref> whereas [[BERT (language model)|BERT]] masks random tokens in order to provide bidirectional context.<ref name=":7">{{Cite journal |last1=Devlin |first1=Jacob |last2=Chang |first2=Ming-Wei |last3=Lee |first3=Kenton |last4=Toutanova |first4=Kristina |title=Proceedings of the 2019 Conference of the North |date=June 2019 |url=https://aclanthology.org/N19-1423 |journal=Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) |location=Minneapolis, Minnesota |publisher=Association for Computational Linguistics |pages=4171–4186 |doi=10.18653/v1/N19-1423|s2cid=52967399 }}</ref>

Other self-supervised techniques extend word embeddings by finding representations for larger text structures such as [[Sentence embedding|sentences]] or paragraphs in the input data.<ref name=":0" /> [[Doc2Vec|Doc2vec]] extends the generative training approach in word2vec by adding an additional input to the word prediction task based on the paragraph it is within, and is therefore intended to represent paragraph level context.<ref>{{Cite journal |last1=Le |first1=Quoc |last2=Mikolov |first2=Tomas |date=2014-06-18 |title=Distributed Representations of Sentences and Documents |url=https://proceedings.mlr.press/v32/le14.html |journal=International Conference on Machine Learning |language=en |publisher=PMLR |pages=1188–1196|arxiv=1405.4053 }}</ref>

=== Image ===
The domain of image representation learning has employed many different self-supervised training techniques, including transformation,<ref>Spyros Gidaris, Praveer Singh, and Nikos Komodakis. [https://openreview.net/pdf?id=S1v4N2l0- Unsupervised representation learning by predicting image rotations.] In ICLR, 2018.</ref> inpainting,<ref name=":5">{{Cite journal |last1=Pathak |first1=Deepak |last2=Krahenbuhl |first2=Philipp |last3=Donahue |first3=Jeff |last4=Darrell |first4=Trevor |last5=Efros |first5=Alexei A. |date=2016 |title=Context Encoders: Feature Learning by Inpainting |url=https://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html |pages=2536–2544|arxiv=1604.07379 }}</ref> patch discrimination<ref name=":6">{{Cite journal |last1=Chen |first1=Ting |last2=Kornblith |first2=Simon |last3=Norouzi |first3=Mohammad |last4=Hinton |first4=Geoffrey |date=2020-11-21 |title=A Simple Framework for Contrastive Learning of Visual Representations |url=https://proceedings.mlr.press/v119/chen20j.html |journal=International Conference on Machine Learning |language=en |publisher=PMLR |pages=1597–1607}}</ref> and clustering.<ref>{{Cite journal |last1=Mathilde |first1=Caron |last2=Ishan |first2=Misra |last3=Julien |first3=Mairal |last4=Priya |first4=Goyal |last5=Piotr |first5=Bojanowski |last6=Armand |first6=Joulin |date=2020 |title=Unsupervised Learning of Visual Features by Contrasting Cluster Assignments |url=https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html |journal=Advances in Neural Information Processing Systems |language=en |volume=33|arxiv=2006.09882 }}</ref>

Examples of generative approaches are Context Encoders, which trains an [[AlexNet]] [[Convolutional neural network|CNN]] architecture to generate a removed image region given the masked image as input,<ref name=":5" /> and iGPT, which applies the [[GPT-2]] language model architecture to images by training on pixel prediction after reducing the [[image resolution]].<ref>{{Cite journal |last1=Chen |first1=Mark |last2=Radford |first2=Alec |last3=Child |first3=Rewon |last4=Wu |first4=Jeffrey |last5=Jun |first5=Heewoo |last6=Luan |first6=David |last7=Sutskever |first7=Ilya |date=2020-11-21 |title=Generative Pretraining From Pixels |url=https://proceedings.mlr.press/v119/chen20s.html |journal=International Conference on Machine Learning |language=en |publisher=PMLR |pages=1691–1703}}</ref>

Many other self-supervised methods use [[siamese networks]], which generate different views of the image through various augmentations that are then aligned to have similar representations. The challenge is avoiding collapsing solutions where the model encodes all images to the same representation.<ref>{{Cite journal |last1=Chen |first1=Xinlei |last2=He |first2=Kaiming |date=2021 |title=Exploring Simple Siamese Representation Learning |url=https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.html |language=en |pages=15750–15758|arxiv=2011.10566 }}</ref> SimCLR is a contrastive approach which uses negative examples in order to generate image representations with a [[Residual neural network|ResNet]] [[Convolutional neural network|CNN]].<ref name=":6" /> Bootstrap Your Own Latent (BYOL) removes the need for negative samples by encoding one of the views with a slow moving average of the model parameters as they are being modified during training.<ref>{{Cite journal |last1=Jean-Bastien |first1=Grill |last2=Florian |first2=Strub |last3=Florent |first3=Altché |last4=Corentin |first4=Tallec |last5=Pierre |first5=Richemond |last6=Elena |first6=Buchatskaya |last7=Carl |first7=Doersch |last8=Bernardo |first8=Avila Pires |last9=Zhaohan |first9=Guo |last10=Mohammad |first10=Gheshlaghi Azar |last11=Bilal |first11=Piot |last12=koray |first12=kavukcuoglu |last13=Remi |first13=Munos |last14=Michal |first14=Valko |date=2020 |title=Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning |url=https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html |journal=Advances in Neural Information Processing Systems |language=en |volume=33}}</ref>

=== Graph ===
The goal of many [[Graph (computer science)|graph]] representation learning techniques is to produce an embedded representation of each [[Vertex (graph theory)|node]] based on the overall [[Topological graph theory|network topology]].<ref>{{Cite journal |last1=Cai |first1=HongYun |last2=Zheng |first2=Vincent W. |last3=Chang |first3=Kevin Chen-Chuan |date=September 2018 |title=A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications |url=https://ieeexplore.ieee.org/document/8294302 |journal=IEEE Transactions on Knowledge and Data Engineering |volume=30 |issue=9 |pages=1616–1637 |doi=10.1109/TKDE.2018.2807452 |arxiv=1709.07604 |s2cid=13999578 |issn=1558-2191}}</ref> [[node2vec]] extends the [[word2vec]] training technique to nodes in a graph by using co-occurrence in [[random walk]]s through the graph as the measure of association.<ref>{{Cite book |last1=Grover |first1=Aditya |last2=Leskovec |first2=Jure |title=Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining |chapter=Node2vec |date=2016-08-13 |chapter-url=https://doi.org/10.1145/2939672.2939754 |series=KDD '16 |volume=2016 |location=New York, NY, USA |publisher=Association for Computing Machinery |pages=855–864 |doi=10.1145/2939672.2939754 |isbn=978-1-4503-4232-2 |pmc=5108654 |pmid=27853626}}</ref> Another approach is to maximize [[mutual information]], a measure of similarity, between the representations of associated structures within the graph.<ref name=":0" /> An example is Deep Graph Infomax, which uses contrastive self-supervision based on mutual information between the representation of a “patch” around each node, and a summary representation of the entire graph. Negative samples are obtained by pairing the graph representation with either representations from another graph in a multigraph training setting, or corrupted patch representations in single graph training.<ref>Velikovi, P., Fedus, W., Hamilton, W. L., Li, P., Bengio, Y., and Hjelm, R. D. [https://openreview.net/pdf?id=rklz9iAcKQ Deep Graph InfoMax.] In International Conference on Learning Representations (ICLR’2019), 2019.</ref>

=== Video ===
With analogous results in masked prediction<ref>{{Cite journal |last1=Luo |first1=Dezhao |last2=Liu |first2=Chang |last3=Zhou |first3=Yu |last4=Yang |first4=Dongbao |last5=Ma |first5=Can |last6=Ye |first6=Qixiang |last7=Wang |first7=Weiping |date=2020-04-03 |title=Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning |url=https://ojs.aaai.org/index.php/AAAI/article/view/6840 |journal=Proceedings of the AAAI Conference on Artificial Intelligence |language=en |volume=34 |issue=7 |pages=11701–11708 |doi=10.1609/aaai.v34i07.6840 |s2cid=209531629 |issn=2374-3468|doi-access=free |arxiv=2001.00294 }}</ref> and clustering,<ref>{{Cite journal |last1=Humam |first1=Alwassel |last2=Dhruv |first2=Mahajan |last3=Bruno |first3=Korbar |last4=Lorenzo |first4=Torresani |last5=Bernard |first5=Ghanem |last6=Du |first6=Tran |date=2020 |title=Self-Supervised Learning by Cross-Modal Audio-Video Clustering |url=https://proceedings.neurips.cc/paper/2020/hash/6f2268bd1d3d3ebaabb04d6b5d099425-Abstract.html |journal=Advances in Neural Information Processing Systems |language=en |volume=33|arxiv=1911.12667 }}</ref> video representation learning approaches are often similar to image techniques but must utilize the temporal sequence of video frames as an additional learned structure. Examples include VCP, which masks video clips and trains to choose the correct one given a set of clip options, and Xu et al., who train a 3D-CNN to identify the original order given a shuffled set of video clips.<ref>{{Cite book |last1=Xu |first1=Dejing |last2=Xiao |first2=Jun |last3=Zhao |first3=Zhou |last4=Shao |first4=Jian |last5=Xie |first5=Di |last6=Zhuang |first6=Yueting |title=2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) |chapter=Self-Supervised Spatiotemporal Learning via Video Clip Order Prediction |date=June 2019 |chapter-url=https://ieeexplore.ieee.org/document/8953292 |pages=10326–10335 |doi=10.1109/CVPR.2019.01058|isbn=978-1-7281-3293-8 |s2cid=195504152 }}</ref>

=== Audio ===
Self-supervised representation techniques have also been applied to many audio data formats, particularly for [[speech processing]].<ref name=":0" /> Wav2vec 2.0 discretizes the [[Waveform|audio waveform]] into timesteps via temporal [[Convolutional neural network|convolutions]], and then trains a [[Transformer (machine learning model)|transformer]] on masked prediction of random timesteps using a contrastive loss.<ref name=":8">{{Cite journal |last1=Alexei |first1=Baevski |last2=Yuhao |first2=Zhou |last3=Abdelrahman |first3=Mohamed |last4=Michael |first4=Auli |date=2020 |title=wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations |url=https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html |journal=Advances in Neural Information Processing Systems |language=en |volume=33|arxiv=2006.11477 }}</ref> This is similar to the [[BERT (language model)|BERT language model]], except as in many SSL approaches to video, the model chooses among a set of options rather than over the entire word vocabulary.<ref name=":7" /><ref name=":8" />

=== Multimodal ===
Self-supervised learning has also been used to develop joint representations of multiple data types.<ref name=":0" /> Approaches usually rely on some natural or human-derived association between the modalities as an implicit label, for instance video clips of animals or objects with characteristic sounds,<ref name=":9">{{Cite journal |last1=Zellers |first1=Rowan |last2=Lu |first2=Jiasen |last3=Lu |first3=Ximing |last4=Yu |first4=Youngjae |last5=Zhao |first5=Yanpeng |last6=Salehi |first6=Mohammadreza |last7=Kusupati |first7=Aditya |last8=Hessel |first8=Jack |last9=Farhadi |first9=Ali |last10=Choi |first10=Yejin |date=2022 |title=MERLOT Reserve: Neural Script Knowledge Through Vision and Language and Sound |url=https://openaccess.thecvf.com/content/CVPR2022/html/Zellers_MERLOT_Reserve_Neural_Script_Knowledge_Through_Vision_and_Language_and_CVPR_2022_paper.html |language=en |pages=16375–16387|arxiv=2201.02639 }}</ref> or captions written to describe images.<ref name="Radford 8748–8763">{{Cite journal |last1=Radford |first1=Alec |last2=Kim |first2=Jong Wook |last3=Hallacy |first3=Chris |last4=Ramesh |first4=Aditya |last5=Goh |first5=Gabriel |last6=Agarwal |first6=Sandhini |last7=Sastry |first7=Girish |last8=Askell |first8=Amanda |last9=Mishkin |first9=Pamela |last10=Clark |first10=Jack |last11=Krueger |first11=Gretchen |last12=Sutskever |first12=Ilya |date=2021-07-01 |title=Learning Transferable Visual Models From Natural Language Supervision |url=https://proceedings.mlr.press/v139/radford21a.html |journal=International Conference on Machine Learning |language=en |publisher=PMLR |pages=8748–8763|arxiv=2103.00020 }}</ref> CLIP produces a joint image-text representation space by training to align image and text encodings from a large dataset of image-caption pairs using a contrastive loss.<ref name="Radford 8748–8763"/> MERLOT Reserve trains a transformer-based encoder to jointly represent audio, subtitles and video frames from a large dataset of videos through 3 joint pretraining tasks: contrastive masked prediction of either audio or text segments given the video frames and surrounding audio and text context, along with contrastive alignment of video frames with their corresponding captions.<ref name=":9" />

[[Multimodal learning|Multimodal]] representation models are typically unable to assume direct correspondence of representations in the different modalities, since the precise alignment can often be noisy or ambiguous. For example, the text "dog" could be paired with many different pictures of dogs, and correspondingly a picture of a dog could be captioned with varying degrees of specificity. This limitation means that downstream tasks may require an additional generative mapping network between modalities to achieve optimal performance, such as in [[DALL-E|DALLE-2]] for text to image generation.<ref>{{cite arXiv |last1=Ramesh |first1=Aditya |last2=Dhariwal |first2=Prafulla |last3=Nichol |first3=Alex |last4=Chu |first4=Casey |last5=Chen |first5=Mark |date=2022-04-12 |title=Hierarchical Text-Conditional Image Generation with CLIP Latents |class=cs.CV |eprint=2204.06125 }}</ref>

== Dynamic Representation Learning ==
Dynamic representation learning methods<ref>{{Cite journal |last1=Zhang |first1=Daokun |last2=Yin |first2=Jie |last3=Zhu |first3=Xingquan |last4=Zhang |first4=Chengqi |date=March 2020 |title=Network Representation Learning: A Survey |url=https://ieeexplore.ieee.org/document/8395024 |journal=IEEE Transactions on Big Data |volume=6 |issue=1 |pages=3–28 |doi=10.1109/TBDATA.2018.2850013 |s2cid=1479507 |issn=2332-7790|arxiv=1801.05852 }}</ref> generate latent embeddings for dynamic systems such as dynamic networks. Since particular distance functions are invariant under particular linear transformations, different sets of embedding vectors can actually represent the same/similar information. Therefore, for a dynamic system, a temporal difference in its embeddings may be explained by misalignment of embeddings due to arbitrary transformations and/or actual changes in the system.<ref>{{Cite journal |last1=Gürsoy |first1=Furkan |last2=Haddad |first2=Mounir |last3=Bothorel |first3=Cécile |date=2023-10-07 |title=Alignment and stability of embeddings: Measurement and inference improvement |url=https://www.sciencedirect.com/science/article/pii/S0925231223006409 |journal=Neurocomputing |language=en |volume=553 |pages=126517 |doi=10.1016/j.neucom.2023.126517 |arxiv=2101.07251 |s2cid=231632462 |issn=0925-2312}}</ref> Therefore, generally speaking, temporal embeddings learned via dynamic representation learning methods should be inspected for any spurious changes and be aligned before consequent dynamic analyses.

==See also==
* [[Automated machine learning]] (AutoML)
* [[Deep learning]]
* [[geometric feature learning]]
* [[Feature detection (computer vision)]]
* [[Feature extraction]]
* [[Word embedding]]
* [[Vector quantization]]
* [[Variational autoencoder]]

==References==
{{Reflist|30em}}

[[Category:Machine learning]]