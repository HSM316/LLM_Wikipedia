{{short description|Online platform}}
[[File:Moral Machine Screenshot.png|thumb|Screenshot of a Moral Machine dilemma]]
'''Moral Machine''' is an online platform, developed by [[Iyad Rahwan]]'s Scalable Cooperation group at the [[Massachusetts Institute of Technology]], that generates [[Ethical dilemma|moral dilemmas]] and collects information on the decisions that people make between two destructive outcomes.<ref name=":1">{{Cite news|url=http://www.nbcnews.com/tech/innovation/driverless-cars-moral-dilemma-who-lives-who-dies-n708276|title=Driverless cars face a moral dilemma: Who lives and who dies?|newspaper=NBC News|access-date=2017-02-16|language=en}}</ref><ref>{{Cite news|url=http://www.slate.com/blogs/future_tense/2016/08/11/moral_machine_from_mit_poses_self_driving_car_thought_experiments.html|title=Should a Self-Driving Car Kill Two Jaywalkers or One Law-Abiding Citizen?|last=Brogan|first=Jacob|date=2016-08-11|newspaper=Slate|access-date=2017-02-16|language=en-US|issn=1091-2339}}</ref> The platform is the idea of Iyad Rahwan and social psychologists Azim Shariff and Jean-François Bonnefon,<ref>{{Cite web|url=https://socialsciences.nature.com/users/182414-edmond-awad/posts/40067-inside-the-moral-machine|title=Inside the Moral Machine|last=Awad|first=Edmond|date=2018-10-24|website=Behavioural and Social Sciences at Nature Research|language=en|archive-url=|archive-date=|access-date=2019-07-04}}</ref> who conceived of the idea ahead of the publication of their article about the ethics of self-driving cars.<ref>{{Cite journal|last1=Bonnefon|first1=Jean-François|last2=Shariff|first2=Azim|last3=Rahwan|first3=Iyad|date=2016-06-24|title=The social dilemma of autonomous vehicles|journal=Science|language=en|volume=352|issue=6293|pages=1573–1576|doi=10.1126/science.aaf2654|issn=0036-8075|pmid=27339987|arxiv=1510.03346|bibcode=2016Sci...352.1573B|s2cid=35400794 }}</ref> The key contributors to building the platform were MIT [[MIT Media Lab|Media Lab]] graduate students Edmond Awad and Sohan Dsouza.

The presented scenarios are often variations of the [[trolley problem]], and the information collected would be used for further research regarding the decisions that [[Artificial intelligence|machine intelligence]] must make in the future.<ref>{{Cite web|url=https://www.media.mit.edu/research/groups/10005/moral-machine|title=Moral Machine {{!}} MIT Media Lab|website=www.media.mit.edu|language=en|access-date=2017-02-16|archive-url=https://web.archive.org/web/20161130114500/http://media.mit.edu/research/groups/10005/moral-machine|archive-date=2016-11-30|url-status=dead}}</ref><ref>{{Cite news|url=http://learningenglish.voanews.com/a/mit-moral-machine/3556873.html|title=MIT Seeks 'Moral' to the Story of Self-Driving Cars|newspaper=VOA|access-date=2017-02-16|language=en}}</ref><ref name=":2">{{Cite web|url=http://moralmachine.mit.edu/|title=Moral Machine|website=Moral Machine|access-date=2017-02-16}}</ref><ref>{{Cite news|url=https://thenextweb.com/cars/2017/01/16/mits-moral-machine-wants-you-to-decide-who-dies-in-self-driving-car-accidents/|title=MIT's 'Moral Machine' wants you to decide who dies in a self-driving car accident|last=Clark|first=Bryan|date=2017-01-16|newspaper=The Next Web|access-date=2017-02-16|language=en-US}}</ref><ref>{{Cite news|url=http://www.popsci.com/mit-game-asks-who-driverless-cars-should-kill|title=MIT Game Asks Who Driverless Cars Should Kill|newspaper=Popular Science|access-date=2017-02-16|language=en}}</ref><ref>{{Cite web|url=https://techcrunch.com/2016/10/04/did-you-save-the-cat-or-the-kid/|title=Play this killer self-driving car ethics game|last=Constine|first=Josh|website=TechCrunch|date=4 October 2016 |access-date=2017-02-16}}</ref> For example, as artificial intelligence plays an increasingly significant role in [[autonomous driving]] technology, research projects like Moral Machine help to find solutions for challenging life-and-death decisions that will face self-driving vehicles.<ref>{{Cite web|url=http://fortune.com/2017/07/22/driverless-cars-autonomous-vehicles-self-driving-uber-google-tesla/|title=What's Taking So Long for Driverless Cars to Go Mainstream?|last=Chopra|first=Ajay|website=Fortune|access-date=2017-08-01}}</ref>

Moral Machine was active from January 2016 to July 2020. The Moral Machine continues to be available on their website for people to experience.<ref name=":1" /><ref name=":2" />

== The experiment ==
The Moral Machine was an ambitious project; it was the first attempt at using such an experimental design to test a large number of humans in over 200 countries worldwide. The study was approved by the Institute Review Board (IRB) at Massachusetts Institute of Technology (MIT). <ref name=":0">{{Cite web |title=Moral Machine |url=http://moralmachine.mit.edu/ |access-date=2022-04-13 |website=Moral Machine}}</ref><ref name="Awad2018">{{cite journal |last1=Awad |first1=Edmond |last2=Dsouza |first2=Sohan |last3=Kim |first3=Richard |last4=Schulz |first4=Jonathan |last5=Henrich |first5=Joseph |last6=Shariff |first6=Azim |last7=Bonnefon |first7=Jean-François |last8=Rahwan |first8=Iyad |date=24 October 2018 |title=The Moral Machine experiment |journal=Nature |volume=563 |issue=7729 |pages=59–64 |bibcode=2018Natur.563...59A |doi=10.1038/s41586-018-0637-6 |pmid=30356211 |hdl-access=free |hdl=10871/39187|s2cid=256770099 }}</ref>

The setup of the experiment asks the viewer to make a decision on a single scenario in which a self-driving car is about to hit pedestrians. The user can decide to have the car either swerve to avoid hitting the pedestrians or keep going straight to preserve the lives it is transporting.

Participants can complete as many scenarios as they want to, however the scenarios themselves are generated in groups of thirteen. Within this thirteen, a single scenario is entirely random while the other twelve are generated from a space in a database of 26 million different possibilities. They are chosen with two dilemmas focused on each of six dimensions of moral preferences: character gender, character age, character physical fitness, character social status, character species, and character number.<ref name=":2" /><ref name="Awad2018" />

The experiment setup remains the same throughout multiple scenarios but each scenario tests a different set of factors. Most notably, the characters involved in the scenario are different in each one. Characters may include ones such as: Stroller, girl, boy, pregnant, Male Doctor, Female Doctor, Female Athlete, Executive Female, Male Athlete, Executive Male, Large Woman, Large Man, homeless, old man, old woman, dog, criminal, and a cat.<ref name=":2" />

Through these different characters researchers were able to understand how a wide variety of people will judge scenarios based on those involved.

== Analysis ==
The Moral Machine collected 40 million moral decisions from 4 million participants in 233 countries,<ref name="verge">{{cite web |last1=Vincent |first1=James |title=Global preferences for who to save in self-driving car crashes revealed |url=https://www.theverge.com/2018/10/24/18013392/self-driving-car-ethics-dilemma-mit-study-moral-machine-results |website=The Verge |publisher=Vox Media |access-date=3 August 2024 |language=en |date=24 October 2018}}</ref><ref name="Karlsson">{{cite journal |last1=Karlsson |first1=Carl-Johan |date=7 July 2021 |title=What Sweden's Covid failure tells us about ageism |url=https://knowablemagazine.org/article/society/2021/what-swedens-covid-failure-tells-us-about-ageism |journal=Knowable Magazine |doi=10.1146/knowable-070621-1 |access-date=9 December 2021 |doi-access=free}}</ref><ref name="forbes" /> analysis of which revealed trends within individual countries and humanity as a whole. It tested for nine factors: preference for sparing humans versus pets, passengers versus pedestrians, men versus women, young versus elderly, fit versus overweight, higher versus lower social status, jaywalkers versus law abiders, larger versus smaller groups, and inaction (i.e. staying on course) versus swerving.<ref name="Awad2018" />

Globally, participants favored human lives over lives of animals like dogs and cats. They preferred to spare more lives if possible, and younger lives as opposed to older.<ref name="forbes">{{cite web |last1=Smith |first1=Oliver |title=A Huge Global Study On Driverless Car Ethics Found The Elderly Are Expendable |url=https://www.forbes.com/sites/oliversmith/2018/03/21/the-results-of-the-biggest-global-study-on-driverless-car-ethics-are-in/ |website=Forbes |access-date=3 August 2024 |language=en}}</ref> Babies were most often spared with cats being the least spared. In terms of gender variations, people tended to spare men over women for doctors and the elderly. All countries generally shared the preference to spare pedestrians over passengers and law-abiders over criminals.

Participants from less wealthy countries showed a higher tendency of sparing pedestrians who crossed illegally compared to those from more wealthy and developed countries. This is most likely due to their experience living in a society where individuals are more likely to deviate from rules due to less stringent enforcement of laws.  Countries of higher [[economic inequality]] overwhelmingly prefer to save wealthier individuals over poorer ones.<ref name="Awad2018" /> 

=== Cultural differences ===
Researchers subdivided 130 countries with similar results into three ‘cultural clusters’.
North America and European countries with significant Christian populations had a higher preference for inaction on the part of the driver and thus had less of a preference for sparing pedestrians as compared to other clusters.
East Asian and Islamic countries, together constituting the second cluster, did not have as much preference to spare younger humans compared to the other two clusters and had a higher preference for sparing law-abiding humans.
Latin America and [[Francophonie|Francophone countries]] had a higher preference for sparing women, the young, the fit, and those of higher status, but a lower preference for sparing humans over pets or other animals.<ref name="Awad2018" /><ref name="forbes" />

Individualistic cultures tended to spare larger groups, and collectivist cultures had a stronger preference for sparing the lives of older people. For instance, China ranked far below the world average for preference to spare the younger over elderly, while the average respondent from the US exhibited a much higher tendency to save younger lives and larger groups.<ref name="Awad2018" />

== Applications of the data ==
The findings from the moral machine can help decision makers when designing self-driving automotive systems. Designers must make sure that these vehicles are able to solve problems on the road that aligns with the moral values of humans around it. <ref name="Awad2018" /><ref name=":0" />

This is a challenge because of the complex nature of humans who may all make different decisions based on their personal values. However, by collecting a large amount of decisions from humans all over the world, researchers can begin to understand patterns in the context of a particular culture, community, and people.

== Other features ==
The Moral Machine was deployed in June 2016. In October 2016, a feature was added that offered users the option to fill a survey about their demographics, political views, and religious beliefs. Between November 2016 and March 2017, the website was progressively translated into nine languages in addition to English (Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, and Spanish).<ref name=":0" />

Overall, the Moral Machine offers four different modes (see Supplementary Information), with  the focus being on the data-gathering feature of the website, called the Judge mode. <ref name=":0" />

This means that the Moral Machine, in addition to providing their own scenarios for users to judge, also invites users to create their own scenarios to be submitted and approved so that other people may also judge those scenarios. Data is also open sourced for anyone to explore via an interactive map that is featured on the Moral Machine website.

== In the literature ==
Studies and research on the Moral Machine have taken a wide variety of approaches. However, theological examinations of the topic are still scarce where two bodies of work that examine such perspective currently exist in this regard: One is Buddhist<ref>{{Cite book |last=Hongladarom |first=Soraj |title=The ethics of AI and robotics: A buddhist viewpoint |date=2020 |publisher=Lexington Books |year=2020 |isbn=978-1498597296}}</ref> while the other is Christian.<ref>{{Cite book |last=Crook |first=Nigel |title=Rise of the Moral Machine: Exploring Virtue Through a Robot's Eyes |date=2022 |publisher=Nigel T. Crook |year=2022 |isbn=978-1739133900}}</ref> 

==References==
{{reflist}}

==External links==
*{{official|http://moralmachine.mit.edu}}

[[Category:Dilemmas]]
[[Category:Thought experiments in ethics]]
[[Category:AI software]]
[[Category:Massachusetts Institute of Technology]]
[[Category:MIT Media Lab]]
[[Category:Moral psychology]]