{{Short description|Meta-algorithmic technique to choose an algorithm}}
{{Use dmy dates|date=September 2017}}

'''Algorithm selection''' (sometimes also called '''per-instance algorithm selection''' or '''offline algorithm selection''') is a meta-[[algorithmic technique]] to choose an algorithm from a portfolio on an instance-by-instance basis. It is motivated by the observation that on many practical problems, different algorithms have different performance characteristics. That is, while one algorithm performs well in some scenarios, it performs poorly in others and vice versa for another algorithm. If we can identify when to use which algorithm, we can optimize for each scenario and improve overall performance. This is what algorithm selection aims to do. The only prerequisite for applying algorithm selection techniques is that there exists (or that there can be constructed) a set of complementary algorithms.

== Definition==

Given a portfolio <math>\mathcal{P}</math> of algorithms <math>\mathcal{A} \in \mathcal{P}</math>, a set of instances <math>i \in \mathcal{I}</math> and a cost metric <math>m: \mathcal{P} \times \mathcal{I} \to \mathbb{R}</math>, the algorithm selection problem consists of finding a mapping <math>s: \mathcal{I} \to \mathcal{P}</math> from instances <math>\mathcal{I}</math> to algorithms <math>\mathcal{P}</math> such that the cost <math> \sum_{i \in \mathcal{I}} m(s(i),i)</math> across all instances is optimized.<ref>{{Cite book |doi = 10.1016/S0065-2458(08)60520-3|chapter = The Algorithm Selection Problem |volume = 15|pages = 65–118|title= Advances in Computers |year = 1976|last1 = Rice|first1 = John R.|isbn = 9780120121151|url = https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1098&context=cstech }}</ref><ref>{{Cite journal |doi = 10.1016/j.artint.2016.04.003|title = ASlib: A benchmark library for algorithm selection|journal = Artificial Intelligence|volume = 237|pages = 41–58|year = 2016|last1 = Bischl|first1 = Bernd|last2 = Kerschke|first2 = Pascal|last3 = Kotthoff|first3 = Lars|last4 = Lindauer|first4 = Marius|last5 = Malitsky|first5 = Yuri|last6 = Fréchette|first6 = Alexandre|last7 = Hoos|first7 = Holger|last8 = Hutter|first8 = Frank|last9 = Leyton-Brown|first9 = Kevin|last10 = Tierney|first10 = Kevin|last11 = Vanschoren|first11 = Joaquin|arxiv = 1506.02465| s2cid=261945 }}</ref>

== Examples==

=== Boolean satisfiability problem (and other hard combinatorial problems)===

A well-known application of algorithm selection is the [[Boolean satisfiability problem]]. Here, the portfolio of algorithms is a set of (complementary) [[SAT solver]]s, the instances are Boolean formulas, the cost metric is for example average runtime or number of unsolved instances. So, the goal is to select a well-performing SAT solver for each individual instance. In the same way, algorithm selection can be applied to many other <math>\mathcal{NP}</math>-hard problems (such as [[Linear programming|mixed integer programming]], [[Constraint satisfaction problem|CSP]], [[Automated planning and scheduling|AI planning]], [[Travelling salesman problem|TSP]], [[MAXSAT]], [[QBF]] and [[answer set programming]]). Competition-winning systems in SAT are SATzilla,<ref name="zilla08">{{Cite journal|author=L. Xu|author2=F. Hutter|author3=H. Hoos|name-list-style=amp|author4=K. Leyton-Brown|date=2008|title=SATzilla: Portfolio-based Algorithm Selection for SAT|journal=Journal of Artificial Intelligence Research|volume=32|pages=565–606|arxiv=1111.2249|doi=10.1613/jair.2490|s2cid=10987043 }}</ref> 3S<ref>{{Cite book|author1=S. Kadioglu |author2=Y. Malitsky |author3=A. Sabharwal |author4=H. Samulowitz |author5=M. Sellmann |date=2011|chapter=Algorithm Selection and Scheduling|title=Principles and Practice of Constraint Programming|volume=6876 |pages=454–469 |doi=10.1007/978-3-642-23786-7_35|editor=Lee, J.|series=Lecture Notes in Computer Science |isbn=978-3-642-23785-0 |citeseerx=10.1.1.211.1807 }}</ref> and CSHC<ref name="CSHC">{{Cite book|author1=Y. Malitsky |author2=A. Sabharwal |author3=H. Samulowitz |author4=M. Sellmann |date=2013|chapter=Algorithm Portfolios Based on Cost-Sensitive Hierarchical Clustering|title=Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence|pages=608–614|isbn=978-1-57735-633-2}}</ref>

=== Machine learning===

In [[machine learning]], algorithm selection is better known as [[meta-learning (computer science)|meta-learning]]. The portfolio of algorithms consists of machine learning algorithms (e.g., Random Forest, SVM, DNN), the instances are data sets and the cost metric is for example the error rate. So, the goal is to predict which machine learning algorithm will have a small error on each data set.

== Instance features==

The algorithm selection problem is mainly solved with machine learning techniques. By representing the problem instances by numerical features <math>f</math>, algorithm selection can be seen as a [[multi-class classification]] problem by learning a mapping <math> f_{i} \mapsto \mathcal{A}</math> for a given instance <math>i</math>.

Instance features are numerical representations of instances. For example, we can count the number of variables, clauses, average clause length for Boolean formulas,<ref>{{Cite journal|author1=E. Nudelman |author2=K. Leyton-Brown |author3=H. Hoos |author4=A. Devkar |author5=Y. Shoham |date=2004|title=Understanding Random SAT: Beyond the Clauses-to-Variables Ratio|url=https://ai.stanford.edu/~shoham/www%20papers/CP04randomsat.pdf|journal=Proceedings of CP}}</ref> or number of samples, features, class balance for ML data sets to get an impression about their characteristics.

=== Static vs. probing features===

We distinguish between two kinds of features: 
# Static features are in most cases some counts and statistics (e.g., clauses-to-variables ratio in SAT). These features ranges from very cheap features (e.g. number of variables) to very complex features (e.g., statistics about variable-clause graphs).
# Probing features (sometimes also called landmarking features) are computed by running some analysis of algorithm behavior on an instance (e.g., accuracy of a cheap decision tree algorithm on an ML data set, or running for a short time a stochastic local search solver on a Boolean formula). These feature often cost more than simple static features.

=== Feature costs===

Depending on the used performance metric <math> m </math>, feature computation can be associated with costs.
For example, if we use running time as performance metric, we include the time to compute our instance features into the performance of an algorithm selection system.
SAT solving is a concrete example, where such feature costs cannot be neglected, since instance features for [[Conjunctive normal form|CNF]] formulas can be either very cheap (e.g., to get the number of variables can be done in constant time for CNFs in the DIMACs format) or very expensive (e.g., graph features which can cost tens or hundreds of seconds).

It is important to take the overhead of feature computation into account in practice in such scenarios; otherwise a misleading impression of the performance of the algorithm selection approach is created. For example, if the decision which algorithm to choose can be made with perfect accuracy, but the features are the running time of the portfolio algorithms, there is no benefit to the portfolio approach. This would not be obvious if feature costs were omitted.

== Approaches==

=== Regression approach===

One of the first successful algorithm selection approaches predicted the performance of each algorithm <math>\hat{m}_{\mathcal{A}}: \mathcal{I} \to \mathbb{R}</math> and selected the algorithm with the best predicted performance <math>arg\min_{\mathcal{A}\in\mathcal{P}} \hat{m}_{\mathcal{A}}(i) </math> for an instance <math>i</math>.<ref name="zilla08" />

=== Clustering approach===

A common assumption is that the given set of instances <math>\mathcal{I}</math> can be clustered into homogeneous subsets 
and for each of these subsets, there is one well-performing algorithm for all instances in there.
So, the training consists of identifying the homogeneous clusters via an unsupervised clustering approach and associating an algorithm with each cluster.
A new instance is assigned to a cluster and the associated algorithm selected.<ref>{{Cite journal|author1=S. Kadioglu |author2=Y. Malitsky |author3=M. Sellmann |author4=K. Tierney |date=2010|title=ISAC – Instance-Specific Algorithm Configuration|url=https://wiwi.uni-paderborn.de/fileadmin/dep3ls7/Downloads/Publikationen/PDFs/isac-ecai2010.pdf|journal=Proceedings of the European Conference on Artificial Intelligence}}</ref>

A more modern approach is cost-sensitive [[hierarchical clustering]]<ref name="CSHC"/> using supervised learning to identify the homogeneous instance subsets.

=== Pairwise cost-sensitive classification approach===

A common approach for multi-class classification is to learn pairwise models between every pair of classes (here algorithms) 
and choose the class that was predicted most often by the pairwise models.
We can weight the instances of the pairwise prediction problem by the performance difference between the two algorithms.
This is motivated by the fact that we care most about getting predictions with large differences correct, but the penalty for an incorrect prediction is small if there is almost no performance difference.
Therefore, each instance <math>i</math> for training a classification model <math>\mathcal{A}_1</math> vs <math>\mathcal{A}_2</math> is associated with a cost <math>|m(\mathcal{A}_1,i) - m(\mathcal{A}_2,i)| </math>.<ref>{{Cite journal|author1=L. Xu |author2=F. Hutter |author3=J. Shen |author4=H. Hoos |author5=K. Leyton-Brown |date=2012|title=SATzilla2012: Improved Algorithm Selection Based on Cost-sensitive Classification Models|url=http://www.academia.edu/download/30605172/sc2012_proceedings.pdf#page=58|journal=Proceedings of the SAT Challenge 2012: Solver and Benchmark Descriptions}}{{dead link|date=July 2022|bot=medic}}{{cbignore|bot=medic}}</ref>

== Requirements==
[[File:Portfolio correlation as.png|thumb|Clustering of SAT solvers from SAT12-INDU ASlib scenario according to the correlation coefficient of spearman.]]
[[File:Shapley Values on SAT12-INDU ASlib Scenario.png|thumb|Shapley values for complementary analysis on SAT12-INDU ASlib Scenario<ref>{{Cite journal|author=A. Frechette|author2=L. Kotthoff|author3=T. Michalak|author4=T. Rahwan|author5=H. Hoos|author6=K. Leyton-Brown|name-list-style=amp|date=2016|title=Using the Shapley Value to Analyze Algorithm Portfolios|url=https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12495/12107|journal=Proceedings of the International Conference on AAAI|volume=30 |doi=10.1609/aaai.v30i1.10440 |s2cid=6676831 |doi-access=free}}</ref>]]
The algorithm selection problem can be effectively applied under the following assumptions:
* The portfolio <math>\mathcal{P}</math> of algorithms is complementary with respect to the instance set <math>\mathcal{I}</math>, i.e.,  there is no single algorithm <math>\mathcal{A} \in \mathcal{P}</math> that dominates the performance of all other algorithms over <math>\mathcal{I}</math> (see figures to the right for examples on complementary analysis).
* In some application, the computation of instance features is associated with a cost. For example, if the cost metric is running time, we have also to consider the time to compute the instance features. In such cases, the cost to compute features should not be larger than the performance gain through algorithm selection.

== Application domains==

Algorithm selection is not limited to single domains but can be applied to any kind of algorithm if the above requirements are satisfied.
Application domains include:

* hard combinatorial problems:<ref>Kotthoff, Lars. "[https://www.aaai.org/ojs/index.php/aimagazine/article/download/2460/2438 Algorithm selection for combinatorial search problems: A survey]." Data Mining and Constraint Programming. Springer, Cham, 2016. 149-190.</ref> [[Boolean satisfiability problem|SAT]], [[Linear programming|Mixed Integer Programming]], [[Constraint satisfaction problem|CSP]], [[Automated planning and scheduling|AI Planning]], [[Travelling salesman problem|TSP]], [[MAXSAT]], [[QBF]] and [[Answer Set Programming]]
* combinatorial auctions
* in machine learning, the problem is known as [[meta-learning (computer science)|meta-learning]]
* software design
* black-box optimization
* [[multi-agent system]]s
* numerical optimization
* linear algebra, differential equations
* [[evolutionary algorithm]]s
* [[vehicle routing problem]]
* power systems

For an extensive list of literature about algorithm selection, we refer to a literature overview.

== Variants of algorithm selection==

=== Online selection===

Online algorithm selection refers to switching between different algorithms during the solving process.  This is useful as a [[hyper-heuristic]]. In contrast, offline algorithm selection selects an algorithm for a given instance only once and before the solving process.

=== Computation of schedules===

An extension of algorithm selection is the per-instance algorithm scheduling problem, in which we do not select only one solver, but we select a time budget for each algorithm on a per-instance base. This approach improves the performance of selection systems in particular if the instance features are not very informative and a wrong selection of a single solver is likely.<ref>{{Cite journal|author1=M. Lindauer|author2=R. Bergdoll  |author3=F. Hutter|date=2016|title=An Empirical Study of Per-Instance Algorithm Scheduling|url=http://ml.informatik.uni-freiburg.de/papers/16-LION-ASschedules.pdf|journal=Proceedings of the International Conference on Learning and Intelligent Optimization|series=Lecture Notes in Computer Science|volume=10079|pages=253–259|doi=10.1007/978-3-319-50349-3_20|isbn=978-3-319-50348-6}}</ref>

=== Selection of parallel portfolios===

Given the increasing importance of parallel computation,
an extension of algorithm selection for parallel computation is parallel portfolio selection,
in which we select a subset of the algorithms to simultaneously run in a parallel portfolio.<ref>{{Cite journal|author=M. Lindauer|author2=H. Hoos|name-list-style=amp|author3=F. Hutter|date=2015|title=From Sequential Algorithm Selection to Parallel Portfolio Selection|url=https://ml.informatik.uni-freiburg.de/papers/15-LION-ParallelAS.pdf|journal= Proceedings of the International Conference on Learning and Intelligent Optimization|series=Lecture Notes in Computer Science|volume=8994|pages=1–16|doi=10.1007/978-3-319-19084-6_1|isbn=978-3-319-19083-9}}</ref>

== External links==

* [http://www.coseal.net/aslib/ Algorithm Selection Library (ASlib)]
* [https://larskotthoff.github.io/assurvey/ Algorithm selection literature]

== References==
{{Reflist}}

[[Category:Machine learning]]
[[Category:Constraint programming]]