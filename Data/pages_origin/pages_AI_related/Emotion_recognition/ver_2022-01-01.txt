{{Short description|Process of visually interpreting emotions}}
'''Emotion recognition''' is the process of identifying human [[emotion]].  People vary widely in their accuracy at recognizing the emotions of others.  Use of technology to help people with emotion recognition is a relatively nascent research area.  Generally, the technology works best if it uses multiple [[Modality (human–computer interaction)|modalities]] in context.  To date, the most work has been conducted on automating the [[Facial expression recognition|recognition of facial expression]]s from video, spoken expressions from audio, written expressions from text, and [[physiology]] as measured by wearables.

==Human==
{{main|Emotion perception}}
Humans show a great deal of variability in their abilities to recognize emotion. A key point to keep in mind when learning about automated emotion recognition is that there are several sources of "ground truth," or truth about what the real emotion is.  Suppose we are trying to recognize the emotions of Alex.  One source is "what would most people say that Alex is feeling?"  In this case, the 'truth' may not correspond to what Alex feels, but may correspond to what most people would say it looks like Alex feels.  For example, Alex may actually feel sad, but he puts on a big smile and then most people say he looks happy.  If an automated method achieves the same results as a group of observers it may be considered accurate, even if it does not actually measure what Alex truly feels.  Another source of 'truth' is to ask Alex what he truly feels.  This works if Alex has a good sense of his internal state, and wants to tell you what it is, and is capable of putting it accurately into words or a number.  However, some people are alexithymic and do not have a good sense of their internal feelings, or they are not able to communicate them accurately with words and numbers.  In general, getting to the truth of what emotion is actually present can take some work, can vary depending on the criteria that are selected, and will usually involve maintaining some level of uncertainty.

==Automatic==
Decades of scientific research have been conducted developing and evaluating methods for automated emotion recognition. There is now an extensive literature proposing and evaluating hundreds of different kinds of methods, leveraging techniques from multiple areas, such as [[signal processing]], [[machine learning]], [[computer vision]], and [[speech processing]]. Different methodologies and techniques may be employed to interpret emotion such as [[Bayesian network]]s.<ref>Miyakoshi, Yoshihiro, and Shohei Kato. [https://ieeexplore.ieee.org/document/5958891 "Facial Emotion Detection Considering Partial Occlusion Of Face Using Baysian Network"]. Computers and Informatics (2011): 96–101.</ref>
, Gaussian [[Mixture model]]s<ref>Hari Krishna Vydana, P. Phani Kumar, K. Sri Rama Krishna and Anil Kumar Vuppala. [https://ieeexplore.ieee.org/document/7058214/references "Improved emotion recognition using GMM-UBMs"]. 2015 International Conference on Signal Processing and Communication Engineering Systems</ref> and [[Hidden Markov model|Hidden Markov Models]]<ref>B. Schuller, G. Rigoll M. Lang. [https://ieeexplore.ieee.org/document/1220939/ "Hidden Markov model-based speech emotion recognition"]. ICME '03. Proceedings. 2003 International Conference on Multimedia and Expo, 2003.</ref> and [[deep neural networks]].<ref>{{Cite book |doi=10.1109/ICCCI50826.2021.9402569|isbn=978-1-7281-5875-4|chapter=Non-linear frequency warping using constant-Q transformation for speech emotion recognition|title=2021 International Conference on Computer Communication and Informatics (ICCCI)|pages=1–4|year=2021|last1=Singh|first1=Premjeet|last2=Saha|first2=Goutam|last3=Sahidullah|first3=Md|arxiv=2102.04029}}</ref>

===Approaches===

The accuracy of emotion recognition is usually improved when it combines the analysis of human expressions from multimodal forms such as texts, physiology, audio, or video.<ref>{{cite journal |last1=Poria |first1=Soujanya |last2=Cambria |first2=Erik |last3=Bajpai |first3=Rajiv |last4=Hussain |first4=Amir |title=A review of affective computing: From unimodal analysis to multimodal fusion |journal=Information Fusion |date=September 2017 |volume=37 |pages=98–125 |doi=10.1016/j.inffus.2017.02.003|hdl=1893/25490 |url=http://researchrepository.napier.ac.uk/Output/1792429 |hdl-access=free }}</ref> Different [[emotion]] types are detected through the integration of information from [[facial expressions]], body movement and [[gesture recognition|gestures]], and speech.<ref>{{cite book |last1=Caridakis |first1=George |last2=Castellano |first2=Ginevra |last3=Kessous |first3=Loic |last4=Raouzaiou |first4=Amaryllis |last5=Malatesta |first5=Lori |last6=Asteriadis |first6=Stelios |last7=Karpouzis |first7=Kostas |title=Multimodal emotion recognition from expressive faces, body gestures and speech |journal=IFIP the International Federation for Information Processing |volume=247 |date=19 September 2007 |pages=375–388 |doi=10.1007/978-0-387-74161-1_41 |language=en|isbn=978-0-387-74160-4 }}</ref>  The technology is said to contribute in the emergence of the so-called emotional or [[emotive Internet]].<ref>{{Cite web|url=https://techcrunch.com/2015/08/23/tapping-into-the-emotional-internet/|title=Tapping Into The Emotional Internet|last=Price|website=TechCrunch|language=en-US|access-date=2018-12-12}}</ref>

The existing approaches in emotion recognition to classify certain [[emotion]] types can be generally classified into three main categories: knowledge-based techniques, statistical methods, and hybrid approaches.<ref name="s1">{{cite journal |last1=Cambria |first1=Erik |title=Affective Computing and Sentiment Analysis |journal=IEEE Intelligent Systems |date=March 2016 |volume=31 |issue=2 |pages=102–107 |doi=10.1109/MIS.2016.31|s2cid=18580557 }}</ref>

====Knowledge-based techniques====

Knowledge-based techniques (sometimes referred to as [[lexicon]]-based techniques), utilize domain knowledge and the [[semantic]] and [[syntactic]] characteristics of language in order to detect certain [[emotion]] types.{{citation needed|date=September 2019}} In this approach, it is common to use knowledge-based resources during the [[emotion classification]] process such as [[WordNet]], SenticNet,<ref>{{cite journal |last1=Cambria |first1=Erik |last2=Poria |first2=Soujanya |last3=Bajpai |first3=Rajiv |last4=Schuller |first4=Bjoern |title=SenticNet 4: A Semantic Resource for Sentiment Analysis Based on Conceptual Primitives |journal=Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers |pages=2666–2677 |date=2016 |url=https://aclanthology.info/papers/C16-1251/c16-1251 |language=en}}</ref> [[ConceptNet]], and EmotiNet,<ref>{{cite journal |last1=Balahur |first1=Alexandra |last2=Hermida |first2=JesúS M |last3=Montoyo |first3=AndréS |title=Detecting implicit expressions of emotion in text: A comparative analysis |journal=Decision Support Systems |date=1 November 2012 |volume=53 |issue=4 |pages=742–753 |doi=10.1016/j.dss.2012.05.024 |url=https://dl.acm.org/citation.cfm?id=2364904 |issn=0167-9236}}</ref> to name a few.<ref name = "s6">{{cite journal |last1=Medhat |first1=Walaa |last2=Hassan |first2=Ahmed |last3=Korashy |first3=Hoda |title=Sentiment analysis algorithms and applications: A survey |journal=Ain Shams Engineering Journal |date=December 2014 |volume=5 |issue=4 |pages=1093–1113 |doi=10.1016/j.asej.2014.04.011|doi-access=free }}</ref> One of the advantages of this approach is the accessibility and economy brought about by the large availability of such knowledge-based resources.<ref name = "s1" /> A limitation of this technique on the other hand, is its inability to handle concept nuances and complex linguistic rules.<ref name = "s1" />

Knowledge-based techniques can be mainly classified into two categories: dictionary-based and corpus-based approaches.{{citation needed|date=September 2019}} Dictionary-based approaches find opinion or [[emotion]] seed words in a [[dictionary]] and search for their [[synonym]]s and [[antonym]]s to expand the initial list of opinions or [[emotion]]s.<ref name = "s3">{{cite book |last1=Madhoushi |first1=Zohreh |last2=Hamdan |first2=Abdul Razak |last3=Zainudin |first3=Suhaila |chapter= Sentiment analysis techniques in recent works|title= 2015 Science and Information Conference (SAI)|pages=288–291 |date=2015 |doi=10.1109/SAI.2015.7237157 |isbn=978-1-4799-8547-0 |s2cid=14821209 }}</ref> Corpus-based approaches on the other hand, start with a seed list of opinion or [[emotion]] words, and expand the database by finding other words with context-specific characteristics in a large [[corpus linguistics|corpus]].<ref name = "s3" /> While corpus-based approaches take into account context, their performance still vary in different domains since a word in one domain can have a different orientation in another domain.<ref>{{cite journal |last1=Hemmatian |first1=Fatemeh |last2=Sohrabi |first2=Mohammad Karim |title=A survey on classification techniques for opinion mining and sentiment analysis |journal=Artificial Intelligence Review |volume=52 |issue=3 |pages=1495–1545 |date=18 December 2017 |doi=10.1007/s10462-017-9599-6|s2cid=11741285 }}</ref>

====Statistical methods====

Statistical methods commonly involve the use of different supervised [[machine learning]] algorithms in which a large set of annotated data is fed into the algorithms for the system to learn and predict the appropriate [[emotion]] types.<ref name = "s1" />  [[Machine learning]] algorithms generally provide more reasonable classification accuracy compared to other approaches, but one of the challenges in achieving good results in the classification process, is the need to have a sufficiently large training set.<ref name="s1" />

Some of the most commonly used [[machine learning]] algorithms include [[support vector machines|Support Vector Machines (SVM)]], [[naive bayes classifier|Naive Bayes]], and [[maximum entropy classifier|Maximum Entropy]].<ref name = "s4">{{cite journal |last1=Sun |first1=Shiliang |last2=Luo |first2=Chen |last3=Chen |first3=Junyu |title=A review of natural language processing techniques for opinion mining systems |journal=Information Fusion |date=July 2017 |volume=36 |pages=10–25 |doi=10.1016/j.inffus.2016.10.004}}</ref> [[Deep learning]], which is under the unsupervised family of [[machine learning]], is also widely employed in emotion recognition.<ref>{{cite journal |last1=Majumder |first1=Navonil |last2=Poria |first2=Soujanya |last3=Gelbukh |first3=Alexander |last4=Cambria |first4=Erik |title=Deep Learning-Based Document Modeling for Personality Detection from Text |journal=IEEE Intelligent Systems |date=March 2017 |volume=32 |issue=2 |pages=74–79 |doi=10.1109/MIS.2017.23|s2cid=206468984 }}</ref><ref>{{cite journal |last1=Mahendhiran |first1=P. D. |last2=Kannimuthu |first2=S. |title=Deep Learning Techniques for Polarity Classification in Multimodal Sentiment Analysis |journal=International Journal of Information Technology & Decision Making |date=May 2018 |volume=17 |issue=3 |pages=883–910 |doi=10.1142/S0219622018500128}}</ref><ref>{{cite book |last1=Yu |first1=Hongliang |last2=Gui |first2=Liangke |last3=Madaio |first3=Michael |last4=Ogan |first4=Amy |last5=Cassell |first5=Justine |last6=Morency |first6=Louis-Philippe |title=Temporally Selective Attention Model for Social and Affective State Recognition in Multimedia Content |date=23 October 2017 |pages=1743–1751 |doi=10.1145/3123266.3123413 |publisher=ACM|isbn=9781450349062 |series=MM '17 |s2cid=3148578 }}</ref> Well-known [[deep learning]] algorithms include different architectures of [[artificial neural network|Artificial Neural Network (ANN)]] such as [[convolutional neural network|Convolutional Neural Network (CNN)]], [[Long short-term memory|Long Short-term Memory (LSTM)]], and [[extreme learning machine|Extreme Learning Machine (ELM)]].<ref name = "s4" /> The popularity of [[deep learning]] approaches in the domain of emotion recognition may be mainly attributed to its success in related applications such as in [[computer vision]], [[speech recognition]], and [[natural language processing|Natural Language Processing (NLP)]].<ref name = "s4" />

====Hybrid approaches====

Hybrid approaches in emotion recognition are essentially a combination of knowledge-based techniques and statistical methods, which exploit complementary characteristics from both techniques.<ref name = "s1" /> Some of the works that have applied an ensemble of knowledge-driven linguistic elements and statistical methods include sentic computing and iFeel, both of which have adopted the concept-level knowledge-based resource SenticNet.<ref>{{cite book |last1=Cambria |first1=Erik |last2=Hussain |first2=Amir |title=Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis |date=2015 |publisher=Springer Publishing Company, Incorporated |isbn=978-3319236537 |url=https://dl.acm.org/citation.cfm?id=2878632}}</ref><ref>{{cite book |last1=Araújo |first1=Matheus |last2=Gonçalves |first2=Pollyanna |last3=Cha |first3=Meeyoung |author-link3= Cha Meeyoung|last4=Benevenuto |first4=Fabrício |title=iFeel: a system that compares and combines sentiment analysis methods |date=7 April 2014 |pages=75–78 |doi=10.1145/2567948.2577013 |publisher=ACM|isbn=9781450327459 |series=WWW '14 Companion |s2cid=11018367 }}</ref> The role of such knowledge-based resources in the implementation of hybrid approaches is highly important in the [[emotion]] classification process.<ref name = "s6" /> Since hybrid techniques gain from the benefits offered by both knowledge-based and statistical approaches, they tend to have better classification performance as opposed to employing knowledge-based or statistical methods independently.{{citation needed|date=September 2019}} A downside of using hybrid techniques however, is the computational complexity during the classification process.<ref name = "s6" />

===Datasets===

Data is an integral part of the existing approaches in emotion recognition and in most cases it is a challenge to obtain annotated data that is necessary to train [[machine learning]] algorithms.<ref name = "s3" /> For the task of classifying different [[emotion]] types from multimodal sources in the form of texts, audio, videos or physiological signals, the following datasets are available:

#HUMAINE: provides natural clips with emotion words and context labels in multiple modalities<ref>{{cite book |editor=Paolo Petta |editor2=Catherine Pelachaud |editor2-link=Catherine Pelachaud |editor3=Roddy Cowie |title=Emotion-oriented systems the humaine handbook |date=2011 |publisher=Springer |location=Berlin |isbn=978-3-642-15184-2}}</ref>
#Belfast database: provides clips with a wide range of emotions from TV programs and interview recordings<ref>{{cite journal |last1=Douglas-Cowie |first1=Ellen |last2=Campbell |first2=Nick |last3=Cowie |first3=Roddy |last4=Roach |first4=Peter |title=Emotional speech: towards a new generation of databases |journal=Speech Communication |date=1 April 2003 |volume=40 |issue=1–2 |pages=33–60 |doi=10.1016/S0167-6393(02)00070-5 |url=https://dl.acm.org/citation.cfm?id=772595 |issn=0167-6393|citeseerx=10.1.1.128.3991 }}</ref>
#SEMAINE: provides audiovisual recordings between a person and a [[Intelligent agent|virtual agent]] and contains [[emotion]] annotations such as angry, happy, fear, disgust, sadness, contempt, and amusement<ref>{{cite journal |last1=McKeown |first1=G. |last2=Valstar |first2=M. |last3=Cowie |first3=R. |last4=Pantic |first4=M. |last5=Schroder |first5=M. |title=The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent |journal=IEEE Transactions on Affective Computing |date=January 2012 |volume=3 |issue=1 |pages=5–17 |doi=10.1109/T-AFFC.2011.20|s2cid=2995377 |url=https://pure.qub.ac.uk/portal/en/publications/the-semaine-database-annotated-multimodal-records-of-emotionally-colored-conversations-between-a-person-and-a-limited-agent(4f349228-ebb5-4964-be2c-18f3559be29f).html }}</ref>
#IEMOCAP: provides recordings of dyadic sessions between actors and contains [[emotion]] annotations such as happiness, anger, sadness, frustration, and neutral state <ref>{{cite journal |last1=Busso |first1=Carlos |last2=Bulut |first2=Murtaza |last3=Lee |first3=Chi-Chun |last4=Kazemzadeh |first4=Abe |last5=Mower |first5=Emily |last6=Kim |first6=Samuel |last7=Chang |first7=Jeannette N. |last8=Lee |first8=Sungbok |last9=Narayanan |first9=Shrikanth S. |title=IEMOCAP: interactive emotional dyadic motion capture database |journal=Language Resources and Evaluation |date=5 November 2008 |volume=42 |issue=4 |pages=335–359 |doi=10.1007/s10579-008-9076-6 |s2cid=11820063 |language=en |issn=1574-020X}}</ref>
#eNTERFACE: provides audiovisual recordings of subjects from seven nationalities and contains [[emotion]] annotations such as happiness, anger, sadness, surprise, disgust, and fear <ref>{{cite book |last1=Martin |first1=O. |last2=Kotsia |first2=I. |last3=Macq |first3=B. |last4=Pitas |first4=I. |title=The eNTERFACE'05 Audio-Visual Emotion Database |date=3 April 2006 |pages=8– |doi=10.1109/ICDEW.2006.145 |url=https://dl.acm.org/citation.cfm?id=1130193 |publisher=IEEE Computer Society|isbn=9780769525716 |series=Icdew '06 |s2cid=16185196 }}</ref>
#DEAP: provides [[electroencephalography]] ([[EEG]]), [[electrocardiography]] ([[ECG]]), and face video recordings, as well as [[emotion]] annotations in terms of [[Valence (psychology)|valence]], [[arousal]], and [[Dominance (psychology)|dominance]] of people watching film clips <ref>{{cite journal |last1=Koelstra |first1=Sander |last2=Muhl |first2=Christian |last3=Soleymani |first3=Mohammad |last4=Lee |first4=Jong-Seok |last5=Yazdani |first5=Ashkan |last6=Ebrahimi |first6=Touradj |last7=Pun |first7=Thierry |last8=Nijholt |first8=Anton |last9=Patras |first9=Ioannis | title=DEAP: A Database for Emotion Analysis Using Physiological Signals | journal=IEEE Transactions on Affective Computing | volume=3 |issue=1 |date=January 2012 |pages=18–31 |doi=10.1109/T-AFFC.2011.15 |issn=1949-3045|citeseerx=10.1.1.593.8470 |s2cid=206597685 }}</ref>
#DREAMER: provides [[electroencephalography]] ([[EEG]]) and [[electrocardiography]] ([[ECG]]) recordings, as well as [[emotion]] annotations in terms of [[Valence (psychology)|valence]], [[arousal]], and [[Dominance (psychology)|dominance]] of people watching film clips <ref>{{cite journal |last1=Katsigiannis|first1=Stamos|last2=Ramzan|first2=Naeem | title=DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices | journal= IEEE Journal of Biomedical and Health Informatics  | volume=22 |issue=1 |date=January 2018 |pages=98–107 |doi=10.1109/JBHI.2017.2688239 |pmid=28368836|s2cid=23477696|issn=2168-2194|url=https://myresearchspace.uws.ac.uk/ws/files/1077176/Accepted_Author_Manuscript.pdf }}</ref>
#MELD: is a multiparty conversational dataset where each utterance is labeled with emotion and sentiment. MELD<ref>{{Cite journal|last1=Poria|first1=Soujanya|last2=Hazarika|first2=Devamanyu|last3=Majumder|first3=Navonil|last4=Naik|first4=Gautam|last5=Cambria|first5=Erik|last6=Mihalcea|first6=Rada|date=2019|title=MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations|journal=Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics|pages=527–536|location=Stroudsburg, PA, USA|publisher=Association for Computational Linguistics|doi=10.18653/v1/p19-1050|arxiv=1810.02508|s2cid=52932143}}</ref> provides conversations in video format and hence suitable for [[Multimodal interaction|multimodal]] emotion recognition and [[sentiment analysis]]. MELD is useful for [[multimodal sentiment analysis]] and emotion recognition, [[dialogue system]]s and [[emotion recognition in conversation]]s.<ref name=":0" />
#MuSe: provides audiovisual recordings of natural interactions between a person and an object.<ref>{{Cite journal|last1=Stappen|first1=Lukas|last2=Schuller|first2=Björn|last3=Lefter|first3=Iulia|last4=Cambria|first4=Erik|last5=Kompatsiaris|first5=Ioannis|date=2020|title=Summary of MuSe 2020: Multimodal Sentiment Analysis, Emotion-target Engagement and Trustworthiness Detection in Real-life Media|journal=Proceedings of the 28th ACM International Conference on Multimedia|pages=4769–4770|location=Seattle, PA, USA|publisher=Association for Computing Machinery|doi=10.1145/3394171.3421901|arxiv=2004.14858|s2cid=222278714}}</ref> It has discrete and continuous [[emotion]] annotations in terms of valence, arousal and trustworthiness as well as speech topics useful for [[multimodal sentiment analysis]] and emotion recognition.
#UIT-VSMEC: is a standard Vietnamese Social Media Emotion Corpus (UIT-VSMEC) with about 6,927 human-annotated sentences with six emotion labels, contributing to emotion recognition research in Vietnamese which is a low-resource language in Natural Language Processing (NLP).<ref>{{Cite journal|last1=Ho|first1=Vong|title=Emotion Recognition for Vietnamese Social Media Text|journal=16th International Conference of the Pacific Association for Computational Linguistics (PACLING 2019)|series=Communications in Computer and Information Science|year=2020|volume=1215|pages=319–333|doi=10.1007/978-981-15-6168-9_27|arxiv=1911.09339|isbn=978-981-15-6167-2| url=https://link.springer.com/chapter/10.1007/978-981-15-6168-9_27}}</ref>
#BED: provides [[electroencephalography]] ([[EEG]]) recordings, as well as [[emotion]] annotations in terms of [[Valence (psychology)|valence]] and [[arousal]] of people watching images. It also includes [[electroencephalography]] ([[EEG]]) recordings of people exposed to various stimuli ([[SSVEP]], resting with eyes closed, resting with eyes open, cognitive tasks) for the task of EEG-based [[biometrics]].<ref>{{cite journal|last1=Arnau-González|first1=Pablo|last2=Katsigiannis|first2=Stamos|last3=Arevalillo-Herráez|first3=Miguel|last4=Ramzan|first4=Naeem | title=BED: A new dataset for EEG-based biometrics | journal=IEEE Internet of Things Journal | volume=(Early Access)|date=February 2021 |page=1|doi=10.1109/JIOT.2021.3061727 |issn=2327-4662|url=https://ieeexplore.ieee.org/document/9361690 }}</ref>

===Applications===

Emotion recognition is used in society for a variety of reasons.  [[Affectiva]], which spun out of [[MIT]], provides [[artificial intelligence]] software that makes it more efficient to do tasks previously done manually by people, mainly to gather facial expression and vocal expression information related to specific contexts where viewers have consented to share this information.  For example, instead of filling out a lengthy survey about how you feel at each point watching an educational video or advertisement, you can consent to have a camera watch your face and listen to what you say, and note during which parts of the experience you show expressions such as boredom, interest, confusion, or smiling.  (Note that this does not imply it is reading your innermost feelings—it only reads what you express outwardly.)  Other uses by [[Affectiva]] include helping children with autism, helping people who are blind to read facial expressions, helping robots interact more intelligently with people, and monitoring signs of attention while driving in an effort to enhance driver safety.<ref>{{cite web|url=http://www.affectiva.com|title=Affectiva}}</ref>

A [https://pdfpiw.uspto.gov/.piw?PageNum=0&docid=10061977&IDKey=20D25A962A60&HomeUrl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect2%3DPTO1%2526Sect2%3DHITOFF%2526p%3D1%2526u%3D%2Fnetahtml%2FPTO%2Fsearch-bool.html%2526r%3D1%2526f%3DG%2526l%3D50%2526d%3DPALL%2526S1%3D10061977.PN.%2526OS%3DPN%2F10061977%2526RS%3DPN%2F10061977 patent] filed by [[Snapchat]] in 2015 describes a method of extracting data about crowds at public events by performing algorithmic emotion recognition on users' geotagged [[selfie]]s.<ref>{{Cite web|url=https://www.scientificamerican.com/article/this-video-watches-you-back/|title=This Video Watches You Back|last=Bushwick|first=Sophie|website=Scientific American|language=en|access-date=2020-01-27}}</ref>

Emotient was a [[startup company]] which applied emotion recognition to reading frowns, smiles, and other expressions on faces, namely [[artificial intelligence]] to predict "attitudes and actions based on facial expressions".<ref name="DeMuth">{{Cite news|url = http://seekingalpha.com/article/3798766-apple-reads-your-mind|title = Apple Reads Your Mind|last = DeMuth Jr.|first = Chris|date = 8 January 2016|work = M&A Daily|access-date = 9 January 2016|publisher = Seeking Alpha}}</ref>  [[Apple Inc.|Apple]] bought Emotient in 2016 and uses emotion recognition technology to enhance the emotional intelligence of its products.<ref name="DeMuth" />

nViso provides real-time emotion recognition for web and mobile applications through a real-time [[API]].<ref>{{Cite web|url=http://www.nviso.ch| website=nViso.ch |title=nViso}}</ref> [[Visage Technologies AB]] offers emotion estimation as a part of their [[Visage SDK]] for [[marketing]] and scientific research and similar purposes.<ref>{{Cite web|url=https://visagetechnologies.com/products-and-services/visagesdk/faceanalysis/|title=Visage Technologies}}</ref>

Eyeris is an emotion recognition company that works with [[embedded system]] manufacturers including car makers and social robotic companies on integrating its face analytics and emotion recognition software; as well as with video content creators to help them measure the perceived effectiveness of their short and long form video creative.<ref>{{cite web|url=http://www.cnet.com/roadshow/news/eyeris-emovu-detects-driver-emotions/ |title=Feeling sad, angry? Your future car will know }}</ref><ref>{{cite news|url=http://www.huffingtonpost.com/entry/drowsy-driving-warning-system_us_56eadd1be4b09bf44a9c96aa |title=Cars May Soon Warn Drivers Before They Nod Off|newspaper=Huffington Post|date=2016-03-22|last1=Varagur|first1=Krithika}}</ref>

Many products also exist to aggregate information from emotions communicated online, including via "like" button presses and via counts of positive and negative phrases in text and affect recognition is increasingly used in some kinds of games and virtual reality, both for educational purposes and to give players more natural control over their social avatars.{{citation needed|date=February 2020}}

==Subfields of emotion recognition==
Emotion recognition is probably to gain the best outcome if applying [[Multimodal interaction|multiple modalities]] by combining different objects, including [[Text (literary theory)|text]] (conversation), audio, video, and [[physiology]] to detect emotions.

===Emotion recognition in text===
Text data is a favorable research object for emotion recognition when it is free and available everywhere in human life. Compare to other types of data, the storage of text data is lighter and easy to compress to the best performance due to the frequent repetition of words and characters in languages. Emotions can be extracted from two essential text forms: written texts and [[conversation]]s (dialogues).<ref>Shivhare, S. N., & Khethawat, S. (2012). Emotion detection from text. arXiv preprint {{arXiv|1205.4944}}</ref> For written texts, many scholars focus on working with sentence level to extract "words/phrases" representing emotions.<ref>Ezhilarasi, R., & Minu, R. I. (2012). [https://www.researchgate.net/profile/R_I_Minu/publication/255704866_Automatic_Emotion_Recognition_and_Classification/links/567f92d308ae19758389f82e.pdf Automatic emotion recognition and classification]. Procedia Engineering, 38, 21-26.</ref><ref>Krcadinac, U., Pasquier, P., Jovanovic, J., & Devedzic, V. (2013). Synesketch: An open source library for sentence-based emotion recognition. IEEE Transactions on Affective Computing, 4(3), 312-325.</ref>

===Emotion recognition in audio===
Different from emotion recognition in text, vocal signals are used for the recognition to [[Emotional speech recognition|extract emotions from audio]].<ref>Schmitt, M., Ringeval, F., & Schuller, B. W. (2016, September). [https://web.archive.org/web/20200215183308/https://pdfs.semanticscholar.org/7ebf/51a3bff0834a33e3313bd51c0c7d7ac50fc2.pdf At the Border of Acoustics and Linguistics: Bag-of-Audio-Words for the Recognition of Emotions in Speech]. In Interspeech (pp. 495-499).</ref>

===Emotion recognition in video===
Video data is a combination of audio data, image data and sometimes texts (in case of [[subtitles]]<ref>Dhall, A., Goecke, R., Lucey, S., & Gedeon, T. (2012). [https://www.researchgate.net/profile/Abhinav_Dhall/publication/256575277_Collecting_Large_Richly_Annotated_Facial-Expression_Databases_from_Movies/links/00b4953b27b9057001000000/Collecting-Large-Richly-Annotated-Facial-Expression-Databases-from-Movies.pdf Collecting large, richly annotated facial-expression databases from movies]. IEEE multimedia, (3), 34-41.</ref>).

===Emotion recognition in conversation===
[[Emotion recognition in conversation]] (ERC) extracts opinions between participants from massive conversational data in [[social network|social platform]]s, such as [[Facebook]], [[Twitter]], YouTube, and others.<ref name=":0">Poria, S., Majumder, N., Mihalcea, R., & Hovy, E. (2019). [https://ieeexplore.ieee.org/iel7/6287639/8600701/08764449.pdf Emotion recognition in conversation: Research challenges, datasets, and recent advances]. IEEE Access, 7, 100943-100953.</ref> ERC can take input data like text, audio, video or a combination form to detect several emotions such as fear, lust, pain, and pleasure.

==See also==
*[[Affective computing]]
*[[Face perception]]
*[[Facial recognition system]]
*[[Sentiment analysis]]
*[[Interpersonal accuracy]]

==References==
{{Reflist}}

{{Emotion-footer}}
{{Psychology}}
{{Use dmy dates|date=August 2016}}

{{Nonverbal communication}}

[[Category:Emotion]]
[[Category:Artificial intelligence applications]]