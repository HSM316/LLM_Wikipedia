{{Short description|Computer backgammon program (1992)}}
'''TD-Gammon''' is a [[computer]] [[backgammon]] program developed in 1992 by [[Gerald Tesauro]] at [[IBM]]'s [[Thomas J. Watson Research Center]]. Its name comes from the fact that it is an [[artificial neural net]] trained by a form of [[temporal-difference learning]], specifically [[temporal-difference learning#TD-Lambda|TD-Lambda]].

TD-Gammon achieved a level of play just slightly below that of the top human backgammon players of the time. It explored strategies that humans had not pursued and led to advances in the theory of correct backgammon play.

==Algorithm for play and learning==
During play, TD-Gammon examines on each turn all possible legal moves and all their possible responses (two-[[ply (game theory)|ply]] [[Combinatorial search|lookahead]]), feeds each resulting board position into its [[evaluation function]], and chooses the move that leads to the board position that got the highest score. In this respect, TD-Gammon is no different than almost any other computer board-game program. TD-Gammon's innovation was in how it learned its evaluation function.

TD-Gammon's learning algorithm consists of updating the weights in its neural net after each turn to reduce the difference between its evaluation of previous turns' board positions and its evaluation of the present turn's board positionâ€”hence "[[temporal-difference learning]]". The score of any board position is a set of four numbers reflecting the program's estimate of the likelihood of each possible game result: White wins normally, Black wins normally, White wins a gammon, Black wins a gammon. For the final board position of the game, the algorithm compares with the actual result of the game rather than its own evaluation of the board position.<ref name="CACM">{{cite journal | url=http://www.bkgm.com/articles/tesauro/tdl.html | title=Temporal Difference Learning and TD-Gammon | date=March 1995 | access-date=Nov 1, 2013 | last=Tesauro | first=Gerald | journal=Communications of the ACM  | volume=38 | issue=3 | doi = 10.1145/203330.203343 }}</ref>

After each turn, the learning algorithm updates each weight in the neural net according to the following rule:

: <math>w_{t+1} - w_t = \alpha(Y_{t+1} - Y_t)\sum_{k=1}^{t}\lambda^{t-k} \nabla_w Y_k</math>
where:
:{|
|- valign="top" 
| <math>w_{t+1} - w_t</math>
| is the amount to change the weight from its value on the previous turn.
|- valign="top" 
| <math>Y_{t+1} - Y_t</math>
| is the difference between the current and previous turn's board evaluations.
|- valign="top" 
| <math>\alpha</math>
| is a "[[learning rate]]" parameter.
|- valign="top" 
| <math>\lambda</math>
| is a parameter that affects how much the present difference in board evaluations should feed back to previous estimates. <math>\lambda = 0</math> makes the program correct only the previous turn's estimate; <math>\lambda = 1</math> makes the program attempt to correct the estimates on all previous turns; and values of <math>\lambda</math> between 0 and 1 specify different rates at which the importance of older estimates should "decay" with time.
|- valign="top" 
| <math>\nabla_w Y_k</math>
| is the [[gradient]] of neural-network output with respect to weights: that is, how much changing the weight affects the output.<ref name=CACM/>
|}

==Experiments and stages of training==
Unlike previous neural-net backgammon programs such as [[Neurogammon]] (also written by Tesauro), where an expert trained the program by supplying the "correct" evaluation of each position, TD-Gammon was at first programmed "knowledge-free".<ref name="CACM"/>  In early experimentation, using only a raw board encoding with no human-designed features, TD-Gammon reached a level of play comparable to Neurogammon: that of an intermediate-level human backgammon player.

Even though TD-Gammon discovered insightful features on its own, Tesauro wondered if its play could be improved by using hand-designed features like Neurogammon's.  Indeed, the self-training TD-Gammon with expert-designed features soon surpassed all previous computer backgammon programs.  It stopped improving after about 1,500,000 games (self-play) using 80 hidden units.<ref>{{cite book | last = Sutton | first = Richard S. |author2=Andrew G. Barto  | title = Reinforcement Learning: An Introduction | publisher = MIT Press | year = 1998 | pages = Table 11.1 | url = http://incompleteideas.net/sutton/book/ebook/node108.html }}</ref>

==Advances in backgammon theory==
TD-Gammon's exclusive training through self-play (rather than tutelage) enabled it to explore strategies that humans previously had not considered or had ruled out erroneously.  Its success with unorthodox strategies had a significant impact on the backgammon community.<ref name="CACM"/>

For example, on the opening play, the conventional wisdom was that given a roll of 2-1, 4-1, or 5-1, White should move a single checker from point 6 to point 5. Known as "slotting", this technique trades the risk of a hit for the opportunity to develop an aggressive position. TD-Gammon found that the more conservative play of 24-23 was superior. Tournament players began experimenting with TD-Gammon's move, and found success. Within a few years, slotting had disappeared from tournament play, though in 2006 it made a reappearance for 2-1.<ref>{{cite web|url=http://www.bkgm.com/openings.html|title=Backgammon: How to Play the Opening Rolls}}</ref>

Backgammon expert [[Kit Woolsey]] found that TD-Gammon's positional judgement, especially its weighing of risk against safety, was superior to his own or any human's.<ref name=CACM/>

TD-Gammon's excellent positional play was undercut by occasional poor endgame play. The endgame requires a more analytical approach, sometimes with extensive lookahead. TD-Gammon's limitation to two-ply lookahead put a ceiling on what it could achieve in this part of the game. TD-Gammon's strengths and weaknesses were the opposite of [[symbolic artificial intelligence]] programs and most computer software in general: it was good at matters that require an intuitive "feel" but bad at systematic analysis.

==See also==
{{Portal|Games}}
*[[World Backgammon Federation]]

==References==
{{Reflist}}

==External links==
*[https://researcher.watson.ibm.com/researcher/view_page.php?id=6853 TD-Gammon] at IBM
*{{github|dellalibera/td-gammon|TD-Gammon}}

{{tables games}}

[[Category:Backgammon]]
[[Category:Applied machine learning]]
[[Category:Reinforcement learning]]