{{Other uses|CNN (disambiguation)}}
{{short description|Artificial neural network}}
{{More citations needed|date=June 2019}}
{{machine learning bar}}
In [[deep learning]], a '''convolutional neural network''' ('''CNN''', or '''ConvNet''') is a class of [[deep neural network]]s, most commonly applied to analyzing visual imagery.<ref name="Valueva Nagornov Lyakhov Valuev 2020 pp. 232–243">{{cite journal | last1=Valueva | first1=M.V. | last2=Nagornov | first2=N.N. | last3=Lyakhov | first3=P.A. | last4=Valuev | first4=G.V. | last5=Chervyakov | first5=N.I. | title=Application of the residue number system to reduce hardware costs of the convolutional neural network implementation | journal=Mathematics and Computers in Simulation | publisher=Elsevier BV | volume=177 | year=2020 | issn=0378-4754 | doi=10.1016/j.matcom.2020.04.031 | pages=232–243 | quote=Convolutional neural networks are a promising tool for solving the problem of pattern recognition. }}</ref> They are also known as '''shift invariant''' or '''space invariant artificial neural networks''' ('''SIANN'''), based on their shared-weights architecture and [[translation invariance]] characteristics.<ref name=":0">{{Cite journal|last=Zhang|first=Wei|date=1988|title=Shift-invariant pattern recognition neural network and its optical architecture|url=https://drive.google.com/file/d/1nN_5odSG_QVae54EsQN_qSz-0ZsX6wA0/view?usp=sharing|journal=Proceedings of Annual Conference of the Japan Society of Applied Physics}}</ref><ref name=":1">{{Cite journal|last=Zhang|first=Wei|date=1990|title=Parallel distributed processing model with local space-invariant interconnections and its optical architecture|url=https://drive.google.com/file/d/0B65v6Wo67Tk5ODRzZmhSR29VeDg/view?usp=sharing|journal=Applied Optics|volume=29|issue=32|pages=4790–7|doi=10.1364/AO.29.004790|pmid=20577468|bibcode=1990ApOpt..29.4790Z}}</ref> They have applications in [[Computer vision|image and video recognition]], [[recommender system]]s,<ref>{{Cite book|url=http://papers.nips.cc/paper/5004-deep-content-based-music-recommendation.pdf|title=Deep content-based music recommendation|last1=van den Oord|first1=Aaron|last2=Dieleman|first2=Sander|last3=Schrauwen|first3=Benjamin|date=2013-01-01|publisher=Curran Associates, Inc.|editor-last=Burges|editor-first=C. J. C.|pages=2643–2651|editor-last2=Bottou|editor-first2=L.|editor-last3=Welling|editor-first3=M.|editor-last4=Ghahramani|editor-first4=Z.|editor-last5=Weinberger|editor-first5=K. Q.}}</ref> [[image classification]], [[Medical image computing|medical image analysis]], [[natural language processing]],<ref>{{Cite book|last1=Collobert|first1=Ronan|last2=Weston|first2=Jason|date=2008-01-01|title=A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning|journal=Proceedings of the 25th International Conference on Machine Learning|series=ICML '08|location=New York, NY, USA|publisher=ACM|pages=160–167|doi=10.1145/1390156.1390177|isbn=978-1-60558-205-4|s2cid=2617020}}</ref> [[Brain–computer interface|brain-computer interfaces]],<ref>{{Cite journal|last1=Avilov|first1=Oleksii|last2=Rimbert|first2=Sebastien|last3=Popov|first3=Anton|last4=Bougrain|first4=Laurent|date=July 2020|title=Deep Learning Techniques to Improve Intraoperative Awareness Detection from Electroencephalographic Signals|url=https://ieeexplore.ieee.org/document/9176228|journal=2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)|volume=2020|location=Montreal, QC, Canada|publisher=IEEE|pages=142–145|doi=10.1109/EMBC44109.2020.9176228|pmid=33017950|isbn=978-1-7281-1990-8|s2cid=221386616}}</ref> and financial [[time series]].<ref name="Tsantekidis 7–12">{{Cite journal|last1=Tsantekidis|first1=Avraam|last2=Passalis|first2=Nikolaos|last3=Tefas|first3=Anastasios|last4=Kanniainen|first4=Juho|last5=Gabbouj|first5=Moncef|last6=Iosifidis|first6=Alexandros|date=July 2017|title=Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks|journal=2017 IEEE 19th Conference on Business Informatics (CBI)|location=Thessaloniki, Greece|publisher=IEEE|pages=7–12|doi=10.1109/CBI.2017.23|isbn=978-1-5386-3035-8|s2cid=4950757}}</ref>

CNNs are [[Regularization (mathematics)|regularized]] versions of [[multilayer perceptron]]s. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The "fully-connectedness" of these networks makes them prone to [[overfitting]] data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.

Convolutional networks were [[mathematical biology|inspired]] by [[biological]] processes<ref name=fukuneoscholar /><ref name="hubelwiesel1968" /><ref name="intro" /><ref name="robust face detection">{{cite journal|last=Matusugu|first=Masakazu|year=2003|title=Subject independent facial expression recognition with robust face detection using a convolutional neural network|url=http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/sparse/matsugo_etal_face_expression_conv_nnet.pdf|journal=Neural Networks|volume=16|issue=5|pages=555–559|doi=10.1016/S0893-6080(03)00115-1|pmid=12850007|author2=Katsuhiko Mori|author3=Yusuke Mitari|author4=Yuji Kaneda|access-date=17 November 2013}}</ref> in that the connectivity pattern between [[Artificial neuron|neurons]] resembles the organization of the animal [[visual cortex]]. Individual [[cortical neuron]]s respond to stimuli only in a restricted region of the [[visual field]] known as the [[receptive field]]. The receptive fields of different neurons partially overlap such that they cover the entire visual field.

CNNs use relatively little pre-processing compared to other [[Image classification|image classification algorithms]]. This means that the network learns the [[Filter (signal processing)|filters]] that in traditional algorithms were [[Feature engineering|hand-engineered]]. This independence from prior knowledge and human effort in feature design is a major advantage.

{{toclimit|3}}

== Definition ==
The name “convolutional neural network” indicates that the network employs a mathematical operation called [[convolution]].
Convolutional networks are a specialized type of neural networks that use convolution in place of general matrix multiplication in at least one of their layers.<ref>{{cite book |last1=Ian Goodfellow and Yoshua Bengio and Aaron Courville |title=Deep Learning |date=2016 |publisher=MIT Press |page=326 |url=http://www.deeplearningbook.org}}</ref>

== Architecture ==
A convolutional neural network consists of an input and an output layer, as well as multiple [[Multilayer perceptron#Layers|hidden layers]]. The hidden layers of a CNN typically consist of a series of convolutional layers that ''convolve'' with a multiplication or other [[dot product]]. The activation function is commonly a [[Rectifier (neural networks)|ReLU layer]], and is subsequently followed by additional convolutions such as pooling layers, fully connected layers and normalization layers, referred to as hidden layers because their inputs and outputs are masked by the activation function and final [[convolution]].

=== Convolutional ===
When programming a CNN, the input is a [[tensor]] with shape (number of images) x (image height) x (image width) x (input [[Channel (digital image)|channels]]). Then after passing through a convolutional layer, the image becomes abstracted to a feature map, with shape (number of images) x (feature map height) x (feature map width) x (feature map [[Channel (digital image)|channels]]). A convolutional layer within a neural network should have the following attributes:

* Convolutional kernels defined by a width and height (hyper-parameters).
* The number of input channels and output channels (hyper-parameter).
* The depth of the Convolution filter (the input channels) must be equal to the number channels (depth) of the input feature map.

Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.<ref name="deeplearning">{{cite web|title=Convolutional Neural Networks (LeNet) – DeepLearning 0.1 documentation|url=http://deeplearning.net/tutorial/lenet.html|work=DeepLearning 0.1|publisher=LISA Lab|access-date=31 August 2013}}</ref> Each convolutional neuron processes data only for its [[receptive field]]. Although [[Multilayer perceptron|fully connected feedforward neural networks]] can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for ''each'' neuron in the second layer. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters.<ref>{{Cite book|title=Guide to convolutional neural networks : a practical application to traffic-sign detection and classification|last=Habibi|first=Aghdam, Hamed|others=Heravi, Elnaz Jahani|isbn=9783319575490|location=Cham, Switzerland|oclc=987790957|date = 2017-05-30}}</ref> For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. By using regularized weights over fewer parameters, the vanishing gradient and exploding gradient problems seen during [[backpropagation]] in traditional neural networks are avoided.<ref>{{Cite book|last1=Venkatesan|first1=Ragav|url=https://books.google.com/books?id=bAM7DwAAQBAJ&q=vanishing+gradient|title=Convolutional Neural Networks in Visual Computing: A Concise Guide|last2=Li|first2=Baoxin|date=2017-10-23|publisher=CRC Press|isbn=978-1-351-65032-8|language=en}}</ref><ref>{{Cite book|last1=Balas|first1=Valentina E.|url=https://books.google.com/books?id=XRS_DwAAQBAJ&q=exploding+gradient|title=Recent Trends and Advances in Artificial Intelligence and Internet of Things|last2=Kumar|first2=Raghvendra|last3=Srivastava|first3=Rajshree|date=2019-11-19|publisher=Springer Nature|isbn=978-3-030-32644-9|language=en}}</ref>

=== Pooling ===
Convolutional networks may include local or global pooling layers to streamline the underlying computation. Pooling layers reduce the dimensions of the data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, typically 2 x 2. Global pooling acts on all the neurons of the convolutional layer.<ref name="flexible" /><ref>{{cite web|last=[[Alex Krizhevsky|Krizhevsky]]|first=Alex|title=ImageNet Classification with Deep Convolutional Neural Networks|url=http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf|access-date=17 November 2013}}</ref> In addition, pooling may compute a max or an average. ''Max pooling'' uses the maximum value from each of a cluster of neurons at the prior layer.<ref name=Yamaguchi111990>{{cite conference |title=A Neural Network for Speaker-Independent Isolated Word Recognition |last1=Yamaguchi |first1=Kouichi |last2=Sakamoto |first2=Kenji |last3=Akabane |first3=Toshio |last4=Fujimoto |first4=Yoshiji |date=November 1990 |location=Kobe, Japan |conference=First International Conference on Spoken Language Processing (ICSLP 90)|url=https://www.isca-speech.org/archive/icslp_1990/i90_1077.html}}</ref><ref name="mcdns">{{cite book |last1=Ciresan |first1=Dan |first2=Ueli |last2=Meier |first3=Jürgen |last3=Schmidhuber |title=Multi-column deep neural networks for image classification |journal=2012 IEEE Conference on Computer Vision and Pattern Recognition |date=June 2012 |pages=3642–3649 |doi=10.1109/CVPR.2012.6248110 |arxiv=1202.2745 |isbn=978-1-4673-1226-4 |oclc=812295155 |publisher=[[Institute of Electrical and Electronics Engineers]] (IEEE) |location=New York, NY|citeseerx=10.1.1.300.3283 |s2cid=2161592 }}</ref> ''Average pooling'' uses the average value from each of a cluster of neurons at the prior layer.<ref name="cnnbackground">"[https://www.academia.edu/37491583/A_Survey_of_FPGA-based_Accelerators_for_Convolutional_Neural_Networks A Survey of FPGA-based Accelerators for Convolutional Neural Networks]", NCAA, 2018</ref>

=== Fully connected ===
Fully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional [[multi-layer perceptron]] neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.

=== Receptive field ===
In neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from ''every'' element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer. Typically the subarea is of a square shape (e.g., size 5 by 5). The input area of a neuron is called its ''receptive field''. So, in a fully connected layer, the receptive field is the entire previous layer. In a convolutional layer, the receptive area is smaller than the entire previous layer.
The subarea of the original input image in the receptive field is increasingly growing as getting deeper in the network architecture. This is due to applying over and over again a convolution which takes into account the value of a specific pixel, but also some surrounding pixels.

=== Weights ===
Each neuron in a neural network computes an output value by applying a specific function to the input values coming from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning, in a neural network, progresses by making iterative adjustments to these biases and weights.

The vector of weights and the bias are called ''filters'' and represent particular [[Feature (machine learning)|feature]]s of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces [[memory footprint]] because a single bias and a single vector of weights are used across all receptive fields sharing that filter, as opposed to each receptive field having its own bias and vector weighting.<ref name="LeCun">{{cite web|url=http://yann.lecun.com/exdb/lenet/|title=LeNet-5, convolutional neural networks|last=LeCun|first=Yann|access-date=16 November 2013}}</ref>

== History ==

CNN design follows vision processing in [[living organisms]].{{citation needed|date=October 2017}}

=== Receptive fields in the visual cortex ===
Work by [[David H. Hubel|Hubel]] and [[Torsten Wiesel|Wiesel]] in the 1950s and 1960s showed that cat and monkey visual [[Cortex (anatomy)|cortex]]es contain neurons that individually respond to small regions of the [[visual field]]. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its [[receptive field]].<ref name=":4" /> Neighboring cells have similar and overlapping receptive fields.{{citation needed|date=October 2017}} Receptive field size and location varies systematically across the cortex to form a complete map of visual space.{{citation needed|date=October 2017}} The cortex in each hemisphere represents the contralateral [[visual field]].{{citation needed|date=October 2017}}

Their 1968 paper identified two basic visual cell types in the brain:<ref name="hubelwiesel1968">{{Cite journal|title = Receptive fields and functional architecture of monkey striate cortex|journal = The Journal of Physiology|date = 1968-03-01|issn = 0022-3751|pmc = 1557912|pmid = 4966457|pages = 215–243|volume = 195|issue = 1|first1 = D. H.|last1 = Hubel|first2 = T. N.|last2 = Wiesel|doi=10.1113/jphysiol.1968.sp008455}}</ref>

*[[simple cells (visual cortex)|simple cell]]s, whose output is maximized by straight edges having particular orientations within their receptive field
*[[complex cells (visual cortex)|complex cell]]s, which have larger [[receptive field]]s, whose output is insensitive to the exact position of the edges in the field.

Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.<ref>
{{cite book
|title = Brain and visual perception: the story of a 25-year collaboration
|author = David H. Hubel and Torsten N. Wiesel
|publisher = Oxford University Press US
|year = 2005
|isbn = 978-0-19-517618-6
|page = 106
|url = https://books.google.com/books?id=8YrxWojxUA4C&pg=PA106 }}</ref><ref name=":4">{{cite journal | pmc = 1363130 | pmid=14403679 | volume=148 | issue=3 | title=Receptive fields of single neurones in the cat's striate cortex | date=October 1959 | journal=J. Physiol. | pages=574–91 | last1 = Hubel | first1 = DH | last2 = Wiesel | first2 = TN | doi=10.1113/jphysiol.1959.sp006308}}</ref>

=== Neocognitron, origin of the CNN architecture ===

The "[[neocognitron]]"<ref name=fukuneoscholar>{{cite journal | last1 = Fukushima | first1 = K. | year = 2007 | title = Neocognitron | journal = Scholarpedia | volume = 2 | issue = 1| page = 1717 | doi=10.4249/scholarpedia.1717| bibcode = 2007SchpJ...2.1717F | doi-access = free }}</ref> was introduced by [[Kunihiko Fukushima]] in 1980.<ref name="intro">{{cite journal|last=Fukushima|first=Kunihiko|title=Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position|journal=Biological Cybernetics|year=1980|volume=36|issue=4|pages=193–202|url=http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf|access-date=16 November 2013|doi=10.1007/BF00344251|pmid=7370364|s2cid=206775608}}</ref><ref name=mcdns /><ref>{{cite journal |first1=Yann |last1=LeCun |first2=Yoshua |last2=Bengio |first3=Geoffrey |last3=Hinton |title=Deep learning |journal=Nature |volume=521 |issue=7553 |year=2015 |pages=436–444 |doi=10.1038/nature14539|pmid=26017442 |bibcode=2015Natur.521..436L |s2cid=3074096 }}</ref>
It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.

In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.<ref name="weng1993">{{cite journal |first1=J |last1=Weng |first2=N |last2=Ahuja |first3=TS |last3=Huang |s2cid=8619176 |title=Learning recognition and segmentation of 3-D objects from 2-D images |journal=Proc. 4th International Conf. Computer Vision |year=1993 |pages=121–128 |doi=10.1109/ICCV.1993.378228 |isbn=0-8186-3870-2 }}</ref> Max-pooling is often used in modern CNNs.<ref name="schdeepscholar" />

Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.<ref name=fukuneoscholar /> Today, however, the CNN architecture is usually trained through [[backpropagation]].

The [[neocognitron]] is the first CNN which requires units located at multiple network positions to have shared weights. Neocognitrons were adapted in 1988 to analyze time-varying signals.<ref>{{cite journal|last=Homma|first=Toshiteru|author2=Les Atlas|author3=Robert Marks II|year=1988|title=An Artificial Neural Network for Spatio-Temporal Bipolar Patters: Application to Phoneme Classification|url=http://papers.nips.cc/paper/20-an-artificial-neural-network-for-spatio-temporal-bipolar-patterns-application-to-phoneme-classification.pdf|journal=Advances in Neural Information Processing Systems|volume=1|pages=31–40}}</ref>

=== Time delay neural networks ===
The [[time delay neural network]] (TDNN) was introduced in 1987 by [[Alex Waibel]] et al. and was the first convolutional network, as it achieved shift invariance.<ref name=Waibel1987>{{cite conference |title=Phoneme Recognition Using Time-Delay Neural Networks |last1=Waibel |first1=Alex |date=December 1987 |location=Tokyo, Japan |conference=Meeting of the Institute of Electrical, Information and Communication Engineers (IEICE)}}</ref> It did so by utilizing weight sharing in combination with [[Backpropagation]] training.<ref name="speechsignal">[[Alex Waibel|Alexander Waibel]] et al., ''[http://www.inf.ufrgs.br/~engel/data/media/file/cmp121/waibel89_TDNN.pdf Phoneme Recognition Using Time-Delay Neural Networks]'' IEEE Transactions on Acoustics, Speech, and Signal Processing, Volume 37, No. 3, pp. 328. - 339 March 1989.</ref> Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.<ref name=Waibel1987 />

TDNNs are convolutional networks that share weights along the temporal dimension.<ref>{{cite encyclopedia |last1=LeCun |first1=Yann |last2=Bengio |first2=Yoshua | editor-last=Arbib |editor-first=Michael A. |title=Convolutional networks for images, speech, and time series |encyclopedia=The handbook of brain theory and neural networks |edition=Second |year=1995 |publisher=The MIT press|pages=276–278 |url=https://www.researchgate.net/publication/2453996}}</ref> They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant which performs a two dimensional convolution.<ref name="Hampshire1990">John B. Hampshire and Alexander Waibel, ''[http://papers.nips.cc/paper/213-connectionist-architectures-for-multi-speaker-phoneme-recognition.pdf Connectionist Architectures for Multi-Speaker Phoneme Recognition]'',  Advances in Neural Information Processing Systems, 1990, Morgan Kaufmann.</ref> Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both shifts in time and in frequency. This inspired translation invariance in image processing with CNNs.<ref name="speechsignal"/> The tiling of neuron outputs can cover timed stages.<ref name="video quality" />

TDNNs now achieve the best performance in far distance speech recognition.<ref name=Ko2017>{{cite conference |title=A Study on Data Augmentation of Reverberant Speech for Robust Speech Recognition |last1=Ko |first1=Tom |last2=Peddinti |first2=Vijayaditya |last3=Povey |first3=Daniel |last4=Seltzer |first4=Michael L. |last5=Khudanpur |first5=Sanjeev |date=March 2018 |location=New Orleans, LA, USA |conference=The 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017)|url=https://www.danielpovey.com/files/2017_icassp_reverberation.pdf}}</ref>

==== Max pooling ====
In 1990 Yamaguchi et al. introduced the concept of max pooling. They did so by combining TDNNs with max pooling in order to realize a speaker independent isolated word recognition system.<ref name="Yamaguchi111990" /> In their system they used several TDNNs per word, one for each [[syllable]]. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.

=== Image recognition with CNNs trained by gradient descent ===
A system to recognize hand-written [[ZIP Code]] numbers<ref>Denker, J S , Gardner, W R., Graf, H. P, Henderson, D, Howard, R E, Hubbard, W, Jackel, L D , BaIrd, H S, and Guyon (1989) [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.852.5499&rep=rep1&type=pdf Neural network recognizer for hand-written zip code digits], AT&T Bell Laboratories</ref> involved convolutions in which the kernel coefficients had been laboriously hand designed.<ref name=":2">Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, [http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf Backpropagation Applied to Handwritten Zip Code Recognition]; AT&T Bell Laboratories</ref>

[[Yann LeCun]] et al. (1989)<ref name=":2" /> used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.

This approach became a foundation of modern [[computer vision]].

==== LeNet-5 ====
{{main|LeNet}}
LeNet-5, a pioneering 7-level convolutional network by [[Yann LeCun|LeCun]] et al. in 1998,<ref name="lecun98">{{cite journal|last=LeCun|first=Yann|author2=Léon Bottou |author3=Yoshua Bengio |author4=Patrick Haffner |title=Gradient-based learning applied to document recognition|journal=Proceedings of the IEEE|year=1998|volume=86|issue=11|pages=2278–2324|url=http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf|access-date=October 7, 2016|doi=10.1109/5.726791|citeseerx=10.1.1.32.9552}}</ref> that classifies digits, was applied by several banks to recognize hand-written numbers on checks ({{Lang-en-GB|cheques}}) digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.

=== Shift-invariant neural network ===

Similarly, a shift invariant neural network was proposed by W. Zhang et al. for image character recognition in 1988.<ref name=":0" /><ref name=":1" /> The architecture and training algorithm were modified in 1991<ref>{{Cite journal|last=Zhang|first=Wei|date=1991|title=Error Back Propagation with Minimum-Entropy Weights: A Technique for Better Generalization of 2-D Shift-Invariant NNs|url=https://drive.google.com/file/d/0B65v6Wo67Tk5dkJTcEMtU2c5Znc/view?usp=sharing|journal=Proceedings of the International Joint Conference on Neural Networks}}</ref> and applied for medical image processing<ref>{{Cite journal|last=Zhang|first=Wei|date=1991|title=Image processing of human corneal endothelium based on a learning network|url=https://drive.google.com/file/d/0B65v6Wo67Tk5cm5DTlNGd0NPUmM/view?usp=sharing|journal=Applied Optics|volume=30|issue=29|pages=4211–7|doi=10.1364/AO.30.004211|pmid=20706526|bibcode=1991ApOpt..30.4211Z}}</ref> and automatic detection of breast cancer in [[Mammography|mammograms]].<ref>{{Cite journal|last=Zhang|first=Wei|date=1994|title=Computerized detection of clustered microcalcifications in digital mammograms using a shift-invariant artificial neural network|url=https://drive.google.com/file/d/0B65v6Wo67Tk5Ml9qeW5nQ3poVTQ/view?usp=sharing|journal=Medical Physics|volume=21|issue=4|pages=517–24|doi=10.1118/1.597177|pmid=8058017|bibcode=1994MedPh..21..517Z}}</ref>

A different convolution-based design was proposed in 1988<ref>Daniel Graupe, Ruey Wen Liu, George S Moschytz."[https://www.researchgate.net/profile/Daniel_Graupe2/publication/241130197_Applications_of_signal_and_image_processing_to_medicine/links/575eef7e08aec91374b42bd2.pdf Applications of neural networks to medical signal processing]". In Proc. 27th IEEE Decision and Control Conf.,  pp. 343–347, 1988.</ref> for application to decomposition of one-dimensional [[electromyography]] convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.<ref>Daniel Graupe, Boris Vern, G. Gruener, Aaron Field, and Qiu Huang. "[https://ieeexplore.ieee.org/abstract/document/100522/ Decomposition of surface EMG signals into single fiber action potentials by means of neural network]". Proc. IEEE International Symp. on Circuits and Systems, pp. 1008–1011, 1989.</ref><ref>Qiu Huang, Daniel Graupe, Yi Fang Huang, Ruey Wen Liu."[http://www.academia.edu/download/42092095/graupe_huang_q_huang_yf_liu_rw_1989.pdf Identification of firing patterns of neuronal signals]." In Proc. 28th IEEE Decision and Control Conf., pp. 266–271, 1989.</ref>

=== Neural abstraction pyramid ===
[[File:Neural Abstraction Pyramid.jpg|alt=Neural Abstraction Pyramid|thumb|Neural abstraction pyramid]]
The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid<ref>{{cite book
|last1=Behnke
|first1=Sven
|year=2003
|title=Hierarchical Neural Networks for Image Interpretation
|url=https://www.ais.uni-bonn.de/books/LNCS2766.pdf
|series=Lecture Notes in Computer Science
|volume=2766
|publisher=Springer
|doi=10.1007/b11963
|isbn=978-3-540-40722-5
|s2cid=1304548
}}</ref> by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.

=== GPU implementations ===

Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on [[graphics processing unit]]s (GPUs).

In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on [[CPU]].<ref>{{cite journal|last1=Oh|first1=KS|last2=Jung|first2=K|title=GPU implementation of neural networks.|journal=Pattern Recognition|date=2004|volume=37|issue=6|pages=1311–1314|doi=10.1016/j.patcog.2004.01.013}}</ref><ref name="schdeepscholar">{{cite journal|last1=Schmidhuber|first1=Jürgen|title=Deep Learning|journal=Scholarpedia|url=http://www.scholarpedia.org/article/Deep_Learning|date=2015|volume=10|issue=11|pages=1527–54|pmid=16764513|doi=10.1162/neco.2006.18.7.1527|citeseerx=10.1.1.76.1541|s2cid=2309950}}</ref> In 2005, another paper also emphasised the value of [[GPGPU]] for [[machine learning]].<ref>{{cite book|author1=Dave Steinkraus|author2=Patrice Simard|author3=Ian Buck|title=12th International Conference on Document Analysis and Recognition (ICDAR 2005)|date=2005|pages=1115–1119|chapter-url=http://www.computer.org/csdl/proceedings/icdar/2005/2420/00/24201115-abs.html|archive-date=2016-03-14|chapter=Using GPUs for Machine Learning Algorithms}}</ref>

The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.<ref>{{cite book|author1=Kumar Chellapilla|author2=Sid Puri|author3=Patrice Simard|editor1-last=Lorette|editor1-first=Guy|title=Tenth International Workshop on Frontiers in Handwriting Recognition|date=2006|publisher=Suvisoft|chapter-url=https://hal.inria.fr/inria-00112631/document|archive-date=2016-03-14|chapter=High Performance Convolutional Neural Networks for Document Processing}}</ref> Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.<ref>{{cite journal|last1=Hinton|first1=GE|last2=Osindero|first2=S|last3=Teh|first3=YW|title=A fast learning algorithm for deep belief nets.|journal=Neural Computation|date=Jul 2006|volume=18|issue=7|pages=1527–54|pmid=16764513|doi=10.1162/neco.2006.18.7.1527|citeseerx=10.1.1.76.1541|s2cid=2309950}}</ref><ref>{{cite journal|last1=Bengio|first1=Yoshua|last2=Lamblin|first2=Pascal|last3=Popovici|first3=Dan|last4=Larochelle|first4=Hugo|title=Greedy Layer-Wise Training of Deep Networks|journal=Advances in Neural Information Processing Systems|date=2007|pages=153–160|url=http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf}}</ref><ref>{{cite journal|last1=Ranzato|first1=MarcAurelio|last2=Poultney|first2=Christopher|last3=Chopra|first3=Sumit|last4=LeCun|first4=Yann|title=Efficient Learning of Sparse Representations with an Energy-Based Model|journal=Advances in Neural Information Processing Systems|date=2007|url=http://yann.lecun.com/exdb/publis/pdf/ranzato-06.pdf}}</ref><ref>{{cite journal|last1=Raina|first1=R|last2=Madhavan|first2=A|last3=Ng|first3=Andrew|title=Large-scale deep unsupervised learning using graphics processors.|journal=ICML|date=2009|pages=873–880|url=http://robotics.stanford.edu/~ang/papers/icml09-LargeScaleUnsupervisedDeepLearningGPU.pdf}}</ref>

In 2010, Dan Ciresan et al. at [[IDSIA]] showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as [[backpropagation]]. Their network outperformed previous machine learning methods on the [[MNIST]] handwritten digits benchmark.<ref>{{cite journal|last1=Ciresan|first1=Dan|last2=Meier|first2=Ueli|last3=Gambardella|first3=Luca|last4=Schmidhuber|first4=Jürgen|title=Deep big simple neural nets for handwritten digit recognition.|journal=Neural Computation|date=2010|volume=22|issue=12|pages=3207–3220|doi=10.1162/NECO_a_00052|pmid=20858131|arxiv=1003.0358|s2cid=1918673}}</ref> In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results.<ref name="flexible">{{cite journal|last=Ciresan|first=Dan|author2=Ueli Meier |author3=Jonathan Masci |author4=Luca M. Gambardella |author5=Jurgen Schmidhuber |title=Flexible, High Performance Convolutional Neural Networks for Image Classification|journal=Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Volume Two|year=2011|volume=2|pages=1237–1242|url=http://www.idsia.ch/~juergen/ijcai2011.pdf|access-date=17 November 2013}}</ref> In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time.<ref>{{Cite web|url=http://benchmark.ini.rub.de/?section=gtsrb&subsection=results|title=IJCNN 2011 Competition result table|website=OFFICIAL IJCNN2011 COMPETITION|language=en-US|access-date=2019-01-14|date=2010}}</ref> Between May 15, 2011 and September 30, 2012, their CNNs won no less than four image competitions.<ref>{{Cite web|url=http://people.idsia.ch/~juergen/computer-vision-contests-won-by-gpu-cnns.html|last1=Schmidhuber|first1=Jürgen|title=History of computer vision contests won by deep CNNs on GPU|language=en-US|access-date=14 January 2019|date=17 March 2017}}</ref><ref name="schdeepscholar" /> In 2012, they also significantly improved on the best performance in the literature for multiple image [[database]]s, including the [[MNIST database]], the NORB database, the HWDB1.0 dataset (Chinese characters) and the [[CIFAR-10|CIFAR10 dataset]] (dataset of 60000 32x32 labeled [[RGB images]]).<ref name="mcdns" />

Subsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the [[ImageNet Large Scale Visual Recognition Challenge]] 2012.<ref name=":02" /> A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.<ref>{{cite journal|last1=He|first1=Kaiming|last2=Zhang|first2=Xiangyu|last3=Ren|first3=Shaoqing|last4=Sun|first4=Jian|title=Deep Residual Learning for Image Recognition.|journal= 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)|pages=770–778|date=2016|url=http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf|doi=10.1109/CVPR.2016.90|arxiv=1512.03385|isbn=978-1-4673-8851-1|s2cid=206594692}}</ref>

=== Intel Xeon Phi implementations ===

Compared to the training of CNNs using [[GPU]]s, not much attention was given to the [[Intel Xeon Phi]] [[coprocessor]].<ref>
{{cite book
 | last1= Viebke
 | first1= Andre
 | last2= Pllana
 | first2= Sabri
 | title= 2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems
 | chapter= The Potential of the Intel (R) Xeon Phi for Supervised Deep Learning
 | pages= 758–765
 | website= IEEE Xplore
 | publisher= IEEE 2015
 | doi= 10.1109/HPCC-CSS-ICESS.2015.45
 | isbn= 978-1-4799-8937-9
 | year= 2015
 | s2cid= 15411954
 | chapter-url= http://urn.kb.se/resolve?urn=urn:nbn:se:lnu:diva-47951
 }}
</ref>
A notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).<ref>
{{cite journal
 | last1= Viebke
 | first1= Andre
 | last2= Memeti
 | first2= Suejb
 | last3= Pllana
 | first3= Sabri
 | last4= Abraham
 | first4= Ajith
 | title= CHAOS: a parallelization scheme for training convolutional neural networks on Intel Xeon Phi
 | journal= The Journal of Supercomputing
 | date= 2019
 | volume= 75
 | issue= 1
 | pages= 197–227
 | doi= 10.1007/s11227-017-1994-x
 | arxiv= 1702.07908
| s2cid= 14135321
 }}
</ref>
CHAOS exploits both the thread- and [[SIMD]]-level parallelism that is available on the Intel Xeon Phi.

== Distinguishing features ==
In the past, traditional [[multilayer perceptron]] (MLP) models have been used for image recognition.{{examples|date=October 2017}} However, due to the full connectivity between nodes, they suffered from the [[curse of dimensionality]], and did not scale well with higher resolution images. A 1000×1000-pixel image with [[RGB color model|RGB color]] channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.
[[File:Conv layers.png|left|thumb|237x237px|CNN layers arranged in 3 dimensions]]
For example, in [[CIFAR-10]], images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.

Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores [[locality of reference]] in image data, both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by [[Spatial locality|spatially local]] input patterns.

Convolutional neural networks are biologically inspired variants of multilayer perceptrons that are designed to emulate the behavior of a [[visual cortex]]. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:
* 3D volumes of neurons. The layers of a CNN have neurons arranged in [[Three-dimensional space|3 dimensions]]: width, height and depth.{{citation needed|date=March 2019}} where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.
* Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned "[[Filter (signal processing)|filters]]" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to [[Nonlinear filter|non-linear filters]] that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.
* Shared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting feature map to be [[Equivariant map|equivariant]] under changes in the locations of input features in the visual field, i.e. they grant translational equivariance.
* Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of [[Translational symmetry|translational invariance]] to the features contained therein, allowing the CNN to be more robust to variations in their positions.

Together, these properties allow CNNs to achieve better generalization on [[Computer vision|vision problems]]. Weight sharing dramatically reduces the number of [[free parameter]]s learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.

== Building blocks ==
{{More citations needed section|date=June 2017}}

A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.[[File:Conv layer.png|left|thumb|Neurons of a convolutional layer (blue), connected to their receptive field (red)|229x229px]]

=== Convolutional layer ===
The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable [[Filter (signal processing)|filters]] (or [[Kernel (image processing)|kernels]]), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is [[Convolution|convolved]] across the width and height of the input volume, computing the [[dot product]] between the entries of the filter and the input and producing a 2-dimensional [[Activation function|activation map]] of that filter. As a result, the network learns filters that activate when it detects some specific type of [[Feature (machine learning)|feature]] at some spatial position in the input.<ref name="Géron Hands-on ML 2019">{{cite book
|last1=Géron
|first1=Aurélien
|title=Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow
|date=2019
|publisher=O'Reilly Media
|location=Sebastopol, CA
|isbn=978-1-492-03264-9
}}, pp. 448</ref>
<ref group="nb">When applied to other types of data than image data, such as sound data, "spatial position" may variously correspond to different points in the [[time domain]], [[frequency domain]] or other [[Space (mathematics)|mathematical spaces]].</ref>

Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.

==== Local connectivity ====

[[File:Typical cnn.png|thumb|395x395px|Typical CNN architecture]]

When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a [[Sparse network|sparse local connectivity]] pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.

The extent of this connectivity is a [[Hyperparameter optimization|hyperparameter]] called the [[receptive field]] of the neuron. The connections are [[Spatial Locality|local in space]] (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.

==== Spatial arrangement ====

Three [[Hyperparameter (machine learning)|hyperparameters]] control the size of the output volume of the convolutional layer: the depth, [[Stride of an array|stride]] and zero-padding.

* The ''<u>depth</u>'' of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.
*<u>''Stride''</u> controls how depth columns around the spatial dimensions (width and height) are allocated. When the stride is 1 then we move the filters one pixel at a time. This leads to heavily [[Intersection (set theory)|overlapping]] receptive fields between the columns, and also to large output volumes. When the stride is 2 then the filters jump 2 pixels at a time as they slide around. Similarly, for any integer <math display="inline">S > 0,</math> a stride of ''S'' causes the filter to be translated by ''S'' units at a time per output. In practice, stride lengths of <math display="inline">S \geq 3</math> are rare. The receptive fields overlap less and the resulting output volume has smaller spatial dimensions when stride length is increased.<ref>{{Cite web|url=https://cs231n.github.io/convolutional-networks/|title=CS231n Convolutional Neural Networks for Visual Recognition|website=cs231n.github.io|access-date=2017-04-25}}</ref>
* Sometimes it is convenient to pad the input with zeros on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume.

The spatial size of the output volume can be computed as a function of the input volume size <math>W</math>, the kernel field size of the convolutional layer neurons <math>K</math>, the stride with which they are applied <math>S</math>, and the amount of zero padding <math>P</math> used on the border. The formula for calculating how many neurons "fit" in a given volume is given by

<math display="block">\frac{W-K+2P}{S} + 1.</math>

If this number is not an [[integer]], then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a [[Symmetry|symmetric]] way. In general, setting zero padding to be <math display="inline">P = (K-1)/2</math> when the stride is <math>S=1</math> ensures that the input volume and output volume will have the same size spatially. However, it's not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.

==== Parameter sharing ====

A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a ''depth slice'', the neurons in each depth slice are constrained to use the same weights and bias.

Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a [[convolution]] of the neuron's weights with the input volume.<ref group="nb">hence the name "convolutional layer"</ref> Therefore, it is common to refer to the sets of weights as a filter (or a [[Kernel (image processing)|kernel]]), which is convolved with the input. The result of this convolution is an [[Activation function|activation map]], and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the [[Translational symmetry|translation invariance]] of the CNN architecture.

Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a "locally connected layer".

=== Pooling layer ===
[[File:Max pooling.png|thumb|314x314px|Max pooling with a 2x2 filter and stride = 2]]
Another important concept of CNNs is pooling, which is a form of non-linear [[Downsampling (signal processing)|down-sampling]]. There are several non-linear functions to implement pooling among which ''max pooling'' is the most common. It [[Partition of a set|partitions]] the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.

Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, [[memory footprint]] and amount of computation in the network, and hence to also control [[overfitting]]. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by a [[#ReLU layer|ReLU layer]]) in a CNN architecture.<ref name="Géron Hands-on ML 2019"/>{{rp|460–461}} The pooling operation can be used as another form of translation invariance.<ref name="Géron Hands-on ML 2019"/>{{rp|458}}

The pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2×2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations:
<math display="block">f_{X,Y}(S)=\max_{a,b=0}^1S_{2X+a,2Y+b}.</math>
In this case, every [[Maximum|max operation]] is over 4 numbers. The depth dimension remains unchanged.

In addition to max pooling, pooling units can use other functions, such as [[average]] pooling or [[Euclidean norm|ℓ<sub>2</sub>-norm]] pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which performs better in practice.<ref name="Scherer-ICANN-2010">{{cite conference 
| url =http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf 
| title =Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition 
| last1 =Scherer 
| first1 =Dominik 
| last2 =Müller 
| first2 =Andreas C. 
| last3 =Behnke 
| first3 =Sven 
| year =2010 
| publisher =Springer 
| book-title =Artificial Neural Networks (ICANN), 20th International Conference on 
| pages =92–101 
| location =Thessaloniki, Greece 
}}</ref>

Due to the aggressive reduction in the size of the representation,{{Which|date=December 2018}} there is a recent trend towards using smaller filters<ref>{{cite arXiv|title = Fractional Max-Pooling|eprint= 1412.6071|date = 2014-12-18|first = Benjamin|last = Graham|class= cs.CV}}</ref> or discarding pooling layers altogether.<ref>{{cite arXiv|title = Striving for Simplicity: The All Convolutional Net|eprint= 1412.6806|date = 2014-12-21|first1 = Jost Tobias|last1 = Springenberg|first2 = Alexey|last2 = Dosovitskiy|first3 = Thomas|last3 = Brox|first4 = Martin|last4 = Riedmiller|class= cs.LG}}</ref>

[[File:RoI pooling animated.gif|thumb|400x300px|RoI pooling to size 2x2. In this example region proposal (an input parameter) has size 7x5.]]
"[[Region of interest|Region of Interest]]" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.<ref>{{Cite web
  | last = Grel
  | first = Tomasz
  | title = Region of interest pooling explained
  | website = deepsense.io
  | date = 2017-02-28
  | url = https://deepsense.io/region-of-interest-pooling-explained/
  | access-date = <!-----5 April 2017----->|language=en}}</ref>

Pooling is an important component of convolutional neural networks for [[object detection]] based on Fast R-CNN<ref name="rcnn">{{cite arXiv
|title = Fast R-CNN
|eprint= 1504.08083
|date = 2015-09-27
|first = Ross
|last = Girshick
|class= cs.CV}}</ref> architecture.

=== ReLU layer ===
ReLU is the abbreviation of [[Rectifier (neural networks)|rectified linear unit]], which applies the non-saturating [[activation function]] <math alt="function of x equals maximum between zero and x" display="inline">f(x)=\max(0,x)</math>.<ref name=":02">{{Cite journal|last1=Krizhevsky|first1=Alex|last2=Sutskever|first2=Ilya|last3=Hinton|first3=Geoffrey E.|date=2017-05-24|title=ImageNet classification with deep convolutional neural networks|url=https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf|journal=Communications of the ACM|volume=60|issue=6|pages=84–90|doi=10.1145/3065386|s2cid=195908774|issn=0001-0782}}</ref> It effectively removes negative values from an activation map by setting them to zero.<ref name="Romanuke4">{{cite journal |last1=Romanuke |first1=Vadim |title=Appropriate number and allocation of ReLUs in convolutional neural networks |journal=Research Bulletin of NTUU "Kyiv Polytechnic Institute" |date=2017 |volume=1 |pages=69–78|doi=10.20535/1810-0546.2017.1.88156|doi-access=free }}</ref> It increases the [[Nonlinearity|nonlinear properties]] of the [[Decision boundary|decision function]] and of the overall network without affecting the receptive fields of the convolution layer.

Other functions are also used to increase nonlinearity, for example the saturating [[hyperbolic tangent]] <math alt="function of x equals hyperbolic tangent of x">f(x)=\tanh(x)</math>, <math alt="function of x equals absolute value of the hyperbolic tangent of x">f(x)=|\tanh(x)|</math>, and the [[sigmoid function]] <math alt="function of x equals the inverse of one plus e to the power of minus x" display="inline">\sigma(x)=(1+e^{-x} )^{-1}</math>. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to [[Generalization (learning)|generalization]] accuracy.<ref>{{cite journal|last=Krizhevsky|first=A.|author2=Sutskever, I. |author3=Hinton, G. E. |title=Imagenet classification with deep convolutional neural networks|journal=Advances in Neural Information Processing Systems |volume=1|year=2012|pages=1097–1105|url=http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}}</ref>

=== Fully connected layer ===
Finally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) [[artificial neural network]]s. Their activations can thus be computed as an [[affine transformation]], with [[matrix multiplication]] followed by a bias offset ([[vector addition]] of a learned or fixed bias term).{{citation needed|date=November 2020}}

=== Loss layer ===
{{Main|Loss function|Loss functions for classification}}
The "loss layer" specifies how [[training]] penalizes the deviation between the predicted (output) and [[Ground truth|true]] labels and is normally the final layer of a neural network. Various [[loss function]]s appropriate for different tasks may be used.

[[Softmax function|Softmax]] loss is used for predicting a single class of ''K'' mutually exclusive classes.<ref group="nb">So-called [[categorical data]].</ref> [[Sigmoid function|Sigmoid]] [[Cross entropy|cross-entropy]] loss is used for predicting ''K'' independent probability values in <math>[0,1]</math>. [[Euclidean distance|Euclidean]] loss is used for [[Regression (machine learning)|regressing]] to [[Real number|real-valued]] labels <math>(-\infty,\infty)</math>.

== Choosing hyperparameters ==
{{More citations needed section|date=June 2017}}
CNNs use more [[Hyperparameter (machine learning)|hyperparameters]] than a standard multilayer perceptron (MLP). While the usual rules for [[learning rate]]s and [[Regularization (mathematics)|regularization]] constants still apply, the following should be kept in mind when optimizing.

=== Number of filters ===

Since feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values ''v<sub>a</sub>'' with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.

The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.

=== Filter shape ===

Common filter shapes found in the literature vary greatly, and are usually chosen based on the dataset.

The challenge is, thus, to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset, and without [[overfitting]].

=== Max pooling shape ===

Typical values are 2×2. Very large input volumes may warrant 4×4 pooling in the lower layers.<ref>{{Cite web|url=https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html|title=The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)|last=Deshpande|first=Adit|website=adeshpande3.github.io|access-date=2018-12-04}}</ref> However, choosing larger shapes will dramatically [[Dimensionality reduction|reduce the dimension]] of the signal, and may result in excess [[Data loss|information loss]]. Often, non-overlapping pooling windows perform best.<ref name="Scherer-ICANN-2010" />

== Regularization methods ==
{{main|Regularization (mathematics)}}{{More citations needed section|date=June 2017}}
[[Regularization (mathematics)|Regularization]] is a process of introducing additional information to solve an [[ill-posed problem]] <nowiki/>or to prevent [[overfitting]]. CNNs use various types of regularization.

=== Empirical ===

==== Dropout ====
Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is [[Dropout (neural networks)|dropout]].<ref>{{cite journal|last=Srivastava|first=Nitish|author2=C. Geoffrey Hinton|author3=Alex Krizhevsky |author4=Ilya Sutskever |author5=Ruslan Salakhutdinov|title=Dropout: A Simple Way to Prevent Neural Networks from overfitting|journal=Journal of Machine Learning Research |year=2014|volume=15|issue=1|pages=1929–1958|url=http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf}}</ref><ref name="DLPATTERNS">{{Cite web | title=A Pattern Language for Deep Learning| author=Carlos E. Perez| url=http://www.deeplearningpatterns.com}}</ref> At each training stage, individual nodes are either "dropped out" of the net with probability <math>1-p</math> or kept with probability <math>p</math>, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.

In the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, however, this probability is typically much lower, since information is directly lost when input nodes are ignored or dropped.

At testing time after training has finished, we would ideally like to find a sample average of all possible <math>2^n</math> dropped-out networks; unfortunately this is unfeasible for large values of <math>n</math>. However, we can find an approximation by using the full network with each node's output weighted by a factor of <math>p</math>, so the [[expected value]] of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates <math>2^n</math> neural nets, and as such allows for model combination, at test time only a single network needs to be tested.

By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for [[deep neural network]]s. The technique seems to reduce node interactions, leading them to learn more robust features{{Clarify|reason=|date=December 2018}} that better generalize to new data.

==== DropConnect ====

DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability <math>1-p</math>. Each unit thus receives input from a random subset of units in the previous layer.<ref>{{Cite journal|title = Regularization of Neural Networks using DropConnect {{!}} ICML 2013 {{!}} JMLR W&CP|pages = 1058–1066|url = http://jmlr.org/proceedings/papers/v28/wan13.html|website = jmlr.org|access-date = 2015-12-17|date = 2013-02-13}}</ref>

DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.

==== Stochastic pooling ====

A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.

In stochastic pooling,<ref>{{cite arXiv|title = Stochastic Pooling for Regularization of Deep Convolutional Neural Networks|eprint= 1301.3557|date = 2013-01-15|first1 = Matthew D.|last1 = Zeiler|first2 = Rob|last2 = Fergus|class= cs.LG}}</ref> the conventional [[Deterministic algorithm|deterministic]] pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a [[multinomial distribution]], given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and [[data augmentation]].

An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local [[Deformation theory|deformations]]. This is similar to explicit [[elastic deformation]]s of the input images,<ref name=":3" /> which delivers excellent performance on the [[MNIST database|MNIST data set]].<ref name=":3">{{Cite journal|title = Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis – Microsoft Research|url = http://research.microsoft.com/apps/pubs/?id=68920|journal = Microsoft Research|access-date = 2015-12-17|date = August 2003|last1 = Platt|first1 = John|last2 = Steinkraus|first2 = Dave|last3 = Simard|first3 = Patrice Y.}}</ref> Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.

==== Artificial data ====
{{Main|Data augmentation}}
Since the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.<ref>{{Cite arXiv|title = Improving neural networks by preventing co-adaptation of feature detectors|eprint=1207.0580|last1= Hinton|first1=Geoffrey E.|last2=Srivastava|first2=Nitish|last3=Krizhevsky|first3=Alex|last4=Sutskever|first4=Ilya|last5= Salakhutdinov|first5=Ruslan R.|class=cs.NE|year=2012}}</ref>

=== Explicit ===

==== Early stopping ====
{{main|Early stopping}}
One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.

==== Number of parameters ====
Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a "[[zero norm]]".

==== Weight decay ====
A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights ([[L1-norm|L1 norm]]) or squared magnitude ([[L2 norm]]) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.

L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.

L1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called [[Elastic net regularization]]). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.

==== Max norm constraints ====
Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use [[Sparse approximation#Projected Gradient Descent|projected gradient descent]] to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector <math>\vec{w}</math> of every neuron to satisfy <math>\|\vec{w}\|_{2}<c</math>. Typical values of <math>c</math> are order of 3–4. Some papers report improvements<ref>{{Cite web|title = Dropout: A Simple Way to Prevent Neural Networks from Overfitting|url = http://jmlr.org/papers/v15/srivastava14a.html|website = jmlr.org|access-date = 2015-12-17}}</ref> when using this form of regularization.

== Hierarchical coordinate frames ==
Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.<ref>{{cite journal | last1 = Hinton | first1 = Geoffrey | year = 1979 | title = Some demonstrations of the effects of structural descriptions in mental imagery | journal = Cognitive Science | volume = 3 | issue = 3| pages = 231–250 | doi=10.1016/s0364-0213(79)80008-7}}</ref>

Currently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the [[retina]]. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.<ref>Rock, Irvin. "The frame of reference." The legacy of Solomon Asch: Essays in cognition and social psychology (1990): 243–268.</ref>

Thus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose ("pose vectors") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human [[visual system]] imposes coordinate frames in order to represent shapes.<ref>J. Hinton, Coursera lectures on Neural Networks, 2012, Url: https://www.coursera.org/learn/neural-networks {{Webarchive|url=https://web.archive.org/web/20161231174321/https://www.coursera.org/learn/neural-networks |date=2016-12-31 }}</ref>

== Applications ==

=== Image recognition ===
CNNs are often used in [[image recognition]] systems. In 2012 an [[Per-comparison error rate|error rate]] of 0.23 percent on the [[MNIST database]] was reported.<ref name="mcdns" /> Another paper on using CNN for image classification reported that the learning process was "surprisingly fast"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.<ref name="flexible" /> Subsequently, a similar CNN called 
[[AlexNet]]<ref name=quartz>{{cite web
|website=[[Quartz (website)|Quartz]]
|author=Dave Gershgorn
|title=The inside story of how AI got good enough to dominate Silicon Valley
|url=https://qz.com/1307091/the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley/
|date=18 June 2018
|access-date=5 October 2018
}}</ref> won the [[ImageNet Large Scale Visual Recognition Challenge]] 2012.

When applied to [[facial recognition system|facial recognition]], CNNs achieved a large decrease in error rate.<ref>{{cite journal|last=Lawrence|first=Steve|author2=C. Lee Giles |author3=Ah Chung Tsoi |author4=Andrew D. Back |title=Face Recognition: A Convolutional Neural Network Approach|journal=IEEE Transactions on Neural Networks|year=1997|volume=8|issue=1|pages=98–113|citeseerx = 10.1.1.92.5813|doi=10.1109/72.554195|pmid=18255614}}</ref> Another paper reported a 97.6 percent recognition rate on "5,600 still images of more than 10 subjects".<ref name="robust face detection" /> CNNs were used to assess [[video quality]] in an objective way after manual training; the resulting system had a very low [[root mean square error]].<ref name="video quality">{{cite journal|last=Le Callet|first=Patrick|author2=Christian Viard-Gaudin|author3=Dominique Barba|year=2006|title=A Convolutional Neural Network Approach for Objective Video Quality Assessment|url=http://hal.univ-nantes.fr/docs/00/28/74/26/PDF/A_convolutional_neural_network_approach_for_objective_video_quality_assessment_completefinal_manuscript.pdf|journal=IEEE Transactions on Neural Networks|volume=17|issue=5|pages=1316–1327|doi=10.1109/TNN.2006.879766|pmid=17001990|s2cid=221185563|access-date=17 November 2013}}</ref>

The [[ImageNet Large Scale Visual Recognition Challenge]] is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,<ref name="ILSVRC2014">{{cite web|url=http://www.image-net.org/challenges/LSVRC/2014/results|title=ImageNet Large Scale Visual Recognition Competition 2014 (ILSVRC2014)|access-date=30 January 2016}}</ref> a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner [[GoogLeNet]]<ref name=googlenet>{{cite journal|first1=Christian |last1=Szegedy  |first2=Wei |last2=Liu |first3=Yangqing |last3=Jia|first4=Pierre |last4=Sermanet|first5=Scott |last5=Reed|first6=Dragomir |last6=Anguelov|first7=Dumitru |last7=Erhan|first8=Vincent |last8=Vanhoucke|first9=Andrew |last9=Rabinovich|title = Going Deeper with Convolutions|url=https://archive.org/details/arxiv-1409.4842 |journal= Computing Research Repository|year=2014 |arxiv= 1409.4842|bibcode=2014arXiv1409.4842S}}</ref> (the foundation of [[DeepDream]]) increased the mean average [[Precision and recall|precision]] of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.<ref>{{cite arXiv|eprint=1409.0575|last1=Russakovsky|first1=Olga|title=Image ''Net'' Large Scale Visual Recognition Challenge|last2=Deng|first2=Jia|last3=Su|first3=Hao|last4=Krause|first4=Jonathan|last5=Satheesh|first5=Sanjeev|last6=Ma|first6=Sean|last7=Huang|first7=Zhiheng|last8=Karpathy|first8=Andrej|author-link8=Andrej Karpathy|last9=Khosla|first9=Aditya|last10=Bernstein|first10=Michael|last11= Berg|first11=Alexander C.|last12=Fei-Fei|first12=Li|class=cs.CV|year=2014|author1-link=Olga Russakovsky}}</ref> The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.{{citation needed|date=June 2019}}

In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.<ref>{{Cite news|url = http://www.technologyreview.com/view/535201/the-face-detection-algorithm-set-to-revolutionize-image-search|title = The Face Detection Algorithm Set To Revolutionize Image Search|date = February 16, 2015|work = Technology Review|access-date = 27 October 2017}}</ref>

=== Video analysis ===
Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.<ref>{{Cite book|publisher = Springer Berlin Heidelberg|date = 2011-11-16|isbn = 978-3-642-25445-1|pages = 29–39|series = Lecture Notes in Computer Science|first1 = Moez|last1 = Baccouche|first2 = Franck|last2 = Mamalet|first3 = Christian|last3 = Wolf|first4 = Christophe|last4 = Garcia|first5 = Atilla|last5 = Baskurt|editor-first = Albert Ali|editor-last = Salah|editor-first2 = Bruno|editor-last2 = Lepri|doi = 10.1007/978-3-642-25446-8_4|chapter = Sequential Deep Learning for Human Action Recognition|title = Human Behavior Unterstanding|volume = 7065|citeseerx = 10.1.1.385.4740}}</ref><ref>{{Cite journal|title = 3D Convolutional Neural Networks for Human Action Recognition|journal = IEEE Transactions on Pattern Analysis and Machine Intelligence|date = 2013-01-01|issn = 0162-8828|pages = 221–231|volume = 35|issue = 1|doi = 10.1109/TPAMI.2012.59|pmid = 22392705|first1 = Shuiwang|last1 = Ji|first2 = Wei|last2 = Xu|first3 = Ming|last3 = Yang|first4 = Kai|last4 = Yu|citeseerx = 10.1.1.169.4046|s2cid = 1923924}}</ref> Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.<ref>{{cite arxiv | last1=Huang | first1=Jie | last2=Zhou | first2=Wengang | last3=Zhang | first3=Qilin | last4=Li | first4=Houqiang | last5=Li | first5=Weiping | title=Video-based Sign Language Recognition without Temporal Segmentation |eprint=1801.10111| class=cs.CV | year=2018 }}</ref><ref>Karpathy, Andrej, et al. "[https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf Large-scale video classification with convolutional neural networks]." IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014.</ref><ref>{{cite arXiv|eprint=1406.2199|last1=Simonyan|first1=Karen|title=Two-Stream Convolutional Networks for Action Recognition in Videos|last2=Zisserman|first2=Andrew|class=cs.CV|year=2014}} (2014).</ref> [[Long short-term memory]] (LSTM) [[Recurrent neural network|recurrent]] units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.<ref name="Wang Duan Zhang Niu p=1657">{{cite journal | last1=Wang | first1=Le | last2=Duan | first2=Xuhuan | last3=Zhang | first3=Qilin | last4=Niu | first4=Zhenxing | last5=Hua | first5=Gang | last6=Zheng | first6=Nanning | title=Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation | journal=Sensors | volume=18 | issue=5 | date=2018-05-22 | issn=1424-8220 | doi=10.3390/s18051657 | pmid=29789447 | pmc=5982167 | page=1657 | url=https://qilin-zhang.github.io/_pages/pdfs/Segment-Tube_Spatio-Temporal_Action_Localization_in_Untrimmed_Videos_with_Per-Frame_Segmentation.pdf}}</ref><ref name="Duan Wang Zhai Zheng 2018 p. ">{{cite conference | last1=Duan | first1=Xuhuan | last2=Wang | first2=Le | last3=Zhai | first3=Changbo | last4=Zheng | first4=Nanning | last5=Zhang | first5=Qilin | last6=Niu | first6=Zhenxing | last7=Hua | first7=Gang | title=Joint Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation | publisher=25th IEEE International Conference on Image Processing (ICIP)
| year=2018 | isbn=978-1-4799-7061-2 | doi=10.1109/icip.2018.8451692 }}</ref> [[Unsupervised learning]] schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted [[Boltzmann machine|Boltzmann Machines]]<ref>{{Cite book|title = Convolutional Learning of Spatio-temporal Features|url = http://dl.acm.org/citation.cfm?id=1888212.1888225|publisher = Springer-Verlag|journal = Proceedings of the 11th European Conference on Computer Vision: Part VI|date = 2010-01-01|location = Berlin, Heidelberg|isbn = 978-3-642-15566-6|pages = 140–153|series = ECCV'10|first1 = Graham W.|last1 = Taylor|first2 = Rob|last2 = Fergus|first3 = Yann|last3 = LeCun|first4 = Christoph|last4 = Bregler}}</ref> and Independent Subspace Analysis.<ref>{{Cite book|title = Learning Hierarchical Invariant Spatio-temporal Features for Action Recognition with Independent Subspace Analysis|publisher = IEEE Computer Society|journal = Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition|date = 2011-01-01|location = Washington, DC, USA|isbn = 978-1-4577-0394-2|pages = 3361–3368|series = CVPR '11|doi = 10.1109/CVPR.2011.5995496|first1 = Q. V.|last1 = Le|first2 = W. Y.|last2 = Zou|first3 = S. Y.|last3 = Yeung|first4 = A. Y.|last4 = Ng|citeseerx = 10.1.1.294.5948|s2cid = 6006618}}</ref>

=== Natural language processing ===
CNNs have also been explored for [[natural language processing]]. CNN models are effective for various NLP problems and achieved excellent results in [[semantic parsing]],<ref>{{cite arXiv|title = A Deep Architecture for Semantic Parsing|eprint= 1404.7296|date = 2014-04-29|first1 = Edward|last1 = Grefenstette|first2 = Phil|last2 = Blunsom|first3 = Nando|last3 = de Freitas|first4 = Karl Moritz|last4 = Hermann|class= cs.CL}}</ref> search query retrieval,<ref>{{Cite journal|title = Learning Semantic Representations Using Convolutional Neural Networks for Web Search – Microsoft Research|url = http://research.microsoft.com/apps/pubs/default.aspx?id=214617|journal = Microsoft Research|access-date = 2015-12-17|date = April 2014|last1 = Mesnil|first1 = Gregoire|last2 = Deng|first2 = Li|last3 = Gao|first3 = Jianfeng|last4 = He|first4 = Xiaodong|last5 = Shen|first5 = Yelong}}</ref> sentence modeling,<ref>{{cite arXiv|title = A Convolutional Neural Network for Modelling Sentences|eprint= 1404.2188|date = 2014-04-08|first1 = Nal|last1 = Kalchbrenner|first2 = Edward|last2 = Grefenstette|first3 = Phil|last3 = Blunsom|class= cs.CL}}</ref> classification,<ref>{{cite arXiv|title = Convolutional Neural Networks for Sentence Classification|eprint= 1408.5882|date = 2014-08-25|first = Yoon|last = Kim|class= cs.CL}}</ref> prediction<ref>Collobert, Ronan, and Jason Weston. "[https://thetalkingmachines.com/sites/default/files/2018-12/unified_nlp.pdf A unified architecture for natural language processing: Deep neural networks with multitask learning]."Proceedings of the 25th international conference on Machine learning. ACM, 2008.</ref> and other traditional NLP tasks.<ref>{{cite arXiv|title = Natural Language Processing (almost) from Scratch|eprint= 1103.0398|date = 2011-03-02|first1 = Ronan|last1 = Collobert|first2 = Jason|last2 = Weston|first3 = Leon|last3 = Bottou|first4 = Michael|last4 = Karlen|first5 = Koray|last5 = Kavukcuoglu|first6 = Pavel|last6 = Kuksa|class= cs.LG}}</ref>

=== Anomaly Detection ===
A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.<ref>{{Cite journal|title=Time-Series Anomaly Detection Service at Microsoft {{!}} Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining|language=EN|arxiv = 1906.03821|last1=Ren|first1=Hansheng|last2=Xu|first2=Bixiong|last3=Wang|first3=Yujing|last4=Yi|first4=Chao|last5=Huang|first5=Congrui|last6=Kou|first6=Xiaoyu|last7=Xing|first7=Tony|last8=Yang|first8=Mao|last9=Tong|first9=Jie|last10=Zhang|first10=Qi|year=2019|doi=10.1145/3292500.3330680|s2cid=182952311}}</ref>

=== Drug discovery ===
CNNs have been used in [[drug discovery]]. Predicting the interaction between molecules and biological [[proteins]] can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based [[Drug design|rational drug design]].<ref>{{cite arXiv|title = AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery|eprint= 1510.02855|date = 2015-10-09|first1 = Izhar|last1 = Wallach|first2 = Michael|last2 = Dzamba|first3 = Abraham|last3 = Heifets|class= cs.LG}}</ref> The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,<ref>{{cite arXiv|title = Understanding Neural Networks Through Deep Visualization|eprint= 1506.06579|date = 2015-06-22|first1 = Jason|last1 = Yosinski|first2 = Jeff|last2 = Clune|first3 = Anh|last3 = Nguyen|first4 = Thomas|last4 = Fuchs|first5 = Hod|last5 = Lipson|class= cs.CV}}</ref> AtomNet discovers chemical features, such as [[aromaticity]], [[orbital hybridisation|sp<sup>3</sup> carbons]] and [[hydrogen bond]]ing. Subsequently, AtomNet was used to predict novel candidate [[biomolecule]]s for multiple disease targets, most notably treatments for the [[Ebola virus]]<ref>{{Cite news|title = Toronto startup has a faster way to discover effective medicines|url = https://www.theglobeandmail.com/report-on-business/small-business/starting-out/toronto-startup-has-a-faster-way-to-discover-effective-medicines/article25660419/|website = The Globe and Mail|access-date = 2015-11-09}}</ref> and [[multiple sclerosis]].<ref>{{Cite web|title = Startup Harnesses Supercomputers to Seek Cures|url = http://ww2.kqed.org/futureofyou/2015/05/27/startup-harnesses-supercomputers-to-seek-cures/|website = KQED Future of You|access-date = 2015-11-09|language = en-us|date = 2015-05-27}}</ref>

=== Health risk assessment and biomarkers of aging discovery ===

CNNs can be naturally tailored to analyze a sufficiently large collection of [[time series]] data representing one-week-long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., the [[National Health and Nutrition Examination Survey|NHANES]] study). A simple CNN was combined with Cox-Gompertz [[proportional hazards model]] and used to produce a proof-of-concept example of digital [[biomarkers of aging]] in the form of all-causes-mortality predictor.<ref name="pmid 29581467">{{cite journal | author1=Tim Pyrkov | author2=Konstantin Slipensky | author3=Mikhail Barg | author4=Alexey Kondrashin | author5=Boris Zhurov | author6=Alexander Zenin | author7=Mikhail Pyatnitskiy | author8=Leonid Menshikov | author9=Sergei Markov | author10=Peter O. Fedichev | title=Extracting biological age from biomedical data via deep learning: too much of a good thing? | journal=Scientific Reports | volume=8 | issue=1 | year=2018 | pages=5210 | doi=10.1038/s41598-018-23534-9 | pmid= 29581467 | pmc=5980076 | bibcode=2018NatSR...8.5210P }}</ref>

=== Checkers game ===
CNNs have been used in the game of [[Draughts|checkers]]. From 1999 to 2001, [[David B. Fogel|Fogel]] and Chellapilla published papers showing how a convolutional neural network could learn to play '''checker''' using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program ([[Blondie24]]) was tested on 165 games against players and ranked in the highest 0.4%.<ref>{{cite journal | pmid = 18252639 | doi=10.1109/72.809083 | volume=10 | issue=6 | title=Evolving neural networks to play checkers without relying on expert knowledge | journal=IEEE Trans Neural Netw | pages=1382–91 | last1 = Chellapilla | first1 = K | last2 = Fogel | first2 = DB| year=1999 }}</ref><ref>{{Cite journal | doi=10.1109/4235.942536| title=Evolving an expert checkers playing program without using human expertise| journal=IEEE Transactions on Evolutionary Computation| volume=5| issue=4| pages=422–428| year=2001| last1=Chellapilla| first1=K.| last2=Fogel| first2=D.B.}}</ref> It also earned a win against the program [[Chinook (draughts player)|Chinook]] at its "expert" level of play.<ref>{{cite book |last= Fogel |first= David |date= 2001 |title= Blondie24: Playing at the Edge of AI |location= San Francisco, CA|publisher= Morgan Kaufmann|isbn= 978-1558607835|author-link= David B. Fogel }}</ref>

=== Go ===
CNNs have been used in [[computer Go]]. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform [[GNU Go]] and win some games against [[Monte Carlo tree search]] Fuego 1.1 in a fraction of the time it took Fuego to play.<ref>{{Cite arXiv|eprint=1412.3409|last1=Clark|first1=Christopher|title=Teaching Deep Convolutional Neural Networks to Play Go|last2=Storkey|first2=Amos|class=cs.AI|year=2014}}</ref> Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a [[Go ranks and ratings|6 dan]] human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program [[GNU Go]] in 97% of games, and matched the performance of the [[Monte Carlo tree search]] program Fuego simulating ten thousand playouts (about a million positions) per move.<ref>{{Cite arXiv|eprint=1412.6564|last1= Maddison|first1= Chris J.|title= Move Evaluation in Go Using Deep Convolutional Neural Networks|last2= Huang|first2= Aja|last3= Sutskever|first3= Ilya|last4= Silver|first4= David|class= cs.LG|year= 2014}}</ref>

A couple of CNNs for choosing moves to try ("policy network") and evaluating positions ("value network") driving MCTS were used by [[AlphaGo]], the first to beat the best human player at the time.<ref>{{cite web|url=https://www.deepmind.com/alpha-go.html|title=AlphaGo – Google DeepMind|access-date=30 January 2016|archive-url=https://web.archive.org/web/20160130230207/http://www.deepmind.com/alpha-go.html|archive-date=30 January 2016|url-status=dead}}</ref>

=== Time series forecasting ===
Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.<ref>{{cite arXiv | last1=Bai | first1=Shaojie | last2=Kolter | first2=J. Zico | last3=Koltun | first3=Vladlen | title=An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling | date=2018-04-19 | eprint=1803.01271 | class=cs.LG }}</ref><ref name="Tsantekidis 7–12"/> Dilated convolutions<ref>{{cite arXiv | last1=Yu | first1=Fisher | last2=Koltun | first2=Vladlen| title=Multi-Scale Context Aggregation by Dilated Convolutions | date=2016-04-30 | eprint=1511.07122 | class=cs.CV }}</ref> might enable one-dimensional convolutional neural networks to effectively learn time series dependences.<ref>{{cite arXiv | last1=Borovykh | first1=Anastasia | last2=Bohte | first2=Sander | last3=Oosterlee | first3=Cornelis W. | title=Conditional Time Series Forecasting with Convolutional Neural Networks | date=2018-09-17 | eprint=1703.04691 | class=stat.ML }}</ref> Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.<ref>{{cite arXiv | last=Mittelman | first=Roni | title=Time-series modeling with undecimated fully convolutional neural networks | date=2015-08-03 | eprint=1508.00317 | class=stat.ML }}</ref> Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.<ref>{{cite arXiv | last1=Chen | first1=Yitian | last2=Kang | first2=Yanfei | last3=Chen | first3=Yixiong | last4=Wang | first4=Zizhuo | title=Probabilistic Forecasting with Temporal Convolutional Neural Network | date=2019-06-11 | eprint=1906.04397 | class=stat.ML }}</ref> CNNs can also be applied to further tasks in time series analysis (e.g., time series classification<ref>{{cite journal | last1=Zhao | first1=Bendong | last2=Lu | first2=Huanzhang | last3=Chen | first3=Shangfeng | last4=Liu | first4=Junliang | last5=Wu | first5=Dongya | date=2017-02-01 | title=Convolutional neural networks for time series classi | journal=Journal of Systems Engineering and Electronics | volume=28 | issue=1 | pages=162–169 | doi=10.21629/JSEE.2017.01.18 }}</ref> or quantile forecasting<ref>{{cite arXiv | last=Petneházi | first=Gábor | title=QCNN: Quantile Convolutional Neural Network | date=2019-08-21 | eprint=1908.07978 | class=cs.LG }}</ref>).

=== Cultural Heritage and 3D-datasets ===
As archaeological findings like [[clay tablet]]s with [[Cuneiform|cuneiform writing]] are increasingly acquired using [[3D scanners]] first benchmark datasets are becoming available like ''HeiCuBeDa''<ref name="HeiCuBeDa_Hilprecht" /> providing almost 2.000 normalized 2D- and 3D-datasets prepared with the [[GigaMesh Software Framework]].<ref name="ICDAR19" /> So [[curvature]] based measures are used in conjunction with Geometric Neural Networks (GNNs) e.g. for period classification of those clay tablets being among the oldest documents of human history.<ref name="ICFHR20" /><ref name="ICFHR20_Presentation" />

== Fine-tuning ==
For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid [[overfitting]]. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.<ref>Durjoy Sen Maitra; Ujjwal Bhattacharya; S.K. Parui, [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7333916&tag=1 "CNN based common approach to handwritten character recognition of multiple scripts,"] in Document Analysis and Recognition (ICDAR), 2015 13th International Conference on, vol., no., pp.1021–1025, 23–26 Aug. 2015</ref>

== Human interpretable explanations ==
End-to-end training and prediction are common practice in [[computer vision]]. However, human interpretable explanations are required for [[Safety-critical system|critical systems]] such as a [[self-driving car]]s.<ref name="Interpretable ML Symposium 2017">{{cite web | title=NIPS 2017 | website=Interpretable ML Symposium | date=2017-10-20 | url=http://interpretable.ml/ | access-date=2018-09-12}}</ref> With recent advances in [[Salience (neuroscience)|visual salience]], [[Visual spatial attention|spatial]] and [[Visual temporal attention|temporal attention]], the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.<ref name="Zang Wang Liu Zhang 2018 pp. 97–108">{{cite book | last1=Zang | first1=Jinliang | last2=Wang | first2=Le | last3=Liu | first3=Ziyi | last4=Zhang | first4=Qilin | last5=Hua | first5=Gang | last6=Zheng | first6=Nanning | title=IFIP Advances in Information and Communication Technology | chapter=Attention-Based Temporal Weighted Convolutional Neural Network for Action Recognition | publisher=Springer International Publishing | location=Cham | year=2018 | isbn=978-3-319-92006-1 | issn=1868-4238 | doi=10.1007/978-3-319-92007-8_9 | pages=97–108 | arxiv=1803.07179 | s2cid=4058889 }}</ref><ref name="Wang Zang Zhang Niu p=1979">{{cite journal | last1=Wang | first1=Le | last2=Zang | first2=Jinliang | last3=Zhang | first3=Qilin | last4=Niu | first4=Zhenxing | last5=Hua | first5=Gang | last6=Zheng | first6=Nanning | title=Action Recognition by an Attention-Aware Temporal Weighted Convolutional Neural Network | journal=Sensors | volume=18 | issue=7 | date=2018-06-21 | issn=1424-8220 | doi=10.3390/s18071979 | pmid=29933555 | pmc=6069475 | page=1979 | url=https://qilin-zhang.github.io/_pages/pdfs/sensors-18-01979-Action_Recognition_by_an_Attention-Aware_Temporal_Weighted_Convolutional_Neural_Network.pdf }}</ref>

== Related architectures ==

=== Deep Q-networks ===
A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with [[Q-learning]], a form of [[reinforcement learning]]. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.<ref name="Ong Chavez Hong 2015">{{cite arXiv |last1=Ong |first1=Hao Yi |last2=Chavez |first2=Kevin |last3=Hong |first3=Augustus |title=Distributed Deep Q-Learning  |date=2015-08-18 |class=cs.LG |eprint=1508.04186v2}}</ref>

Preliminary results were presented in 2014, with an accompanying paper in February 2015.<ref name="DQN">{{cite journal|last1=Mnih|first1=Volodymyr|display-authors=etal|date=2015|title=Human-level control through deep reinforcement learning|journal=Nature|volume=518|issue=7540|pages=529–533|doi=10.1038/nature14236|pmid=25719670|bibcode=2015Natur.518..529M|s2cid=205242740}}</ref> The research described an application to [[Atari 2600]] gaming. Other deep reinforcement learning models preceded it.<ref>{{Cite journal|last1=Sun|first1=R.|last2=Sessions|first2=C.|date=June 2000|title=Self-segmentation of sequences: automatic formation of hierarchies of sequential behaviors|journal=IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics|volume=30|issue=3|pages=403–418|doi=10.1109/3477.846230|pmid=18252373|issn=1083-4419|citeseerx=10.1.1.11.226}}</ref>

=== Deep belief networks ===
{{Main|Deep belief network}}
Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like [[deep belief network]]s. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR<ref name="CDBN-CIFAR">{{cite web|url=http://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf|title=Convolutional Deep Belief Networks on CIFAR-10}}</ref> have been obtained using CDBNs.<ref name="CDBN">{{cite book|last1=Lee|first1=Honglak|last2=Grosse|first2=Roger|last3=Ranganath|first3=Rajesh|last4=Ng|first4=Andrew Y.|date=1 January 2009|title=Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations|journal=Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09|publisher=ACM|pages=609–616|doi=10.1145/1553374.1553453|isbn=9781605585161|citeseerx=10.1.1.149.6800|s2cid=12008458}}</ref>

== Notable libraries ==
* [[Caffe (software)|Caffe]]: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in [[C++]], and has [[Python (programming language)|Python]] and [[MATLAB]] wrappers.
* [[Deeplearning4j]]: Deep learning in [[Java (programming language)|Java]] and [[Scala (programming language)|Scala]] on multi-GPU-enabled [[Apache Spark|Spark]]. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.
*[[Dlib]]: A toolkit for making real world machine learning and data analysis applications in C++.
* [[Microsoft Cognitive Toolkit]]: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in [[C Sharp (programming language)|C#]] and Java.
*[[TensorFlow]]: [[Apache License#Version 2.0|Apache 2.0]]-licensed Theano-like library with support for CPU, GPU, Google's proprietary [[tensor processing unit]] (TPU),<ref>{{cite news|url=https://www.wired.com/2016/05/google-tpu-custom-chips/|title=Google Built Its Very Own Chips to Power Its AI Bots|author=Cade Metz|date=May 18, 2016|newspaper=Wired}}</ref> and mobile devices.
*[[Theano (software)|Theano]]: The reference deep-learning library for Python with an API largely compatible with the popular [[NumPy]] library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to [[CUDA]] code for a fast, [[Compute kernel|on-the-GPU]] implementation.
*[[Torch (machine learning)|Torch]]: A [[scientific computing]] framework with wide support for machine learning algorithms, written in [[C (programming language)|C]] and [[Lua (programming language)|Lua]]. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.

== Notable APIs ==
*[[Keras]]: A high level API written in [[Python (programming language)|Python]] for [[TensorFlow]] and [[Theano (software)|Theano]] convolutional neural networks.<ref>{{cite web|title=Keras Documentation|url=https://keras.io/#you-have-just-found-keras|website=keras.io|language=en}}</ref>

== See also ==
* [[Attention (machine learning)]]
* [[Convolution]]
* [[Deep learning]]
* [[Natural-language processing]]
* [[Neocognitron]]
* [[Scale-invariant feature transform]]
* [[Time delay neural network]]
* [[Vision processing unit]]

== Notes ==
{{Reflist|group=nb}}

== References ==
{{reflist|30em|refs=
<ref name="ICDAR19">
{{citation|surname1=Hubert Mara and Bartosz Bogacz|periodical=Proceedings of the 15th International Conference on Document Analysis and Recognition (ICDAR)|title=Breaking the Code on Broken Tablets: The Learning Challenge for Annotated Cuneiform Script in Normalized 2D and 3D Datasets|location=Sydney, Australien|date=2019|pages=148–153|language=de|doi=10.1109/ICDAR.2019.00032
|isbn=978-1-7281-3014-9|s2cid=211026941}}
</ref>
<ref name="HeiCuBeDa_Hilprecht">
{{citation|surname1=Hubert Mara|title=HeiCuBeDa Hilprecht – Heidelberg Cuneiform Benchmark Dataset for the Hilprecht Collection|publisher=heiDATA – institutional repository for research data of Heidelberg University|date=2019-06-07|language=de|doi=10.11588/data/IE8CCN
}}
</ref><ref name="ICFHR20">
{{citation
   |last1=Bogacz|first1=Bartosz
   |last2=Mara|first2=Hubert
   |periodical=Proceedings of the 17th International Conference on Frontiers of Handwriting Recognition (ICFHR)
   |title=Period Classification of 3D Cuneiform Tablets with Geometric Neural Networks
   |location=Dortmund, Germany
   |date=2020
}}</ref>
<ref name="ICFHR20_Presentation">{{YouTube
   |id=-iFntE51HRw
   |title=Presentation of the ICFHR paper on Period Classification of 3D Cuneiform Tablets with Geometric Neural Networks
}}</ref>
}}

== External links ==
* [https://cs231n.github.io/ CS231n: Convolutional Neural Networks for Visual Recognition] — [[Andrej Karpathy]]'s [[Stanford University|Stanford]] computer science course on CNNs in computer vision
* [https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/ An Intuitive Explanation of Convolutional Neural Networks] — A beginner level introduction to what Convolutional Neural Networks are and how they work
* [https://www.completegate.com/2017022864/blog/deep-machine-learning-images-lenet-alexnet-cnn/all-pages Convolutional Neural Networks for Image Classification] — Literature Survey

[[Category:Artificial neural networks]]
[[Category:Computer vision]]
[[Category:Computational neuroscience]]
[[Category:Machine learning]]