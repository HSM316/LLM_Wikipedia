{{Short description|Scalable machine learning system for gradient boosting}}
{{Infobox software
| name                   = XGBoost
| logo                   = XGBoost_logo.png
| screenshot             =
| caption                =
| developer              = The XGBoost Contributors
| released               = {{Start date and age|2014|03|27}}
| latest release version = 0.90
| latest release date    = {{release date|2019|05|30}}
| operating system       = [[Linux]], [[macOS]], [[Windows]]
| programming language   = [[C++]]
| genre                  = [[Machine learning]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://xgboost.ai/}}
}}

'''XGBoost'''<ref name="source-code">{{cite web|url=https://github.com/dmlc/xgboost |title=GitHub project webpage}}</ref> is an [[Open-source software|open-source]] [[Library (computing)|software library]] which provides a [[gradient boosting]] framework for [[C++]], [[Java (programming language)|Java]],
[[Python (programming language)|Python]],<ref name="xgboost-python">{{Cite web|url=https://pypi.python.org/pypi/xgboost/|title=Python Package Index PYPI: xgboost|access-date=2016-08-01}}</ref>
[[R (programming language)|R]],<ref name="xgboost-cran">{{Cite web|url=https://cran.r-project.org/web/packages/xgboost/index.html|title=CRAN package xgboost|access-date=2016-08-01}}</ref> and
[[Julia (programming language)|Julia]].<ref name="xgboost-julia">{{Cite web|url=http://pkg.julialang.org/?pkg=XGBoost#XGBoost|title=Julia package listing xgboost|access-date=2016-08-01}}</ref>
It works on [[Linux]],
[[Windows]],<ref name="xgboost-windows">{{Cite web|title=Installing XGBoost for Anaconda in Windows|url=https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en|accessdate=2016-08-01}}</ref> and
[[macOS]].<ref name="xgboost-macos">{{Cite web|title=Installing XGBoost on Mac OSX|url=https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_on_Mac_OSX?lang=en|accessdate = 2016-08-01}}</ref> From the project description, it aims to provide a "Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library". It runs on a single machine, as well as the distributed processing frameworks [[Apache Hadoop]], [[Apache Spark]], and [[Apache Flink]].
It has gained much popularity and attention recently as the algorithm of choice for many winning teams of machine learning competitions.<ref name="xgboost-competition-winners">{{Cite web|title=XGBoost - ML winning solutions (incomplete list)|url=https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions|access-date=2016-08-01}}</ref>

==History==
XGBoost initially started as a research project by Tianqi Chen<ref name="history">{{Cite web|url=http://homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html|title=Story and Lessons behind the evolution of XGBoost|access-date=2016-08-01}}</ref> as part of the Distributed (Deep) Machine Learning Community (DMLC) group. Initially, it began as a terminal application which could be configured using a [[libsvm]] configuration file. It became well known in the ML competition circles after its use in the winning solution of the Higgs Machine Learning Challenge. Soon after, the Python and R packages were built, and XGBoost now has package implementations for Julia, [[Scala (programming language)|Scala]], Java, and other languages. This brought the library to more developers and contributed to its popularity among the [[Kaggle]] community, where it has been used for a large number of competitions.<ref name="xgboost-competition-winners" />

It was soon integrated with a number of other packages making it easier to use in their respective communities. It has now been integrated with [[scikit-learn]] for [[Python (programming language)|Python]] users and with the [https://cran.rstudio.com/web/packages/caret/vignettes/caret.html caret] package for [[R (programming language)|R]] users. It can also be integrated into Data Flow frameworks like [[Apache Spark]], [[Apache Hadoop]], and [[Apache Flink]] using the abstracted Rabit<ref name="rabit">{{Cite web|url=https://github.com/dmlc/rabit|title=Rabit - Reliable Allreduce and Broadcast Interface|access-date=2016-08-01}}</ref> and XGBoost4J.<ref name="xgboost4j">{{Cite web|url=https://xgboost.readthedocs.io/en/latest/jvm/index.html|title=XGBoost4J|access-date=2016-08-01}}</ref> XGBoost is also available on [[OpenCL]] for [[Field-programmable_gate_array|FPGAs]]<ref name="xgboost FPGA">{{Cite web|url=https://github.com/InAccel/xgboost|title=XGBoost on FPGAs|access-date=2019-08-01}}</ref>. An efficient, scalable implementation of XGBoost has been published by Tianqi Chen and Carlos Guestrin.<ref name="paper">{{cite conference
 | last1 = Chen | first1 = Tianqi
 | last2 = Guestrin | first2 = Carlos
 | editor1-last = Krishnapuram | editor1-first = Balaji
 | editor2-last = Shah | editor2-first = Mohak
 | editor3-last = Smola | editor3-first = Alexander J.
 | editor4-last = Aggarwal | editor4-first = Charu C.
 | editor5-last = Shen | editor5-first = Dou
 | editor6-last = Rastogi | editor6-first = Rajeev
 | arxiv = 1603.02754
 | contribution = XGBoost: A Scalable Tree Boosting System
 | doi = 10.1145/2939672.2939785
 | pages = 785â€“794
 | publisher = ACM
 | title = Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016
 | year = 2016}}</ref>

==Awards==
* [[John Chambers (statistician)|John Chambers]] Award <small>(2016)</small><ref name="john-chambers">{{Cite web|url=http://stat-computing.org/awards/jmc/winners.html|title=John Chambers Award Previous Winners|access-date=2016-08-01}}</ref>
* High Energy Physics meets Machine Learning award (HEP meets ML) <small>(2016)</small><ref name="hep-meets-ml">{{Cite web|url=https://higgsml.lal.in2p3.fr/prizes-and-award/award/|title=HEP meets ML Award|access-date=2016-08-01}}</ref>

==References==
{{reflist}}

[[Category:Data mining and machine learning software]]
[[Category:Free data analysis software]]
[[Category:Software using the Apache license]]
[[Category:Big data products]]
[[Category:Free software programmed in C++]]
[[Category:2014 software]]


{{artificial-intelligence-stub}}