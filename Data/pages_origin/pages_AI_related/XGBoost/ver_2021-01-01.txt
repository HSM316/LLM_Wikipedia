{{short description|Scalable implementation of the gradient boosted tree machine learning algorithm}}
{{Infobox software
| name                   = XGBoost
| logo                   = XGBoost_logo.png
| screenshot             =
| caption                =
| developer              = The XGBoost Contributors
| released               = {{Start date and age|2014|03|27}}
| latest release version = 1.2.1<ref>{{Cite web|title=Release 1.2.1· dmlc/xgboost|url=https://github.com/dmlc/xgboost/releases/tag/v1.2.1|access-date=2020-08-08|website=GitHub|language=en}}</ref>
| latest release date    = {{Start date and age|2020|10|13}}
| operating system       = [[Linux]], [[macOS]], [[Windows]]
| programming language   = [[C++]]
| genre                  = [[Machine learning]]
| license                = [[Apache License 2.0]]
| website                = {{URL|https://xgboost.ai/}}
}}
'''XGBoost'''<ref name="source-code">{{cite web|url=https://github.com/dmlc/xgboost |title=GitHub project webpage}}</ref> is an [[Open-source software|open-source]] [[Library (computing)|software library]] which provides a [[gradient boosting]] framework for [[C++]], [[Java (programming language)|Java]],
[[Python (programming language)|Python]],<ref name="xgboost-python">{{Cite web|url=https://pypi.python.org/pypi/xgboost/|title=Python Package Index PYPI: xgboost|access-date=2016-08-01}}</ref>
[[R (programming language)|R]],<ref name="xgboost-cran">{{Cite web|url=https://cran.r-project.org/web/packages/xgboost/index.html|title=CRAN package xgboost|access-date=2016-08-01}}</ref>
[[Julia (programming language)|Julia]],<ref name="xgboost-julia">{{Cite web|url=http://pkg.julialang.org/?pkg=XGBoost#XGBoost|title=Julia package listing xgboost|access-date=2016-08-01}}</ref>
[[Perl (programming language)|Perl]],<ref name="xgboost-perl">{{Cite web|url=https://metacpan.org/pod/AI::XGBoost|title=CPAN module AI::XGBoost|access-date=2020-02-09}}</ref> and [[Scala (programming language)|Scala]].
It works on [[Linux]],
[[Windows]],<ref name="xgboost-windows">{{Cite web|title=Installing XGBoost for Anaconda in Windows|url=https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en|accessdate=2016-08-01}}</ref> and
[[macOS]].<ref name="xgboost-macos">{{Cite web|title=Installing XGBoost on Mac OSX|url=https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_on_Mac_OSX?lang=en|accessdate = 2016-08-01}}</ref> From the project description, it aims to provide a "Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library". It runs on a single machine, as well as the distributed processing frameworks [[Apache Hadoop]], [[Apache Spark]], and [[Apache Flink]].
It has gained much popularity and attention recently as the algorithm of choice for many winning teams of machine learning competitions.<ref name="xgboost-competition-winners">{{Cite web|title=XGBoost - ML winning solutions (incomplete list)|url=https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions|access-date=2016-08-01}}</ref>

==History==
XGBoost initially started as a research project by Tianqi Chen<ref name="history">{{Cite web|url=http://homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html|title=Story and Lessons behind the evolution of XGBoost|access-date=2016-08-01}}</ref> as part of the Distributed (Deep) Machine Learning Community (DMLC) group. Initially, it began as a terminal application which could be configured using a [[libsvm]] configuration file. It became well known in the ML competition circles after its use in the winning solution of the Higgs Machine Learning Challenge. Soon after, the Python and R packages were built, and XGBoost now has package implementations for Java, [[Scala (programming language)|Scala]], Julia, [[Perl]], and other languages. This brought the library to more developers and contributed to its popularity among the [[Kaggle]] community, where it has been used for a large number of competitions.<ref name="xgboost-competition-winners" />

It was soon integrated with a number of other packages making it easier to use in their respective communities. It has now been integrated with [[scikit-learn]] for [[Python (programming language)|Python]] users and with the [https://cran.rstudio.com/web/packages/caret/vignettes/caret.html caret] package for [[R (programming language)|R]] users. It can also be integrated into Data Flow frameworks like [[Apache Spark]], [[Apache Hadoop]], and [[Apache Flink]] using the abstracted Rabit<ref name="rabit">{{Cite web|url=https://github.com/dmlc/rabit|title=Rabit - Reliable Allreduce and Broadcast Interface|access-date=2016-08-01}}</ref> and XGBoost4J.<ref name="xgboost4j">{{Cite web|url=https://xgboost.readthedocs.io/en/latest/jvm/index.html|title=XGBoost4J|access-date=2016-08-01}}</ref> XGBoost is also available on [[OpenCL]] for [[field-programmable_gate_array|FPGAs]].<ref name="xgboost FPGA">{{Cite web|url=https://github.com/InAccel/xgboost|title=XGBoost on FPGAs|access-date=2019-08-01}}</ref> An efficient, scalable implementation of XGBoost has been published by Tianqi Chen and Carlos Guestrin.<ref name="paper">{{cite conference
 | last1 = Chen | first1 = Tianqi
 | last2 = Guestrin | first2 = Carlos
 | editor1-last = Krishnapuram | editor1-first = Balaji
 | editor2-last = Shah | editor2-first = Mohak
 | editor3-last = Smola | editor3-first = Alexander J.
 | editor4-last = Aggarwal | editor4-first = Charu C.
 | editor5-last = Shen | editor5-first = Dou
 | editor6-last = Rastogi | editor6-first = Rajeev
 | arxiv = 1603.02754
 | contribution = XGBoost: A Scalable Tree Boosting System
 | doi = 10.1145/2939672.2939785
 | pages = 785–794
 | publisher = ACM
 | title = Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016
 | year = 2016}}</ref>

==Features==
Salient features of XGBoost which make it different from other gradient boosting algorithms include:<ref>{{Cite web|url=https://medium.com/hackernoon/gradient-boosting-and-xgboost-90862daa6c77|title=Gradient Boosting and XGBoost|last=Gandhi|first=Rohith|date=2019-05-24|website=Medium|language=en|access-date=2020-01-04}}</ref><ref>{{Cite web|url=https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d|title=Boosting algorithm: XGBoost|first=|date=2017-05-14|website=Towards Data Science|language=en|url-status=live|archive-url=|archive-date=|access-date=2020-01-04}}</ref><ref>{{Cite web|url=https://syncedreview.com/2017/10/22/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition/|title=Tree Boosting With XGBoost – Why Does XGBoost Win "Every" Machine Learning Competition?|date=2017-10-22|website=Synced|language=en-US|url-status=live|archive-url=|archive-date=|access-date=2020-01-04}}</ref>

* Clever penalization of trees
* A proportional shrinking of leaf nodes
* [[Newton's method in optimization|Newton Boosting]]
* Extra [[randomization]] parameter
*Implementation on single, [[Distributed computing|distributed]] systems and [[out-of-core]] computation

==Awards==
* [[John Chambers (statistician)|John Chambers]] Award <small>(2016)</small><ref name="john-chambers">{{Cite web|url=http://stat-computing.org/awards/jmc/winners.html|title=John Chambers Award Previous Winners|access-date=2016-08-01}}</ref>
* High Energy Physics meets Machine Learning award (HEP meets ML) <small>(2016)</small><ref name="hep-meets-ml">{{Cite web|url=https://higgsml.lal.in2p3.fr/prizes-and-award/award/|title=HEP meets ML Award|access-date=2016-08-01}}</ref>

==See also==
* [[LightGBM]]

==References==
{{Reflist}}

[[Category:Data mining and machine learning software]]
[[Category:Free data analysis software]]
[[Category:Software using the Apache license]]
[[Category:Big data products]]
[[Category:Free software programmed in C++]]
[[Category:2014 software]]


{{artificial-intelligence-stub}}