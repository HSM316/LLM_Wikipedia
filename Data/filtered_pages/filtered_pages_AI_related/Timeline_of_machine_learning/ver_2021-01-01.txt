This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.

Overview
 Decade  Summary <1950s Statistical methods are discovered and refined. 1950s  Pioneering machine learning research is conducted using simple algorithms. 1960s  Bayesian methods are introduced for probabilistic inference in machine learning.Solomonoff, Ray J. "A formal theory of inductive inference. Part II." Information and control 7.2 (1964): 224–254. 1970s  'AI Winter' caused by pessimism about machine learning effectiveness. 1980s  Rediscovery of backpropagation causes a resurgence in machine learning research. 1990s  Work on Machine learning shifts from a knowledge-driven approach to a data-driven approach. Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions or "learn" from the results. Support vector machines (SVMs) and recurrent neural networks (RNNs) become popular. The fields of  computational complexity via neural networks and super-Turing computation started.  2000s  Support Vector Clustering   and other Kernel methods  and unsupervised machine learning methods become widespread.   2010s  Deep learning becomes feasible, which leads to machine learning becoming integral to many widely used software services and applications.

Timeline
A simple neural network with two input units and one output unit

 Year  Event type  Caption  Event 1763  Discovery  The Underpinnings of Bayes' Theorem  Thomas Bayes's work An Essay towards solving a Problem in the Doctrine of Chances is published two years after his death, having been amended and edited by a friend of Bayes, Richard Price. The essay presents work which underpins Bayes theorem. 1805  Discovery  Least Square  Adrien-Marie Legendre describes the "méthode des moindres carrés", known in English as the least squares method. The least squares method is used widely in data fitting. 1812   Bayes' Theorem  Pierre-Simon Laplace publishes Théorie Analytique des Probabilités, in which he expands upon the work of Bayes and defines what is now known as Bayes' Theorem. 1913  Discovery  Markov Chains  Andrey Markov first describes techniques he used to analyse a poem. The techniques later become known as Markov chains. 1950   Turing's Learning Machine  Alan Turing proposes a 'learning machine' that could learn and become artificially intelligent. Turing's specific proposal foreshadows genetic algorithms.  1951   First Neural Network Machine  Marvin Minsky and Dean Edmonds build the first neural network machine, able to learn, the SNARC. and  1952   Machines Playing Checkers  Arthur Samuel joins IBM's Poughkeepsie Laboratory and begins working on some of the very first machine learning programs, first creating programs that play checkers. 1957  Discovery  Perceptron  Frank Rosenblatt invents the perceptron while working at the Cornell Aeronautical Laboratory. The invention of the perceptron generated a great deal of excitement and was widely covered in the media. 1963  Achievement  Machines Playing Tic-Tac-Toe  Donald Michie creates a 'machine' consisting of 304 match boxes and beads, which uses reinforcement learning to play Tic-tac-toe (also known as noughts and crosses). 1967   Nearest Neighbor  The nearest neighbor algorithm was created, which is the start of basic pattern recognition. The algorithm was used to map routes. 1969   Limitations of Neural Networks  Marvin Minsky and Seymour Papert publish their book Perceptrons, describing some of the limitations of perceptrons and neural networks. The interpretation that the book shows that neural networks are fundamentally limited is seen as a hindrance for research into neural networks. 1970   Automatic Differentiation (Backpropagation)  Seppo Linnainmaa publishes the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions.Seppo Linnainmaa (1970). "The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors." Master's Thesis (in Finnish), Univ. Helsinki, 6–7. This corresponds to the modern version of backpropagation, but is not yet named as such.Griewank, Andreas and Walther, A. Principles and Techniques of Algorithmic Differentiation, Second Edition. SIAM, 2008. 1979   Stanford Cart  Students at Stanford University develop a cart that can navigate and avoid obstacles in a room. 1979  Discovery  Neocognitron  Kunihiko Fukushima first publishes his work on the neocognitron, a type of artificial neural network (ANN). Neocognition later inspires convolutional neural networks (CNNs). 1981   Explanation Based Learning  Gerald Dejong introduces Explanation Based Learning, where a computer algorithm analyses data and creates a general rule it can follow and discard unimportant data. 1982  Discovery  Recurrent Neural Network  John Hopfield popularizes Hopfield networks, a type of recurrent neural network that can serve as content-addressable memory systems. 1985   NetTalk  A program that learns to pronounce words the same way a baby does, is developed by Terry Sejnowski. 1986  Application  Backpropagation  Seppo Linnainmaa's reverse mode of automatic differentiation (first applied to neural networks by Paul Werbos) is used in experiments by David Rumelhart, Geoff Hinton and Ronald J. Williams to learn internal representations. 1989  Discovery  Reinforcement Learning  Christopher Watkins develops Q-learning, which greatly improves the practicality and feasibility of reinforcement learning. 1989  Commercialization  Commercialization of Machine Learning on Personal Computers  Axcelis, Inc. releases Evolver, the first software package to commercialize the use of genetic algorithms on personal computers. 1992  Achievement  Machines Playing Backgammon  Gerald Tesauro develops TD-Gammon, a computer backgammon program that uses an artificial neural network trained using temporal-difference learning (hence the 'TD' in the name). TD-Gammon is able to rival, but not consistently surpass, the abilities of top human backgammon players. 1995  Discovery  Random Forest Algorithm  Tin Kam Ho publishes a paper describing random decision forests. 1995  Discovery  Support Vector Machines  Corinna Cortes and Vladimir Vapnik publish their work on support vector machines. 1997  Achievement  IBM Deep Blue Beats Kasparov  IBM's Deep Blue beats the world champion at chess. 1997  Discovery  LSTM  Sepp Hochreiter and Jürgen Schmidhuber invent long short-term memory (LSTM) recurrent neural networks, greatly improving the efficiency and practicality of recurrent neural networks. 1998   MNIST database  A team led by Yann LeCun releases the MNIST database, a dataset comprising a mix of handwritten digits from American Census Bureau employees and American high school students. The MNIST database has since become a benchmark for evaluating handwriting recognition. 2002   Torch Machine Learning Library  Torch, a software library for machine learning, is first released. 2006   The Netflix Prize  The Netflix Prize competition is launched by Netflix. The aim of the competition was to use machine learning to beat Netflix's own recommendation software's accuracy in predicting a user's rating for a film given their ratings for previous films by at least 10%. The prize was won in 2009.2009AchievementImageNetImageNet is created. ImageNet is a large visual database envisioned by Fei-Fei Li from Stanford University, who realized that the best machine learning algorithms wouldn't work well if the data didn't reflect the real world. For many, ImageNet was the catalyst for the AI boom of the 21st century.  2010   Kaggle Competition  Kaggle, a website that serves as a platform for machine learning competitions, is launched. 2010   Wall Street Journal Profiles Machine Learning Investing  The WSJ Profiles new wave of investing and focuses on RebellionResearch.com which would be the subject of author Scott Patterson's Novel, Dark Pools. 2011  Achievement  Beating Humans in Jeopardy  Using a combination of machine learning, natural language processing and information retrieval techniques, IBM's Watson beats two human champions in a Jeopardy! competition. 2012  Achievement  Recognizing Cats on YouTube  The Google Brain team, led by Andrew Ng and Jeff Dean, create a neural network that learns to recognize cats by watching unlabeled images taken from frames of YouTube videos. 2014   Leap in Face Recognition  Facebook researchers publish their work on DeepFace, a system that uses neural networks that identifies faces with 97.35% accuracy. The results are an improvement of more than 27% over previous systems and rivals human performance. 2014   Sibyl  Researchers from Google detail their work on Sibyl, a proprietary platform for massively parallel machine learning used internally by Google to make predictions about user behavior and provide recommendations. 2016  Achievement  Beating Humans in Go Google's AlphaGo program becomes the first Computer Go program to beat an unhandicapped professional human player using a combination of machine learning and tree search techniques. Later improved as AlphaGo Zero and then in 2017 generalized to Chess and more two-player games with AlphaZero. 