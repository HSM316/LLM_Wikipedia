In the field of artificial intelligence (AI), AI alignment research aims to steer AI systems towards their designers’ intended goals and interests. An aligned AI system advances the intended objective; a misaligned AI system is competent at advancing some objective, but not the intended one.

AI systems can be challenging to align and misaligned systems can malfunction or cause harm. It can be difficult for AI designers to specify the full range of desired and undesired behaviors. Therefore, they use easy-to-specify proxy goals that omit some desired constraints. However, AI systems exploit the resulting loopholes. As a result, they accomplish their proxy goals efficiently but in unintended, sometimes harmful ways (reward hacking). AI systems can also develop unwanted instrumental behaviors such as seeking power, as this helps them achieve their given goals. Furthermore, they can develop emergent goals that may be hard to detect before the system is deployed, facing new situations and data distributions. These problems affect existing commercial systems such as robots, language models, autonomous vehicles, and social media recommendation engines. However, more powerful future systems may be more severely affected since these problems partially result from high capability.

The AI research community and the United Nations have called for technical research and policy solutions to ensure that AI systems are aligned with human values.

AI alignment is a subfield of AI safety, the study of building safe AI systems. Other subfields of AI safety include robustness, monitoring, and capability control. Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, as well as preventing emergent AI behaviors like power-seeking. Alignment research has connections to interpretability research, robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and the social sciences, among others.

The alignment problem
right|An AI system that was intended to complete a boat race instead learned that it could collect more points by indefinitely looping and crashing into targets—an example of specification gaming.
In 1960, AI pioneer Norbert Wiener articulated the AI alignment problem as follows: “If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively … we had better be quite sure that the purpose put into the machine is the purpose which we really desire.” More recently, AI alignment has emerged as an open problem for modern AI systems and a research field within AI.

 Specification gaming and complexity of value 
To specify the purpose of an AI system, AI designers typically provide an objective function, examples, or feedback to the system. However, AI designers often fail to completely specify all important values and constraints. As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming, reward hacking, or Goodhart’s law.right|This AI system was trained using human feedback to grab a ball, but instead learned that it could give the false impression of having grabbed the ball by placing the hand between the ball and the camera. Research on AI alignment partly aims to avert solutions that are false but convincing. 

Specification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding it for hitting targets along the track; instead it learned to loop and crash into the same targets indefinitely (see video). Chatbots often produce falsehoods because they are based on language models trained to imitate diverse but fallible internet text. When they are retrained to produce text that humans rate as true or helpful, they can fabricate fake explanations that humans find convincing.  Similarly, a simulated robot was trained to grab a ball by rewarding it for getting positive feedback from humans; however, it learned to place its hand between the ball and camera, making it falsely appear successful (see video). Alignment researchers aim to help humans detect specification gaming, and steer AI systems towards carefully specified objectives that are safe and useful to pursue. 

Berkeley computer scientist Stuart Russell has noted that omitting an implicit constraint can result in harm: “A system [...] will often set [...] unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.”alt=Midas Gold|In an ancient myth, King Midas wished that “everything” he touched would turn to gold, but failed to specify exceptions for his food and his daughter. By analogy, when AI practitioners specify a goal, it is difficult for them to foresee and rule out every possible side-effect the AI should avoid.When misaligned AI is deployed, the side-effects can be consequential. Social media platforms have been known to optimize clickthrough rates as a proxy for optimizing user enjoyment, but this addicted some users, decreasing their well-being. Stanford researchers comment that such recommender algorithms are misaligned with their users because they “optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being”.

To avoid side effects, it is sometimes suggested that AI designers could simply list forbidden actions or formalize ethical rules such as Asimov’s Three Laws of Robotics. However, Russell and Norvig have argued that this approach ignores the complexity of human values: “It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.”

Additionally, when an AI system understands human intentions fully, it may still disregard them. This is because it acts according to the objective function, examples, or feedback its designers actually provide, not the ones they intended to provide.

Systemic risks
Commercial and governmental organizations may have incentives to take shortcuts on safety and deploy insufficiently aligned AI systems. An example are the aforementioned social media recommender systems, which have been profitable despite creating unwanted addiction and polarization on a global scale. In addition, competitive pressure can create a race to the bottom on safety standards, as in the case of Elaine Herzberg, a pedestrian who was killed by a self-driving car after engineers disabled the emergency braking system because it was over-sensitive and slowing down development.

 Risks from advanced misaligned AI 
Some researchers are particularly interested in the alignment of increasingly advanced AI systems. This is motivated by the high rate of progress in AI, the large efforts from industry and governments to develop advanced AI systems, and the greater difficulty of aligning them.

As of 2020, OpenAI, DeepMind, and 70 other public projects had the stated aim of developing artificial general intelligence (AGI), a hypothesized system that matches or outperforms humans in a broad range of cognitive tasks. Indeed, researchers who scale modern neural networks observe that increasingly general and unexpected capabilities emerge. Such models have learned to operate a computer, write their own programs, and perform a wide range of other tasks from a single model. Surveys find that some AI researchers expect AGI to be created soon, some believe it is very far off, and many consider both possibilities.

 Power-seeking 
Current systems still lack capabilities such as long-term planning and strategic awareness that are thought to pose the most catastrophic risks. Future systems (not necessarily AGIs) that have these capabilities may seek to protect and grow their influence over their environment. This tendency is known as power-seeking or convergent instrumental goals. Power-seeking is not explicitly programmed but emerges since power is instrumental for achieving a wide range of goals. For example, AI agents may acquire financial resources and computation, or may evade being turned off, including by running additional copies of the system on other computers. Power-seeking has been observed in various reinforcement learning agents. Later research has mathematically shown that optimal reinforcement learning algorithms seek power in a wide range of environments. As a result, it is often argued that the alignment problem must be solved early, before advanced AI that exhibits emergent power-seeking is created.

Existential risk
According to some scientists, creating misaligned AI that broadly outperforms humans would challenge the position of humanity as Earth’s dominant species; accordingly it would lead to the disempowerment or possible extinction of humans. Notable computer scientists who have pointed out risks from highly advanced misaligned AI include Alan Turing, Ilya Sutskever, Yoshua Bengio, Judea Pearl, Murray Shanahan, Norbert Wiener, Marvin Minsky, Francesca Rossi, Scott Aaronson, Bart Selman, David McAllester, Jürgen Schmidhuber, Markus Hutter, Shane Legg, Eric Horvitz, and Stuart Russell. Skeptical researchers such as François Chollet, Gary Marcus, Yann LeCun, and Oren Etzioni have argued that AGI is far off, or would not seek power (successfully).

Alignment may be especially difficult for the most capable AI systems since several risks increase with the system’s capability: the system’s ability to find loopholes in the assigned objective, cause side-effects, protect and grow its power, grow its intelligence, and mislead its designers; the system’s autonomy; and the difficulty of interpreting and supervising the AI system.

 Research problems and approaches 