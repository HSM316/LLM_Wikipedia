Feature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data.  The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process.

 Process  
The feature engineering process is:

Brainstorming or testing features;
 Deciding what features to create;
 Creating features;
 Testing the impact of the identified features on the task;
 Improving your features if needed;
 Repeat.

 Typical engineered features 
The following list provides some typical ways to engineer useful features
 Numerical transformations (like taking fractions or scaling)
 Category encoder like one-hot or target encoder (for categorical data)
 Clustering 
 Group aggregated values
 Principal component analysis (for numerical data)

 Relevance 
Features vary in significance. Even relatively insignificant features may contribute to a model. Feature selection can reduce the number of features to prevent a model from becoming too specific to the training data set (overfitting).

 Explosion 
Feature explosion occurs when the number of identified features grows inappropriately. Common causes include:

 Feature templates - implementing feature templates instead of coding new features
 Feature combinations - combinations that cannot be represented by a linear system

Feature explosion can be limited via techniques such as: regularization, kernel methods, and feature selection.

 Automation 
Automation of feature engineering is a research topic that dates back to the 1990s. Machine learning software that incorporates automated feature engineering has been commercially available since 2016. Related academic literature can be roughly separated into two types:

 Multi-relational decision tree learning (MRDTL) uses a supervised algorithm that is similar to a decision tree. 
 Deep Feature Synthesis uses simpler methods.

 Multi-relational decision tree learning (MRDTL) 
MRDTL generates features in the form of SQL queries by successively adding clauses to the queries. For instance, the algorithm might start out with

SELECT COUNT(*) FROM ATOM t1 LEFT JOIN MOLECULE t2 ON t1.mol_id = t2.mol_id GROUP BY t1.mol_id

The query can then successively be refined by adding conditions, such as "WHERE t1.charge <= -0.392".

However, most  MRDTL studies base implementations on relational databases, which results in many redundant operations. These redundancies can be reduced by using tricks such as tuple id propagation. Efficiency can be increased by using incremental updates, which eliminates redundancies.

 Deep Feature Synthesis 
The Deep Feature Synthesis algorithm beat 615 of 906 human teams in a competition.

Libraries:

 Featuretools. 
 OneBM 
 ExploreKit. 

 Feature stores 

A feature store includes the ability to store code used to generate features, apply the code to raw data, and serve those features to models upon request. Useful capabilities include feature versioning and policies governing the circumstances under which features can be used.

Feature stores can be standalone software tools or built into machine learning platforms. For example, Feast is an open source feature store, while platforms like Uber's Michelangelo use feature stores as a component.