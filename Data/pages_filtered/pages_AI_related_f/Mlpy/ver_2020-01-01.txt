mlpy is a Python, open-source, machine learning library built on top of NumPy/SciPy, the GNU Scientific Library and it makes an extensive use of the Cython language. mlpy provides a wide range of state-of-the-art machine learning methods for supervised and unsupervised problems and it is aimed at finding a reasonable compromise among modularity, maintainability, reproducibility, usability and efficiency. mlpy is multiplatform, it works with Python 2 and 3 and it is distributed under GPL3.

Suited for general-purpose machine learning tasks,Soleymani et al (2011). Continuous emotion detection in response to music videos. IEEE International Conference on Automatic Face & Gesture Recognition and Workshops 2011.Megies, T. et al (2011). ObsPy â€“ What can it do for data centers and observatories? Annals of Geophysics, 2011.Nguyen, M. H (2010). Nguyen et al. Optimal feature selection for support vector machines. Pattern Recognition, 2010.Santana R. (2011) R. Santana. Estimation of distribution algorithms: from available implementations to potential developments. Proceedings of the 13th annual conference companion on Genetic and evolutionary computation, 2011. mlpy's motivating application field is bioinformatics, i.e. the analysis of high throughput omics data.Wuchty S. (2010). Gene pathways and subnetworks distinguish between major glioma subtypes and elucidate potential underlying biology. Journal of Biomedical Informatics, 2010

Features
 Regression: least squares, ridge regression, least angle regression, elastic net, kernel ridge regression, support vector machines (SVM), partial least squares (PLS)
 Classification: linear discriminant analysis (LDA), Basic perceptron, Elastic Net, logistic regression, (Kernel) Support Vector Machines (SVM), Diagonal Linear Discriminant Analysis (DLDA), Golub Classifier, Parzen-based, (kernel) Fisher Discriminant Classifier, k-nearest neighbor, Iterative RELIEF, Classification Tree, Maximum Likelihood Classifier
 Clustering: hierarchical clustering, Memory-saving Hierarchical Clustering, k-means
 Dimensionality reduction: (Kernel) Fisher discriminant analysis (FDA), Spectral Regression Discriminant Analysis (SRDA), (kernel) Principal component analysis (PCA)

Kernel-based functions are managed through a common kernel layer. In particular, the user can choose between supplying the data or a precomputed kernel in input space. Linear, polynomial, Gaussian, exponential and sigmoid kernels are available as default choices, and custom kernels can be defined as well. Many classification and regression algorithms are endowed with an internal feature ranking procedure: in alternative, mlpy implements the I-Relief algorithm. Recursive feature elimination (RFE) for linear classifiers and the KFDA-RFE algorithm are available for feature selection. Methods for feature list analysis (for example the Canberra stability indicator), data resampling and error evaluation are provided, together with different clustering analysis methods (Hierarchical, Memory-saving Hierarchical, k-means). Finally, dedicated
submodules are included for longitudinal data analysis through wavelet transform (Continuous, Discrete and Undecimated) and dynamic programming algorithms (Dynamic Time Warping and variants).