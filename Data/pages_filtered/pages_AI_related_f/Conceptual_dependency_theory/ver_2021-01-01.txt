Conceptual dependency theory is a model of natural language understanding used in artificial intelligence systems.

Roger Schank at Stanford University introduced the model in 1969, in the early days of artificial intelligence.Roger Schank, 1969, A conceptual dependency parser for natural language Proceedings of the 1969 conference on Computational linguistics, Sång-Säby, Sweden pages 1-3 This model was extensively used by Schank's students at Yale University such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.

Schank developed the model to represent knowledge for natural language input into computers. Partly influenced by the work of Sydney Lamb, his goal was to make the meaning independent of the words used in the input, i.e. two sentences identical in meaning, would have a single representation. The system was also intended to draw logical inferences.Cardiff University on Conceptual dependency theory 

The model uses the following basic representational tokens:Language, mind, and brain by Thomas W. Simon, Robert J. Scholes 1982  page 105
 real world objects, each with some attributes. 
 real world actions, each with attributes
 times 
 locations

A set of conceptual transitions then act on this representation, e.g. an ATRANS is used to represent a transfer such as "give" or "take" while a PTRANS is used to act on locations such as "move" or "go". An MTRANS represents mental acts such as "tell", etc.

A sentence such as "John gave a book to Mary" is then represented as the action of an ATRANS on two real world objects John and Mary.
DESCRIPTIONACTIONEXAMPLETransfer of abstract relationshipATRANSgiveTransfer of the physical location of the objectPTRANSgoApplication of physical force to an objectPROPELpushGrasping of an object by an actorGRASPclutchMovement of a body part by its ownerMOVEkick